---
id: metrics-f1
title: F1
sidebar_label: F1
---

<head>
  <link
    rel="canonical"
    href="https://deepeval.com/docs/metrics-f1"
  />
</head>

import Equation from "@site/src/components/Equation";
import MetricTagsDisplayer from "@site/src/components/MetricTagsDisplayer";

<MetricTagsDisplayer singleTurn={true} usesLLMs={false} referenceless={false} />

The F1 metric measures the **harmonic mean** of Precision and Recall, providing a balanced score between **exactness** and **completeness** of your application's response.

:::note
The `F1Metric` does **not** rely on an LLM for evaluation. It purely compares the outputs on a lexical level (string or token-level comparison).
:::

## Required Arguments

To use the `F1Metric`, you'll have to provide the following arguments when creating an [`LLMTestCase`](/docs/evaluation-test-cases#llm-test-case):

- `input`
- `actual_output`
- `expected_output`

Read the [How Is It Calculated](#how-is-it-calculated) section below to learn how test case parameters are used for metric calculation.

## Usage

```python
from deepeval import evaluate
from deepeval.metrics import F1Metric
from deepeval.test_case import LLMTestCase

metric = F1Metric(
    threshold=0.5,
    verbose_mode=True,
)

test_case = LLMTestCase(
    input="Translate 'Hello' to French",
    actual_output="Bonjour le monde",
    expected_output="Bonjour"
)

# To run metric as a standalone
# metric.measure(test_case)
# print(metric.score, metric.reason)

evaluate(test_cases=[test_case], metrics=[metric])
```

There are **TWO** optional parameters when creating an `F1Metric`:

- [Optional] `threshold`: a float representing the minimum passing threshold, defaulted to 0.5.
- [Optional] `verbose_mode`: a boolean which when set to `True`, prints the intermediate steps used to calculate said metric to the console, as outlined in the [How Is It Calculated](#how-is-it-calculated) section. Defaulted to `False`.

### As a Standalone

You can also run the `F1Metric` on a single test case as a standalone, one-off execution.

```python
...

metric.measure(test_case)
print(metric.score, metric.reason)
```

## How Is It Calculated?

The `F1Metric` score is calculated according to the following equation:

<Equation formula="\text{F1} = \frac{2 \times \text{Precision} \times \text{Recall}}{\text{Precision} + \text{Recall}}" />

Where [Precision](/docs/metrics-precision) and [Recall](/docs/metrics-recall) are calculated as described in their respective metrics.

The `F1Metric` provides a single score that balances both the accuracy and completeness of your model's output.

