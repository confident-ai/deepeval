---
id: evaluation-metrics
title: Metrics
sidebar_label: Metrics
---

## Quick Summary

In `deepeval`, a metric serves as a standard of measurement for evaluating the performance of an LLM output based on a specific criteria of interest. Essentially, while the metric acts as the ruler, the test case represents what you're assessing. `deepeval` offers a range of default metrics for you to quickly get started with, which includes:

- Factual Consistency
- Answer Relevancy
- Conceptual Similarity
- RAGAS
- Toxicity
- Bias

`deepeval` also offers you a straightforward way to develop your own custom LLM-based evaluation metrics. This is noteworthy because all default metrics in `deepeval` are derived from traditional NLP models, not LLMs. All metrics are measured on a test case. Visit the [test cases section](evaluation-test-cases) to learn how to apply any metric on test cases for evaluation.

## Types of Metrics

A **_custom_** metric is a type of metric you can easily create by implementing abstract methods and properties of base classes provided by `deepeval`. They are extremely versitle and seamlessly integrate with Confident AI without requiring any additional setup. As you'll see later, a custom metric can either be an **_LLM evaluated_** or **_classic_** metric. A classic metric is a type of metric whose criteria isn't evaluated using an LLM.

`deepeval` also offer **_default_** metrics. All default metrics offered by `deepeval` are classic metrics. This means all default metrics in `deepeval` does not use LLMs for evaluation. This is delibrate for two main reasons:

- LLM evaluated metrics are versitle in nature and it's better if you create one using `deepeval`'s build-ins
- Classic metrics are much harder to compute and requires extensive research

All of `deepeval`'s default metrics output a score between 0-1, and require a `minimum_score` argument to instantiate. A default metric is only successful if the evaluation score is equal to or greater than `minimum_score`.

:::note
Our suggestion is to begin with custom LLM evaluated metrics (which frequently surpass and offer more versatility than leading NLP models), and gradually transition to `deepeval`'s default metrics when feasible. We recommend using default metrics as an optimization to your evaluation workflow because they are more cost-effective.
:::

## Custom LLM Evaluated Metrics

A custom LLM evalated metric, is a custom metric whose evaluation is powered by LLMs. To create a custom metric that uses LLMs for evaluation, simply instantiate an `LLMEvalMetric` class:

```python
from deepeval.metrics.llm_eval_metric import LLMEvalMetric
from deepeval.test_case import LLMTestCase, LLMTestCaseParams

summarization_metric = LLMEvalMetric(
    name="Summarization",
    criteria="Summarization - determine if the actual output is an accurate and concise summarization of the input.",
    evaluation_params=[LLMTestCaseParams.INPUT, LLMTestCaseParams.ACTUAL_OUTPUT],
    minimum_score=0.5,
    model="gpt-4"
)
```

There are three mandatory and two optional parameters required when instantiating an `LLMEvalMetric` class:

- `name`: name of metric
- `criteria`: a description outlining the specific evaluation aspects for each test case.
- `evaluation_params`: a list of type `LLMTestCaseParams`. Include only the parameters that are relevant for evaluation.
- [Optional] `minimum_score`: the passing threshold, defaulted to 0.5.
- [Optional] `model`: the model name. This is defaulted to 'gpt-4-1106-preview' and we currently only support models from OpenAI.

All instances of `LLMEvalMetric` returns a score ranging from 0 - 1. A metric is only successful if the evaluation score is equal to or greater than `minimum_score`.

:::danger
For accurate and valid results, only the parameters that are mentioned in `criteria` should be included as a member of `evaluation_params`.
:::

By default, `LLMEvalMetric` is evaluated using `GPT-4` from OpenAI. Azure OpenAI endpoints are also supported via LLMEvalMetric, the `deployment_id` parameter should be passed
during instantiation of `LLMEvalMetric` class

```python
summarization_metric = LLMEvalMetric(
    name="Summarization",
    criteria="Summarization - determine if the actual output is an accurate and concise summarization of the input.",
    evaluation_params=[LLMTestCaseParams.INPUT, LLMTestCaseParams.ACTUAL_OUTPUT],
    minimum_score=0.5,
    deployment_id="your-deployment-id" 
)
```

:::note
Don't forget to set your Azure OpenAI key 
```python
openai.api_key = "your-Azure-OpenAI-API-key"
``` 
:::

## JudgementalGPT

`JudgementalGPT` is an LLM agent developed in-house by [Confident AI](https://confident-ai.com) that's dedicated to evaluation and is superior to `LLMEvalMetric`. While it operates similarly to `LLMEvalMetric` by utilizing LLMs for scoring, it:

- offers enhanced accuracy and reliability.
- is capable of generating justifications for its scores
- has the ability to conditionally execute code that helps detect logical fallacies during evaluations

To use `JudgementalGPT`, start by logging into Confident AI:

```console
deepeval login
```

Then paste in the following code to define a metric powered by `JudgementalGPT`:

```python
from deepeval.metrics.judgemental_gpt import JudgementalGPT
from deepeval.test_case import LLMTestCase, LLMTestCaseParams

code_correctness_metric = JudgementalGPT(
    name="Code Correctness",
    criteria="Code Correctness - determine whether the python code in the 'actual output' produces a valid JSON.",
    evaluation_params=[LLMTestCaseParams.ACTUAL_OUTPUT],
    minimum_score=0.5,
)
```

Under the hood, `JudgementalGPT(...)` sends a request to Confident AI's servers that hosts `JudgementalGPT`. `JudgementalGPT` accepts four arguments:

- `name`: name of metric
- `criteria`: a description outlining the specific evaluation aspects for each test case.
- `evaluation_params`: a list of type `LLMTestCaseParams`. Include only the parameters that are relevant for evaluation.
- [Optional] `minimum_score`: the passing threshold, defaulted to 0.5.

## Custom Classic Metrics

A custom classic metric, is a metric not provided by `deepeval` and whose criteria isn't evaluated using an LLM.

```python
from deepeval.metrics.metric import Metric

class LengthMetric(Metric):
    # This metric checks if the output length is greater than 10 characters
    def __init__(self, max_length: int=10):
        self.minimum_score = max_length

    def measure(self, test_case: LLMTestCase):
        # Set self.success and self.score in the "measure" method
        self.success = len(test_case.actual_output) > self.minimum_score
        if self.success:
            self.score = 1
        else:
            self.score = 0
        return self.score

    def is_successful(self):
        return self.success

    @property
    def name(self):
        return "Length"
```

Noticed that we accessed `test_case.actual_output` in `measure`. you will have to supply the optional `context` or `expected_output` arguments in the `LLMTestCase` depending on your `measure` implementation.

In this example, you would instantiate `LengthMetric` as follows:

```python
length_metric = LengthMetric(max_length=20)
```

:::info
You must implement `measure`, `is_successful`, and `name` yourself, as these are abstract methods and properties inherited from `Metric`.
:::

## Factual Consistency

Factual consistency measures how factually correct the `actual_output` of your LLM application is compared to the provided `context`. You'll have to supply `context` when creating an `LLMTestCase` to evaluate factual consistency.

```python
import pytest
from deepeval.metrics.factual_consistency import FactualConsistencyMetric
from deepeval.test_case import LLMTestCase
from deepeval.evaluator import run_test

input = "What if these shoes don't fit?"
context = ["All customers are eligible for a 30 day full refund at no extra cost."]

# Replace this with the actual output from your LLM application
actual_output = "We offer a 30-day full refund at no extra cost."

factual_consistency_metric = FactualConsistencyMetric(minimum_score=0.7)
test_case = LLMTestCase(input=input, actual_output=actual_output, context=context)

run_test(test_case, [metric])
```

## Answer Relevancy

Answer Relevancy measures how relevant the `actual_output` of your LLM application is compared to the provided `input`. You don't have to supply `context` or `expected_output` when creating an `LLMTestCase` if you're just evaluating answer relevancy.

```python
import pytest
from deepeval.metrics.answer_relevancy import AnswerRelevancyMetric
from deepeval.test_case import LLMTestCase
from deepeval.evaluator import run_test


input = "What if these shoes don't fit?"

# Replace this with the actual output from your LLM application
actual_output = "We offer a 30-day full refund at no extra cost."

answer_relevancy_metric = AnswerRelevancyMetric(minimum_score=0.7)
test_case = LLMTestCase(input=input, actual_output=actual_output)

run_test(test_case, [answer_relevancy_metric])
```

## Conceptual Similarity

Conceptual Similarity measures how conceptually similar the `actual_output` of your LLM application is compared to the provided `context`. You'll have to supply `context` when creating an `LLMTestCase` to evaluate conceptual similarity.

```python
import pytest
from deepeval.metrics.conceptual_similarity import ConceptualSimilarityMetric
from deepeval.test_case import LLMTestCase
from deepeval.evaluator import run_test


input = "What if these shoes don't fit?"
context = ["All customers are eligible for a 30 day full refund at no extra cost."]

# Replace this with the actual output from your LLM application
actual_output = "We offer a 30-day full refund at no extra cost."

conceptual_similarity_metric = ConceptualSimilarityMetric(minimum_score=0.7)
test_case = LLMTestCase(input=input, actual_output=actual_output, context=context)

run_test(test_case, [conceptual_similarity_metric])
```

## RAGAS

The RAGAS metric is useful for evaluating RAG pipelines (ie. LLM applications built with RAG). The RAGAS score is calculated by taking an unweighted harmonic mean of five distinct metrics.

1. **Faithfulness Metric**: measures hallucination to ensure output align with context. Calculated using the `actual_output` and `context`.

2. **Answer Relevancy Metric**: measures how relevant an answer is relative to the question. Penalizes redundancy or incompleteness. Derived from the `input` and `actual_output`.

3. **Contextual Relevancy Metric**: assesses the relevance of retrieved contexts to input. Penalizes redundant information. Based on the `input` and `context`.

4. **Context Recall Metric**: gauges the recall of the retrieved context using the annotated answer as a reference. Based on the `expected_output` and `context`.

5. **Harmfulness Metric**: measures how harmful the output is. Does not require any reference for evaluation.

The Faithfulness and Answer Relevancy metric assess the quality of the generator in your RAG pipeline, while the Contextual Relevancy and Recall metric evaluate the performance of your retriever.

To begin, install `ragas`:

```console
pip install ragas
```

Create an `LLMTestCase` and supply all parameters to calculate the RAGAS score:

```python
from deepeval.metrics.ragas_metric import RagasMetric
from deepeval.test_case import LLMTestCase

input = "What if these shoes don't fit?"
context = ["All customers are eligible for a 30 day full refund at no extra cost."]
expected_output = "You're eligible for a 30 day refund at no extra cost."

# Replace this with the actual output from your LLM application
actual_output = "We offer a 30-day full refund at no extra cost."

ragas_metric = RagasMetric()
test_case = LLMTestCase(
    input=input,
    actual_output=actual_output,
    expected_output=expected_output,
    context=context,
)

run_test(test_case, [ragas_metric])
```

As mentioned earlier, the RAGAS score is the harmonic mean of five different metrics. You can however import these metrics individually and utilize them in exactly the same way as all other metrics offered by `deepeval`.

```python
from deepeval.metrics.ragas_metric import RagasContextualRelevancyMetric
from deepeval.metrics.ragas_metric import RagasAnswerRelevancyMetric
from deepeval.metrics.ragas_metric import RagasFaithfulnessMetric
from deepeval.metrics.ragas_metric import RagasContextRecallMetric
from deepeval.metrics.ragas_metric import RagasConcisenessMetric
from deepeval.metrics.ragas_metric import RagasCorrectnessMetric
from deepeval.metrics.ragas_metric import RagasCoherenceMetric
from deepeval.metrics.ragas_metric import RagasMaliciousnessMetric
```

## Toxicity

Unlike other default metrics, Toxicity is a **referenceless** metric, meaning it doesn't require comparison to a "source of truth" for evaluation. First, install detoxify.

```console
pip install detoxify
```

Being a referenceless metric means `NonToxicMetric` requires an extra parameter named `evaluation_params`. This parameter is an array, containing elements of the type `LLMTestCaseParams`, and specifies the parameter(s) of a given `LLMTestCase` that will be assessed for toxicity. The `NonToxicMetric` will then compute a score based on the average toxicity levels of each individual component being evaluated.

```python
from deepeval.metrics.toxic_classifier import NonToxicMetric
from deepeval.evaluator import run_test
from deepeval.test_case import LLMTestCase, LLMTestCaseParams


input = "What if these shoes don't fit?"
context = ["All customers are eligible for a 30 day full refund at no extra cost."]

# Replace this with the actual output from your LLM application
actual_output = "We offer a 30-day full refund at no extra cost."

non_toxic_metric = NonToxicMetric(
    evaluation_params=[LLMTestCaseParams.INPUT, LLMTestCaseParams.ACTUAL_OUTPUT],
    minimum_score=0.7
)

test_case = LLMTestCase(
    input=input,
    actual_output=actual_output
)

run_test(test_case, metrics=[non_toxic_metric])
```

Notice that `expected_output` or `context` are not required as `NonToxicMetric` is a referenceless metric.

:::note
In `deepeval`, a higher score is always better (which remember, ranges from 0-1). This is why the metric is called `NonToxicMetric` instead of `ToxicMetric`.
:::

## Bias

`deepeval` offers an `UnBiasedMetric` to tackle bias that can occur after finetuning from any RLHF or optimizations (gender, racial, and political, just to name a few).

```console
pip install Dbias
```

`UnBiasedMetric` is similar to `NonToxicMetric` because it is also a referenceless metric.

```python
from deepeval.metrics.bias_classifier import UnBiasedMetric
from deepeval.test_case import LLMTestCase, LLMTestCaseParams
from deepeval.evaluator import run_test


input = "What if these shoes don't fit?"

# Replace this with the actual output from your LLM application
actual_output = "We offer a 30-day full refund at no extra cost."

unbias_metric = UnBiasedMetric(
    evaluation_params=[LLMTestCaseParams.ACTUAL_OUTPUT],
    minimum_score=0.7
)

test_case = LLMTestCase(
    input=input,
    actual_output=actual_output
)

run_test(test_case, [unbias_metric])
```
