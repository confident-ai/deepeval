---
id: metrics-turn-contextual-precision
title: Turn Contextual Precision
sidebar_label: Turn Contextual Precision
---

<head>
  <link
    rel="canonical"
    href="https://deepeval.com/docs/metrics-turn-contextual-precision"
  />
</head>

import Equation from "@site/src/components/Equation";
import MetricTagsDisplayer from "@site/src/components/MetricTagsDisplayer";

<MetricTagsDisplayer multiTurn={true} chatbot={true} referenceFree={false} />

The turn contextual precision metric is a conversational metric that evaluates whether relevant nodes in your retrieval context are ranked higher than irrelevant nodes **throughout a conversation**.

## Required Arguments

To use the `TurnContextualPrecisionMetric`, you'll have to provide the following arguments when creating a [`ConversationalTestCase`](/docs/evaluation-multiturn-test-cases):

- `turns`
- `expected_outcome`

You must provide the `role`, `content`, and `retrieval_context` for evaluation to happen. Read the [How Is It Calculated](#how-is-it-calculated) section below to learn more.

## Usage

The `TurnContextualPrecisionMetric()` can be used for [end-to-end](/docs/evaluation-end-to-end-llm-evals) multi-turn evaluation:

```python
from deepeval import evaluate
from deepeval.test_case import Turn, ConversationalTestCase
from deepeval.metrics import TurnContextualPrecisionMetric

content = "We offer a 30-day full refund at no extra cost."
retrieval_context = [
    "All customers are eligible for a 30 day full refund at no extra cost."
]

convo_test_case = ConversationalTestCase(
    turns=[
        Turn(role="user", content="What if these shoes don't fit?"),
        Turn(role="assistant", content=content, retrieval_context=retrieval_context)
    ],
    expected_outcome="The chatbot must explain the store policies like refunds, discounts, ..etc.",
)

metric = TurnContextualPrecisionMetric(threshold=0.5)

# To run metric as a standalone
# metric.measure(convo_test_case)
# print(metric.score, metric.reason)

evaluate(test_cases=[convo_test_case], metrics=[metric])
```

There are **SEVEN** optional parameters when creating a `TurnContextualPrecisionMetric`:

- [Optional] `threshold`: a float representing the minimum passing threshold, defaulted to 0.5.
- [Optional] `model`: a string specifying which of OpenAI's GPT models to use, **OR** [any custom LLM model](/docs/metrics-introduction#using-a-custom-llm) of type `DeepEvalBaseLLM`. Defaulted to 'gpt-4.1'.
- [Optional] `include_reason`: a boolean which when set to `True`, will include a reason for its evaluation score. Defaulted to `True`.
- [Optional] `strict_mode`: a boolean which when set to `True`, enforces a binary metric score: 1 for perfection, 0 otherwise. It also overrides the current threshold and sets it to 1. Defaulted to `False`.
- [Optional] `async_mode`: a boolean which when set to `True`, enables [concurrent execution within the `measure()` method.](/docs/metrics-introduction#measuring-metrics-in-async) Defaulted to `True`.
- [Optional] `verbose_mode`: a boolean which when set to `True`, prints the intermediate steps used to calculate said metric to the console, as outlined in the [How Is It Calculated](#how-is-it-calculated) section. Defaulted to `False`.

### As a standalone

You can also run the `TurnContextualPrecisionMetric` on a single test case as a standalone, one-off execution.

```python
...

metric.measure(convo_test_case)
print(metric.score, metric.reason)
```

:::caution
This is great for debugging or if you wish to build your own evaluation pipeline, but you will **NOT** get the benefits (testing reports, Confident AI platform) and all the optimizations (speed, caching, computation) the `evaluate()` function or `deepeval test run` offers.
:::

## How Is It Calculated?

The `TurnContextualPrecisionMetric` score is calculated according to the following equation:

<Equation formula="\text{Turn Contextual Precision} = \frac{\sum \text{Interaction Precision Scores}}{\text{Number of Interactions}}" />

The `TurnContextualPrecisionMetric` first constructs unit interactions of `user` and `assistant` exchanges from the entire conversation. For each interaction, it:

1. **Evaluates each retrieval context node** to determine if it was useful in arriving at the expected outcome
2. **Calculates weighted precision** where earlier relevant nodes contribute more to the score:

<Equation formula="\text{Contextual Precision} = \frac{1}{\text{Number of Relevant Nodes}} \sum_{k=1}^{n} \left( \frac{\text{Number of Relevant Nodes Up to Position } k}{k} \times r_{k} \right)" />

:::info

- **_k_** is the (i+1)<sup>th</sup> node in the `retrieval_context`
- **_n_** is the length of the `retrieval_context`
- **_r<sub>k</sub>_** is the binary relevance for the k<sup>th</sup> node in the `retrieval_context`. _r<sub>k</sub>_ = 1 for nodes that are relevant, 0 if not.

:::

3. Where nodes ranked higher (lower rank number) contribute more weight to the score

The final score is the average of all interaction precision scores across the conversation. This ensures that relevant retrieval context nodes appear earlier in the ranking.