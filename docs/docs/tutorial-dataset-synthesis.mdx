---
id: tutorial-dataset-synthesis
title: Generating Synthetic Data
sidebar_label: Generating Synthetic Data
---

## Quick Summary

If you plan to evaluate your LLM at scale, you'll need a large evaluation dataset. [However, manually curating test data can be time-consuming and costly](https://www.confident-ai.com/blog/the-definitive-guide-to-synthetic-data-generation-using-llms). Fortunately, synthetic data generation offers a way to create high-quality datasets without the limitations of manual curation.

:::note
DeepEval's `Synthesizer` provides a fast and easy way to generate **high-quality goldens** (input, expected output, context) for your evaluation datasets in just a few lines of code. 
:::

In this tutorial, we’ll use DeepEval's `Synthesizer` to generate a high-quality **synthetic dataset** for testing our medical chatbot at scale. Specifically, we'll explore two approaches:

- **Generating from Documents**
- **Generating from Scratch**

:::info
DeepEval also allows you to [generate synthetic data directly from contexts](https://docs.confident-ai.com/docs/synthesizer-generate-from-contexts) if you have access to **pre-prepared contexts**.
:::

## Generating Synthetic Data from Documents

In this section, we'll be generating data from our **knowledge base document**, the *Gale Encyclopedia of Medicine*. The synthesizer first extracts contexts from the document to create inputs and expected outputs, which are then evolved to enhance data diversity.

### 1. Style Configuration

To ensure that our synthetic data aligns with real user queries and use cases, we'll need to define some styling configurations before generating the data.

:::tip
DeepEval lets you **customize the output style and format** of any `input` and/or `expected_output` using the `StylingConfig` object.
:::

Our chatbot supports many scenarios and use cases, but we'll start with the most simple one: generating inputs that represent **individuals describing their medical symptoms** and expected outputs that reflect the chatbot’s ability to provide accurate diagnoses and ask for additional details when needed.

```python
from deepeval.synthesizer.config import StylingConfig

styling_config = StylingConfig(
    expected_output="Ensure the output resembles a medical chatbot tasked with diagnosing a patient’s illness. It should pose additional questions if the details are inadequate or provide a diagnosis when the input is sufficiently detailed.",
    input_format="Mimic the kind of queries or statements a patient might share with a medical chatbot when seeking a diagnosis.",
    task="The chatbot acts as a specialist in medical diagnosis, integrated with a patient scheduling system. It manages tasks in a sequence to ensure precise and effective appointment setting and diagnosis processing.",
    scenario="Non-medical patients describing symptoms to seek a diagnosis.",
)
```

:::info
In addition to styling, DeepEval lets you **customize** other parts of the generation process, from context construction to evolutions.
:::

### 2. Goldens generation

With our styling configuration defined, let’s begin **generating our dataset**.

```python
from deepeval.dataset import EvaluationDataset
from deepeval.synthesizer import Synthesizer

dataset=EvaluationDataset()
synthesizer = Synthesizer(styling_config=styling_config)
dataset.generate_goldens_from_docs(
  document_paths=["./synthesizer_data/encyclopedia.pdf"],
  synthesizer=synthesizer
)

print(dataset.goldens[0])
```

:::note
We're generating goldens using DeepEval's `EvaluationDataset`, but this can also be done directly through the `Synthesizer`.
:::

Let's take a look at an example golden we've generated.

```python
Golden(
  input='''
    I have been experiencing symptoms of oral thrush. Could this be related 
    to other underlying health issues?''',
  expected_output='''
    Experiencing oral thrush can indeed be an indication of underlying health 
    issues. It is often seen in individuals with weakened immune systems. To 
    better understand your situation, could you provide more information about
    any other symptoms you might be experiencing, such as fever, weight loss, 
    or persistent fatigue?''',
  context=["The general physical examination may\nrange from normal findin..."]
  ...
)
```
You can see that even though the input for this synthetic golden is in its simplest form (with no evolutions applied), it remains relevant, aligns with user behavior, and is high quality. Should you want more advanced inputs, you can enhance the generated goldens' complexity by adjusting the evolution settings when initializing the `Synthesizer` object.

### 3. Additional Styling Configurations

We've defined a configuration for patients seeking diagnoses, but it's also important to be exploring additional styling configurations. Using multiple styling configurations allows you to generate a truly **diverse dataset** that is not only comprehensvie but also captures edge cases.

#### Ambiguous Inputs

For example, you may want to generate synthetic goldens where user inputs describe borderline symptoms—providing enough detail to narrow options but insufficient for a definitive diagnosis. This tests the chatbot's ability to handle ambiguity and ask clarifying questions.

```python
styling_config_ambiguous = StylingConfig(
    expected_output="Provide a cautious and detailed response to borderline or ambiguous symptoms. The chatbot should ask clarifying questions when necessary to avoid making unsupported conclusions.",
    input_format="Simulate user inputs that describe borderline symptoms, where the details are vague or insufficient for a definitive diagnosis.",
    task="The chatbot acts as a specialist in medical diagnosis, integrated with a patient scheduling system. It manages tasks in a sequence to ensure precise and effective appointment setting and diagnosis processing.",
    scenario="Non-medical patients describing symptoms that are vague or ambiguous, requiring further clarification from the chatbot."
)
```

#### Challenging User Scenarios
In another scenario, you may want to test how the chatbot responds to users who are rude or in a hurry, evaluating its professionalism and ability to provide effective assistance under pressure. This ensures the chatbot maintains a calm and empathetic tone while providing actionable advice, even in challenging situations.

```python
styling_config_challenging = StylingConfig(
    expected_output="Respond politely and remain professional, even when the user is rude or impatient. The chatbot should maintain a calm and empathetic tone while providing clear and actionable advice.",
    input_format="Simulate users being rushed, frustrated, or disrespectful in their queries.",
    task="The chatbot acts as a specialist in medical diagnosis, integrated with a patient scheduling system. It manages tasks in a sequence to ensure precise and effective appointment setting and diagnosis processing.",
    scenario="Users who are impatient, stressed, or rude but require medical assistance and advice."
)
```


## Generating Synthetic Data from Scratch

Generating synthetic data from documents requires a knowledge base, meaning the generated goldens are designed to test user queries that prompt the LLM to use the RAG engine.

However, since our medical chatbot operates as an Agentic RAG, there are cases where the LLM responses **do not rely on the RAG tool**, necessitating the generation of data from scratch without any context.

### 1. Defining Style Configuration

When generating synthetic goldens from scratch, you can customize the `input` and `expected_output` to test any type of interaction. For example, we'll define a styling configuration to mimic users booking an appointment by providing their name and email details.

```python
from deepeval.synthesizer.config import StylingConfig

styling_config = StylingConfig(
    input_format="User inputs including name and email information for appointment booking.",
    expected_output_format="Structured chatbot response to confirm appointment details.",
    task="The chatbot acts as a specialist in medical diagnosis, integrated with a patient scheduling system. It manages tasks in a sequence to ensure precise and effective appointment setting and diagnosis processing.",
    scenario="Non-medical patients describing symptoms to seek a diagnosis."
)
```
Alternatively, it's also possible to simulate a simple greeting:

```python
styling_config = StylingConfig(
    input_format="Simple greeting from the user to start interaction with the chatbot.",
    expected_output_format="Chatbot's friendly acknowledgment and readiness to assist.",
    task="The chatbot acts as a specialist in medical diagnosis, integrated with a patient scheduling system. It manages tasks in a sequence to ensure precise and effective appointment setting and diagnosis processing.",
    scenario="A patient greeting the chatbot to initiate an interaction."
)
```

### 2. Generating the Goldens

With the configurations defined, simply create a `Synthesizer` object with your desired configuration and begin generating goldens from scratch from your dataset.

```python
from deepeval.dataset import EvaluationDataset
from deepeval.synthesizer import Synthesizer

dataset=EvaluationDataset()
synthesizer = Synthesizer(styling_config=styling_config)
dataset.generate_goldens_from_scratch(
  synthesizer=synthesizer
)
```
At this point, you might be wondering why the `expected_output` is any better than your LLM's `actual_output`, especially when we're generating these expected outputs from scratch without any contexts.

:::info
The reason the `expected_output` works better in synthetic generation pipelines is because it is hyper-focused on a specific task or topic. In contrast, a typical LLM application operates with a broader system prompt or fine-tuning, making it less precise for handling specific queries.
:::

To recap, we've explored two methods for generating high-quality synthetic datasets that are relevant to medical use cases and closely aligned with user behavior. However, synthetic data generation has its challenges. While DeepEval provides quality filtration configurations, dataset review is still essential. 

:::note
Fortunately, Confident AI offers  **dataset management tool** that allows engineers, evaluators, and domain experts to collaborate seamlessly. In the next section, we'll push our generated datasets to the platform for review.
:::









