---
# id: using-custom-llms
title: Using Custom LLMs for Evaluation
sidebar_label: Using Custom LLMs for Evaluation
---

All of `deepeval`'s metrics uses LLMs for evaluation, and is currently defaulted to OpenAI's GPT models. However, for users that don't wish to use OpenAI's GPT models and would instead prefer other providers such as Claude (Anthropic), Gemini (Google), Llama-3 (Meta), or Mistral, `deepeval` provides an easy way for anyone to use literaly **ANY** custom LLM for evaluation.

This guide will show you how to create custom LLMs for evaluation in `deepeval`, and demonstrate various methods to enforce valid JSON LLM outputs that are required for evaluation through the following examples:

- Mistral-7B v0.3 from Hugging Face `transformers`
- Llama-3 8B from Hugging Face `transformers`
- Gemini 1.5 Flash from Vertex AI
- Claude Opus 3 from Anthropic

## Creating A Custom LLM

Here's a quick example on a custom Llama-3 8B model being used for evaluation in `deepeval`:

```python
import transformers
from transformers import BitsAndBytesConfig
from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline

from deepeval.models import DeepEvalBaseLLM

class CustomLlama3_8B(DeepEvalBaseLLM):
    def __init__(self):
        quantization_config = BitsAndBytesConfig(
            load_in_4bit=True,
            bnb_4bit_compute_dtype=torch.float16,
            bnb_4bit_quant_type="nf4",
            bnb_4bit_use_double_quant=True,
        )

        model_4bit = AutoModelForCausalLM.from_pretrained(
            "meta-llama/Meta-Llama-3-8B-Instruct", device_map="auto",quantization_config=quantization_config)
        tokenizer = AutoTokenizer.from_pretrained("meta-llama/Meta-Llama-3-8B-Instruct")

        self.model = model_4bit
        self.tokenizer = tokenizer

    def load_model(self):
        return self.model

    def generate(self, prompt: str) -> str:
        model = self.load_model()

        pipeline = transformers.pipeline(
            "text-generation",
            model=model,
            tokenizer=self.tokenizer,
            use_cache=True,
            device_map="auto",
            max_length=2500,
            do_sample=True,
            top_k=5,
            num_return_sequences=1,
            eos_token_id=self.tokenizer.eos_token_id,
            pad_token_id=self.tokenizer.eos_token_id,
        )

        return pipeline(prompt)

    async def a_generate(self, prompt: str) -> str:
        return self.generate(prompt)

    def get_model_name(self):
        return "Llama-3 8B"
```

There are **SIX** rules to follow when creating a custom LLM evaluation model:

1. Inherit `DeepEvalBaseLLM`.
2. Implement the `get_model_name()` method, which simply returns a string representing your custom model name.
3. Implement the `load_model()` method, which will be responsible for returning a model object.
4. Implement the `generate()` method with **one and only one** parameter of type string that acts as the prompt to your custom LLM.
5. The `generate()` method should return the generated string output from your custom LLM. Note that we called `pipeline(prompt)` to access the model generations in this particular example, but this could be different depending on the implementation of your custom model object.
6. Implement the `a_generate()` method, with the same function signature as `generate()`. **Note that this is an async method**. In this example, we called `self.generate(prompt)`, which simply reuses the synchronous `generate()` method. However, although optional, you should implement an asynchronous version (if possible) to speed up evaluation.

:::caution
In later sections, you'll an exception to rules 4. and 5., as the `generate()` and `a_generate()` method can actually be rewritten to optimize custom LLM outputs that are essential for evaluation.
:::

Then, instatiate the `CustomLlama3_8B` class and test the `generate()` (or `a_generate()`) method out:

```python
...

custom_llm = CustomLlama3_8B(model=model_4bit, tokenizer=tokenizer)
print(custom_llm.generate("Write me a joke"))
```

Finally, supply it to a metric to run evaluations using your custom LLM:

```python
from deepeval.metrics import AnswerRelevancyMetric
...

metric = AnswerRelevancyMetric(model=custom_llm)
```

**Congratulations ðŸŽ‰!** You can now evaluate using any custom LLM of your choice on all LLM evaluation metrics offered by `deepeval`.

## JSON Enforcement for Custom LLMs

In the previous section, we learnt how to create a custom LLM, but if you've ever used custom LLMs for evaluation in `deepeval`, you may have encountered the following error:

```bash
ValueError: Evaluation LLM outputted an invalid JSON. Please use a better evaluation model.
```

This error arises when the custom LLM used for evaluation is unable to generate valid JSONs during metric calculation, which stops the evaluation process altogether. This happens because for smaller and less powerful LLMs, prompt engineering alone is not sufficient to enforce JSON outputs, which so happens to be the method used in `deepeval`'s metrics. As a result, it's vital to find a workaround for users not using OpenAI's GPT models for evaluation.

:::info
All of `deepeval`'s metrics require the evaluation model to generate valid JSONs to extract properties such as: reasons, verdicts, statements, and other types of LLM-generated responses that are later used for calculating metric scores, and so when the generated JSONs required to extract these properties are invalid (eg. missing brackets, incomplete string quotations, extra trailing commas, or mismatched keys), `deepeval` won't be able to use the necessary information required for metric calculation. Here's an example of an invalid JSON an open-source model like `mistralai/Mistral-7B-Instruct-v0.3` might output:

```console
{
    "reaso: "The actual output does directly not address the input",
}
```

:::

## Rewriting the `generate()` and `a_generate()` Method

## More Examples

## JSON Enforcement libraries

### The `lm-format-enforcer` Library

The **LM-Format-Enforcer** is a versatile library designed to standardize the output formats of language models. It supports Python-based language models across various platforms, including popular frameworks such as Transformers, LangChain, LlamaIndex, llama.cpp, vLLM, Haystack, NVIDIA, TensorRT-LLM, and ExLlamaV2. For comprehensive details about the package and advanced usage instructions, [please visit the LM-format-enforcer github page](https://github.com/noamgat/lm-format-enforcer).

The LM-Format-Enforcer combines a **character-level parser** with a **tokenizer prefix tree**. Unlike other libraries that strictly enforce output formats, this method enables LLMs to sequentially generate tokens that meet output format constraints, thereby enhancing the quality of the output.

### The `instructor` Library

**Instructor** is a user-friendly python library built on top of Pydantic. It enables straightforward confinement of your LLM's output by encapsulating your LLM client within an Instructor method. It simplifies the process of extracting structured data, such as JSON, from LLMs including GPT-3.5, GPT-4, GPT-4-Vision, and open-source models like Mistral/Mixtral, Anyscale, Ollama, and llama-cpp-python. For more information on advanced usage or integration with other models not covered here, [please consult the documentation](https://github.com/jxnl/instructor).

## Tutorials

To enforce JSON output in a custom `DeepEvalLLM`, your custom LLM's generate and a_generate methods must accept a `schema` argument of type Pydantic `BaseModel`.

```python
class Mistral7B(DeepEvalBaseLLM):

    def generate(self, prompt: str, schema: BaseModel) -> BaseModel:
        ...

    async def a_generate(self, prompt: str, schema: BaseModel) -> BaseModel:
        ...
```

:::caution
Your `generate` and `a_generate` functions **must always output an object of type `BaseModel`**.
:::

LLM JSON confinement is possible across **a range of LLM models**, including:

- Hugging Face models (Mistral-7b-v0.2, Llama-3-70b-Instruct, etc)
- LangChain, LlamaIndex, Haystack models
- llama.cpp models
- OpenAI models (GPT-4o, GPT-3.5, etc)
- Anthropic models (Claude-3 Opus, etc)
- Gemini models

In the following set of tutorials, we'll go through setting up Pydantic enforcement using the libraries from the above section for:

1.  [Mistral-7B v0.3 (through HF)](#1-mistral-7b-v03)
2.  [Llama-3 70B Instruct (through HF)](#2-llama-3-8b)
3.  [Gemini 1.5 Flash (through Google AI)](#3-gemini-15-flash)
4.  [Claude 3 Opus Chat (through Anthropic)](#4-claude-3-opus)

## Mistral-7B v0.3

### 1. Install `lm-format-enforcer`

Begin by installing the `lm-format-enforcer` package via pip:

```bash
pip install lm-format-enforcer
```

### 2. Create your custom LLM

Create your custom Mistral-7B v0.3 LLM class using the `DeepEvalLLM` base class. Define the `schema` parameter in your generate and a_generate method signatures.

```python
from pydantic import BaseModel

class Mistral7B(DeepEvalBaseLLM):
    def __init__(
        self,
        model,
        tokenizer
    ):
        self.model = model
        self.tokenizer = tokenizer

    def load_model(self):
        return self.model

    def get_model_name(self):
        return "Mistral-7B v0.3"

    def generate(self, prompt: str, schema: BaseModel) -> BaseModel:
        ...

    async def a_generate(self, prompt: str, schema: BaseModel) -> BaseModel:
        ...
```

### 3. Write the `generate` method

Write the `generate` method for your custom LLM by utilizng `JsonSchemaParser` and `build_transformers_prefix_allowed_tokens_fn` from the `lmformatenforcer` library to ensure that the model's outputs strictly adhere to the defined JSON `schema`.

The `lmformatenforcer` helps language models output a JSON object that follows the Pydantic `schema`, not the actual Pydantic schema itself. Therefore, we must convert this back to an object of type `schema` for evaluation.

```python
from lmformatenforcer import JsonSchemaParser
from lmformatenforcer.integrations.transformers import build_transformers_prefix_allowed_tokens_fn

class Mistral7B(DeepEvalBaseLLM):
    ...

    def generate(self, prompt: str, schema: BaseModel) -> BaseModel:
        hf_pipeline = pipeline(
                "text-generation",
                model=self.model,
                tokenizer=self.tokenizer,
                use_cache=True,
                device_map="auto",
                max_length=2500,
                do_sample=True,
                top_k=5,
                num_return_sequences=1,
                eos_token_id=self.tokenizer.eos_token_id,
                pad_token_id=self.tokenizer.eos_token_id,
        )
        parser = JsonSchemaParser(pydantic_model.schema())
        prefix_function = build_transformers_prefix_allowed_tokens_fn(hf_pipeline.tokenizer, parser)
        output_dict = hf_pipeline(prompt, prefix_allowed_tokens_fn=prefix_function)
        output = output_dict[0]['generated_text'][len(prompt):]
        json_result = json.loads(output)
        return pydantic_model(**json_result)
```

### 4. Instantiating your model

Load your models from Hugging Face's `transformers` library. Optionally, you can pass in a `quantization_config` parameter if your compute resources are limited.

```python
from transformers import BitsAndBytesConfig
from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline

quantization_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_compute_dtype=torch.float16,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_use_double_quant=True,
)

model_4bit = AutoModelForCausalLM.from_pretrained(
    "mistralai/Mistral-7B-Instruct-v0.3", device_map="auto",quantization_config=quantization_config)
tokenizer = AutoTokenizer.from_pretrained("mistralai/Mistral-7B-Instruct-v0.3")
mistral_custom = Mistral7B(model_4bit, tokenizer)
```

### 5. Running evaluations

Finally, evaluate your test cases using your desired metric on the custom Mistral-7B v0.3 model. You'll find that some of your LLM test cases, which previously failed to evaluate due to an invalid JSON error, will now run successfully after you have defined the `schema` parameter.

```python
from deepeval.metrics import AnswerRelevancyMetric
from deepeval.test_case import LLMTestCase

test_case = LLMTestCase(...)

metric = AnswerRelevancyMetric(threshold=0.5, model=mistral_custom, verbose_mode=True)
metric.measure(test_case)
print(metric.reason)
```

## Llama-3 8B

### 1. Install `lm-format-enforcer`

Begin by installing the `lm-format-enforcer` package via pip:

```bash
pip install lm-format-enforcer
```

### 2. Create your custom LLM

Create your custom Llama-3 8B LLM class using the `DeepEvalLLM` base class. Define the `schema` parameter in your generate and a_generate method signatures.

```python
from pydantic import BaseModel

class Llama8B(DeepEvalBaseLLM):
    def __init__(
        self,
        model,
        tokenizer
    ):
        self.model = model
        self.tokenizer = tokenizer

    def load_model(self):
        return self.model

    def get_model_name(self):
        return "Mistral-3 8B"

    def generate(self, prompt: str, schema: BaseModel) -> BaseModel:
        ...

    async def a_generate(self, prompt: str, schema: BaseModel) -> BaseModel:
        ...
```

### 3. Write the `generate` method

Write the `generate` method for your custom LLM by utilizng `JsonSchemaParser` and `build_transformers_prefix_allowed_tokens_fn` from the `lmformatenforcer` library to ensure that the model's outputs strictly adhere to the defined JSON `schema`.

The `lmformatenforcer` helps language models output a JSON object that follows the Pydantic `schema`, not the actual Pydantic schema itself. Therefore, we must convert this back to an object of type `schema` for evaluation.

```python
from lmformatenforcer import JsonSchemaParser
from lmformatenforcer.integrations.transformers import build_transformers_prefix_allowed_tokens_fn

class Llama8B(DeepEvalBaseLLM):
    ...

    def generate(self, prompt: str, schema: BaseModel) -> BaseModel:
        hf_pipeline = pipeline(
                "text-generation",
                model=self.model,
                tokenizer=self.tokenizer,
                use_cache=True,
                device_map="auto",
                max_length=2500,
                do_sample=True,
                top_k=5,
                num_return_sequences=1,
                eos_token_id=self.tokenizer.eos_token_id,
                pad_token_id=self.tokenizer.eos_token_id,
        )
        parser = JsonSchemaParser(pydantic_model.schema())
        prefix_function = build_transformers_prefix_allowed_tokens_fn(hf_pipeline.tokenizer, parser)
        output_dict = hf_pipeline(prompt, prefix_allowed_tokens_fn=prefix_function)
        output = output_dict[0]['generated_text'][len(prompt):]
        json_result = json.loads(output)
        return pydantic_model(**json_result)
```

### 4. Instantiating your model

Load your models from Hugging Face's `transformers` library. Optionally, you can pass in a `quantization_config` parameter if your compute resources are limited.

```python
from transformers import BitsAndBytesConfig
from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline

quantization_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_compute_dtype=torch.float16,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_use_double_quant=True,
)

model_4bit = AutoModelForCausalLM.from_pretrained(
    "meta-llama/Meta-Llama-3-8B-Instruct", device_map="auto",quantization_config=quantization_config)
tokenizer = AutoTokenizer.from_pretrained("meta-llama/Meta-Llama-3-8B-Instruct")
mistral_custom = Llama8B(model_4bit, tokenizer)
```

### 5. Running evaluations

Finally, evaluate your test cases using your desired metric on the custom Llama-3 8B model. You'll find that some of your LLM test cases, which previously failed to evaluate due to an invalid JSON error, will now run successfully after you have defined the schema parameter.

```python
from deepeval.metrics import AnswerRelevancyMetric
from deepeval.test_case import LLMTestCase

test_case = LLMTestCase(...)

metric = AnswerRelevancyMetric(threshold=0.5, model=mistral_custom, verbose_mode=True)
metric.measure(test_case)
print(metric.reason)
```

## Gemini 1.5 Flash

### 1. Install `instructor`

Begin by installing the `instructor` package via pip:

```bash
pip install -U instructor
```

### 2. Create your custom LLM

Create your custom Gemini 1.5 Flash LLM class using the `DeepEvalLLM` base class. Define the `schema` parameter in your generate and a_generate method signatures.

```python
from pydantic import BaseModel
import google.generativeai as genai

class GeminiFlash(DeepEvalBaseLLM):
    def __init__(
        self
    ):
        self.model = genai.GenerativeModel(
            model_name="models/gemini-1.5-flash-latest"
        )

    def load_model(self):
        return self.model

    def get_model_name(self):
        return "Gemini 1.5 Flash"

    def generate(self, prompt: str, schema: BaseModel) -> BaseModel:
        ...

    async def a_generate(self, prompt: str, schema: BaseModel) -> BaseModel:
        ...
```

### 3. Write the `generate` method

The `instructor` client automatically allows you to create a structured response by defining a response_model parameter which accepts a schema model that inherits from `BaseModel`. To write your `a_generate` function, wrap the Instructor client around your Async Gemini client. Alternatively, you can call the generate function directly if you prefer not to write an async method.

```python
import instructor

class GeminiFlash(DeepEvalBaseLLM):
    ...

    def generate(self, prompt: str, schema: BaseModel) -> BaseModel:
        client = self.load_model()
        instructor_client = instructor.from_gemini(
            client=client,
        mode=instructor.Mode.GEMINI_JSON,
        )
        resp = instructor_client.messages.create(
            messages=[
                {
                    "role": "user",
                    "content": prompt,
                }
            ],
            response_model=schema,
        )
        return resp

    async def a_generate(self, prompt: str, schema: BaseModel) -> BaseModel:
        ...
```

### 4. Running evaluations

Finally, evaluate your test cases using your desired metric on the custom Gemini 1.5 Flash model. You'll find that some of your LLM test cases, which previously failed to evaluate due to an invalid JSON error, will now run successfully after you have defined the schema parameter.

```python
from deepeval.metrics import AnswerRelevancyMetric
from deepeval.test_case import LLMTestCase

test_case = LLMTestCase(...)
gemini_custom = GeminiFlash()

metric = AnswerRelevancyMetric(threshold=0.5, model=gemini_custom, verbose_mode=True)
metric.measure(test_case)
print(metric.reason)
```

## Claude 3 Opus

### 1. Install `instructor`

Begin by installing the `instructor` package via pip:

```bash
pip install -U instructor
```

### 2. Create your custom LLM

Create your custom Claude 3 Opus LLM class using the `DeepEvalLLM` base class. Define the `schema` parameter in your generate and a_generate method signatures.

```python
from pydantic import BaseModel
from anthropic import Anthropic

class ClaudeOpus(DeepEvalBaseLLM):
    def __init__(
        self
    ):
        self.model = Anthropic()

    def load_model(self):
        return self.model

    def get_model_name(self):
        return "Claude 3 Opus LLM"

    def generate(self, prompt: str, schema: BaseModel) -> BaseModel:
        ...

    async def a_generate(self, prompt: str, schema: BaseModel) -> BaseModel:
        ...
```

### 3. Write the `generate` method

The `instructor` client automatically allows you to create a structured response by defining a response_model parameter which accepts a schema model that inherits from `BaseModel`. To write your `a_generate` function, wrap the Instructor client around your Async Anthropic client. Alternatively, you can call the generate function directly if you prefer not to write an async method.

```python
import instructor

class ClaudeOpus(DeepEvalBaseLLM):
    ...

    def generate(self, prompt: str, pydantic_model: BaseModel) -> BaseModel:
        client = self.load_model()
        instructor_client = instructor.from_anthropic(client)
        resp = instructor_client.messages.create(
            model="claude-3-opus-20240229",
            max_tokens=1024,
            messages=[
                {
                    "role": "user",
                    "content": prompt,
                }
            ],
            response_model=User,
        )
        return resp

    async def a_generate(self, prompt: str) -> BaseModel:
        ...
```

### 4. Running evaluations

Finally, evaluate your test cases using your desired metric on the custom Claude 3 Opus model. You'll find that some of your LLM test cases, which previously failed to evaluate due to an invalid JSON error, will now run successfully after you have defined the schema parameter.

```python
from deepeval.metrics import AnswerRelevancyMetric
from deepeval.test_case import LLMTestCase

test_case = LLMTestCase(...)
claude_custom = ClaudeOpus()

metric = AnswerRelevancyMetric(threshold=0.5, model=claude_custom, verbose_mode=True)
metric.measure(test_case)
print(metric.reason)
```

## How All of This Fits into Improving Evaluations

Deepeval metrics will automatically check for the `schema` parameter in custom LLMs. When provided, they utilize the associated `pydantic` model for the task.

:::caution
Ensure that the `schema` field is always of type `BaseModel` and the generate functions returns an object of type `BaseModel`.
:::
