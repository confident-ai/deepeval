---
id: prompt-optimization-gepa
title: How GEPA Works
sidebar_label: GEPA
---
<head>
  <link
    rel="canonical"
    href="https://deepeval.com/docs/prompt-optimization-gepa"
  />
</head>

import Tabs from "@theme/Tabs";
import TabItem from "@theme/TabItem";

`deepeval`’s optimizer is built on **GEPA** (Genetic-Pareto), adapted from the DSPy paper [GEPA: Genetic Pareto Optimization of LLM Prompts](https://arxiv.org/pdf/2507.19457). GEPA is a sample-efficient algorithm that explores prompt variants using metric-driven feedback and multi-objective selection.

## GEPA At A Glance

Each GEPA run starts from your current prompt and a set of goldens, then iteratively proposes new prompts and keeps the ones that look meaningfully better.

In broad strokes:

1. Start from your current prompt and evaluation goldens.
2. Split goldens into a **Pareto set** for stable comparison and a **feedback pool** for exploration.
3. Repeat for a fixed number of iterations: gather feedback on a minibatch, rewrite the prompt, score the new candidate on the Pareto set, decide whether to keep it.
4. Return the best prompt found, plus an `OptimizationReport` with details of the run.

The rest of this page unpacks how those steps work inside `deepeval`.

## Splitting Goldens

When you call:

```python
optimized_prompt = optimizer.optimize(prompt=prompt, goldens=goldens)
````

GEPA first splits your goldens into two disjoint subsets:

- `D_pareto` is a fixed subset used as a stable benchmark when comparing prompts.
- `D_feedback` contains the remaining goldens and is used for sampling minibatches and collecting feedback.

The size of `D_pareto` is controlled by `pareto_size` in `GEPAConfig`. The split is randomized but reproducible using a shared random seed.

In practice:

- `D_pareto` gives you a consistent lens for comparing prompts over time.
- `D_feedback` gives you variety so the optimizer does not overfit to a single slice of the dataset.

You can use GEPA with few or many goldens. A larger pool of goldens improves coverage, while the cost per step is controlled by `pareto_size` and the minibatch settings rather than by the total number of goldens.

:::info Minibatch Sizing

GEPA samples a minibatch from `D_feedback` on each iteration. The effective size is chosen from four fields in `GEPAConfig`:

- `minibatch_size` is a fixed size. When set, it overrides dynamic sizing.
- `minibatch_min_size` is a lower bound that keeps the batch from being too small.
- `minibatch_max_size` is an upper bound that prevents very large batches.
- `minibatch_ratio` is a fraction of **`len(goldens)`** used to compute a dynamic size.

Conceptually, if `minibatch_size` is not set, the dynamic size is derived from `minibatch_ratio` and then bounded between the min and max minibatch size. Higher effective minibatch sizes explore more examples per step and therefore increase evaluation cost; lower sizes are cheaper but noisier.
:::

## Scoring & Feedback

GEPA treats your metrics as the source of both scores and feedback.

On `D_pareto`, each metric produces numeric scores for a prompt. These are aggregated into:

- averages per-metric used for Pareto comparisons, and
- a single scalar aggregate used when deciding whether a child prompt should replace its parent.

On `D_feedback`, metrics can also produce natural language reasons describing where the model succeeded or failed. During each iteration, GEPA:

- samples a minibatch from `D_feedback`,
- runs your app through `model_callback`,
- scores the outputs with your metrics,
- collects and normalizes the metric feedback into a single `feedback_text` string.

This `feedback_text` is what the optimizer's internal prompt rewriter uses to propose the next prompt candidate.

## GEPA Loop

Once goldens are split and the initial prompt is scored, GEPA enters its main loop. Each iteration does the following:

1. Select a parent prompt from the current Pareto frontier.
2. Run that parent on a minibatch from `D_feedback` using `model_callback` and your metrics.
3. Aggregate metric reasons into `feedback_text`.
4. Call the prompt rewriter with the current prompt and `feedback_text` to generate a child prompt.
5. Score the child prompt on `D_pareto`.
6. Compare the child and parent using the aggregate score and a few acceptance thresholds.

`min_delta` in `GEPAConfig` controls how much better the child must be before it is considered a clear improvement. `tie_tolerance` defines how close two scores can be before they are treated as a tie. Ties are resolved using `tie_breaker`, which can prefer either the parent or the child.

Intuitively, each iteration is:

> "Pick a good prompt, ask your metrics where it falls short, rewrite it based on that feedback, and keep it if it looks meaningfully better."

## Pareto Frontier

Because you may care about several metrics at once, GEPA maintains a **Pareto frontier** of prompts rather than a single scalar-best candidate.

A prompt is considered dominated if there exists another prompt that is at least as good on every metric and strictly better on at least one. The Pareto frontier is the set of non-dominated prompts that represent different trade-offs among your metrics.

Inside `deepeval`, GEPA:

- tracks a score vector per prompt on `D_pareto`,
- updates the frontier whenever a new child is accepted,
- and uses the scalar aggregate score only for local decisions such as "accept vs reject vs tie" when comparing parent and child.

You can inspect these score vectors later via the optimization report to understand how each metric behaved over the run.

## GEPA Configuration

`GEPAConfig` holds the main settings for the algorithm. The most important fields are:

- `iterations`
  Total number of mutation attempts. Higher values give more search but cost more evaluations.
- `pareto_size`
  Number of goldens in `D_pareto`. This controls the cost of scoring each candidate on the stable set.

- `minibatch_size`, `minibatch_min_size`, `minibatch_max_size`, `minibatch_ratio`
  Control how many goldens are sampled from `D_feedback` on each iteration. If `minibatch_size` is set, it is used directly (bounded by `len(goldens)`). Otherwise, a dynamic size is computed from `minibatch_ratio` and then bounded between the min and max sizes.
- `random_seed`
  Seed used for all GEPA randomness, including the Pareto/feedback split and sampling. Use this to make runs reproducible.
- `min_delta`
  Minimum improvement required for a child to be considered clearly better than its parent.
- `tie_tolerance` and `tie_breaker`
  Control how near-equal scores are treated. If two candidates are within `tie_tolerance`, they are considered tied and the `tie_breaker` policy decides whether to keep the parent, the child, or another consistent choice.

Most users can start with the defaults. When you want more control, you can construct a `GEPAConfig`, pass it to `GEPARunner`, and attach that runner to your `PromptOptimizer`.

## What GEPA Returns

After the configured number of iterations, GEPA selects a best prompt from the Pareto frontier and returns it as a regular `Prompt`:

- `optimized_prompt.text_template` is the optimized prompt string you can use in your app.
- `optimized_prompt.optimization_report` is an `OptimizationReport` with a unique optimization id, the accepted candidates, their parent–child relationships, and metric summaries over time.

You can log or store this report alongside your prompt to compare different optimization runs and to reproduce successful configurations later.

:::info Where To Go Next

For how to wire GEPA into your own system, see the [Prompt Optimization Introduction](/docs/prompt-optimization-introduction). For details on callback structure and available kwargs, refer to the "Create An Optimizer" section on that page and the discussion of `model_callback`.
:::
