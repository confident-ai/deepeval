---
id: evaluation-flags-and-configs
title: Flags and Configs
sidebar_label: Flags and Configs
---

Sometimes you might want to customize the behavior of different settings for `evaluate()` and `assert_test()`, and this can be done using "configs" (short for configurations) and "flags".

:::note
For example, if you're using a [custom LLM judge for evaluation](/guides/guides-using-custom-llms), you may wish to `ignore_errors` to not interrupt evaluations whenever your model fails to produce a valid JSON, or avoid rate limit errors entirely by lowering the `max_concurrent` value.
:::

## Environment Variables

Looking for provider-specific variables (OpenAI, Bedrock, Azure, etc.)? See [Model Settings](#model-settings).

### DeepEval Settings

| Variable | Values | Effect |
| --- | --- | --- |
| `DEEPEVAL_DISABLE_DOTENV` | `1` / unset | Disable dotenv autoload at import. |
| `ENV_DIR_PATH` | path / unset | Directory containing `.env` files (defaults to CWD when unset). |
| `APP_ENV` | string / unset | When set, loads `.env.{APP_ENV}` between `.env` and `.env.local`. |
| `DEEPEVAL_DISABLE_LEGACY_KEYFILE` | `1` / unset | Disable reading legacy `.deepeval/.deepeval` JSON keystore into env. |
| `DEEPEVAL_DEFAULT_SAVE` | `dotenv[:path]` / unset | Default persistence target for `deepeval set-* --save` when `--save` is omitted. |
| `DEEPEVAL_FILE_SYSTEM` | `READ_ONLY` / unset | Restrict file writes in constrained environments. |
| `DEEPEVAL_RESULTS_FOLDER` | path / unset | Export a timestamped JSON of the latest test run into this directory (created if needed). |
| `DEEPEVAL_IDENTIFIER` | string / unset | Default identifier for runs (same idea as `deepeval test run -id ...`). |

DeepEval autoloads dotenv files in this order: `.env` -> `.env.{APP_ENV}` -> `.env.local` (highest precedence among dotenv files), and never overwrites existing process environment variables (process env wins).

### Display / Truncation

| Variable | Values | Effect |
| --- | --- | --- |
| `DEEPEVAL_MAXLEN_TINY` | int | Max length used for "tiny" shorteners (default: 40). |
| `DEEPEVAL_MAXLEN_SHORT` | int | Max length used for "short" shorteners (default: 60). |
| `DEEPEVAL_MAXLEN_MEDIUM` | int | Max length used for "medium" shorteners (default: 120). |
| `DEEPEVAL_MAXLEN_LONG` | int | Max length used for "long" shorteners (default: 240). |
| `DEEPEVAL_SHORTEN_DEFAULT_MAXLEN` | int / unset | Overrides the default max length used by `shorten(...)` (falls back to `DEEPEVAL_MAXLEN_LONG` when unset). |
| `DEEPEVAL_SHORTEN_SUFFIX` | string | Suffix used by `shorten(...)` (default: `...`). |
| `DEEPEVAL_VERBOSE_MODE` | `1` / unset | Enable verbose mode globally (where supported). |
| `DEEPEVAL_LOG_STACK_TRACES` | `1` / unset | Log stack traces for errors (where supported). |

### Retry / Backoff Tuning

These flags control retry and backoff behavior for API calls.

| Variable                         | Type  | Default | Notes                         |
| -------------------------------- | ----- | ------- | ----------------------------- |
| `DEEPEVAL_RETRY_MAX_ATTEMPTS`    | int   | `2`     | Total attempts (1 retry)      |
| `DEEPEVAL_RETRY_INITIAL_SECONDS` | float | `1.0`   | Initial backoff               |
| `DEEPEVAL_RETRY_EXP_BASE`        | float | `2.0`   | Exponential base (≥ 1)        |
| `DEEPEVAL_RETRY_JITTER`          | float | `2.0`   | Random jitter added per retry |
| `DEEPEVAL_RETRY_CAP_SECONDS`     | float | `5.0`   | Max sleep between retries     |
| `DEEPEVAL_SDK_RETRY_PROVIDERS`   | list / unset | Provider slugs for which retries are delegated to provider SDKs (supports `["*"]`). |
| `DEEPEVAL_RETRY_BEFORE_LOG_LEVEL` | int / unset | Log level for "before retry" logs (defaults to `LOG_LEVEL` if set, else INFO). |
| `DEEPEVAL_RETRY_AFTER_LOG_LEVEL` | int / unset | Log level for "after retry" logs (defaults to ERROR). |

### Timeouts / Concurrency

| Variable | Values | Effect |
| --- | --- | --- |
| `DEEPEVAL_MAX_CONCURRENT_DOC_PROCESSING` | int | Max concurrent document processing tasks (default: 2). |
| `DEEPEVAL_TIMEOUT_THREAD_LIMIT` | int | Max threads used by timeout machinery (default: 128). |
| `DEEPEVAL_TIMEOUT_SEMAPHORE_WARN_AFTER_SECONDS` | float | Warn if acquiring timeout semaphore takes too long (default: 5.0). |
| `DEEPEVAL_PER_ATTEMPT_TIMEOUT_SECONDS_OVERRIDE` | float / unset | Per-attempt timeout override for provider calls (preferred override key). |
| `DEEPEVAL_PER_TASK_TIMEOUT_SECONDS_OVERRIDE` | float / unset | Outer timeout budget override for a metric/test-case (preferred override key). |
| `DEEPEVAL_TASK_GATHER_BUFFER_SECONDS_OVERRIDE` | float / unset | Override extra buffer time added to gather/drain after tasks complete. |
| `DEEPEVAL_PER_ATTEMPT_TIMEOUT_SECONDS` | float (computed) | Read-only computed value. To override, set `DEEPEVAL_PER_ATTEMPT_TIMEOUT_SECONDS_OVERRIDE`. |
| `DEEPEVAL_PER_TASK_TIMEOUT_SECONDS` | float (computed) | Read-only computed value. To override, set `DEEPEVAL_PER_TASK_TIMEOUT_SECONDS_OVERRIDE`. |
| `DEEPEVAL_TASK_GATHER_BUFFER_SECONDS` | float (computed) | Read-only computed value. To override, set `DEEPEVAL_TASK_GATHER_BUFFER_SECONDS_OVERRIDE`. |

### Telemetry / Debug

| Variable | Values | Effect |
| --- | --- | --- |
| `DEEPEVAL_DEBUG_ASYNC` | `1` / unset | Enable extra async debugging (where supported). |
| `DEEPEVAL_TELEMETRY_OPT_OUT` | `1` / unset | Opt out of telemetry (unset defaults to telemetry enabled). |
| `DEEPEVAL_UPDATE_WARNING_OPT_IN` | `1` / unset | Opt in to update warnings (where supported). |
| `DEEPEVAL_GRPC_LOGGING` | `1` / unset | Enable extra gRPC logging. |

#### Persisting CLI settings with `--save`

The `deepeval set-*` and `unset-*` commands accept an optional `--save` flag:

```bash
# Write config/secrets to a dotenv file (recommended)
deepeval set-openai --model gpt-4o-mini --save=dotenv:.env.local
deepeval unset-openai --save=dotenv:.env.local
```

**Behavior:**

- Without `--save`: non-secret settings are stored in `.deepeval/.deepeval` (JSON). Secrets are never written there.

- With `--save=dotenv[:path]`: both secrets and non-secrets are written to the specified dotenv (default: `.env.local`).

- Dotenv files should be git ignored.

- When unsetting a provider with `--save`, only that provider’s keys are removed from the dotenv file.

Project default (optional): set a default so you don’t have to pass `--save` every time:

```bash
export DEEPEVAL_DEFAULT_SAVE="dotenv:.env.local"
```

If `DEEPEVAL_DEFAULT_SAVE` is set, it is used whenever `--save` is omitted.

> Roadmap: future versions may support secure stores, such as keyring. The `--save` flag is designed to be pluggable.

## Configs for `evaluate()`

### Async Configs

The `AsyncConfig` controls how concurrently `metrics`, `observed_callback`, and `test_cases` will be evaluated during `evaluate()`.

```python
from deepeval.evaluate import AsyncConfig
from deepeval import evaluate

evaluate(async_config=AsyncConfig(), ...)
```

There are **THREE** optional parameters when creating an `AsyncConfig`:

- [Optional] `run_async`: a boolean which when set to `True`, enables concurrent evaluation of test cases **AND** metrics. Defaulted to `True`.
- [Optional] `throttle_value`: an integer that determines how long (in seconds) to throttle the evaluation of each test case. You can increase this value if your evaluation model is running into rate limit errors. Defaulted to 0.
- [Optional] `max_concurrent`: an integer that determines the maximum number of test cases that can be ran in parallel at any point in time. You can decrease this value if your evaluation model is running into rate limit errors. Defaulted to `20`.

The `throttle_value` and `max_concurrent` parameter is only used when `run_async` is set to `True`. A combination of a `throttle_value` and `max_concurrent` is the best way to handle rate limiting errors, either in your LLM judge or LLM application, when running evaluations.

### Display Configs

The `DisplayConfig` controls how results and intermediate execution steps are displayed during `evaluate()`.

```python
from deepeval.evaluate import DisplayConfig
from deepeval import evaluate

evaluate(display_config=DisplayConfig(), ...)
```

There are **FOUR** optional parameters when creating an `DisplayConfig`:

- [Optional] `verbose_mode`: a optional boolean which when **IS NOT** `None`, overrides each [metric's `verbose_mode` value](/docs/metrics-introduction#debugging-a-metric). Defaulted to `None`.
- [Optional] `display`: a str of either `"all"`, `"failing"` or `"passing"`, which allows you to selectively decide which type of test cases to display as the final result. Defaulted to `"all"`.
- [Optional] `show_indicator`: a boolean which when set to `True`, shows the evaluation progress indicator for each individual metric. Defaulted to `True`.
- [Optional] `print_results`: a boolean which when set to `True`, prints the result of each evaluation. Defaulted to `True`.
- [Optional] `file_output_dir`: a string which when set, will write the results of the evaluation to the specified directory. Defaulted to `None`.

### Error Configs

The `ErrorConfig` controls how error is handled in `evaluate()`.

```python
from deepeval.evaluate import ErrorConfig
from deepeval import evaluate

evaluate(error_config=ErrorConfig(), ...)
```

There are **TWO** optional parameters when creating an `ErrorConfig`:

- [Optional] `skip_on_missing_params`: a boolean which when set to `True`, skips all metric executions for test cases with missing parameters. Defaulted to `False`.
- [Optional] `ignore_errors`: a boolean which when set to `True`, ignores all exceptions raised during metrics execution for each test case. Defaulted to `False`.

If both `skip_on_missing_params` and `ignore_errors` are set to `True`, `skip_on_missing_params` takes precedence. This means that if a metric is missing required test case parameters, it will be skipped (and the result will be missing) rather than appearing as an ignored error in the final test run.

### Cache Configs

The `CacheConfig` controls the caching behavior of `evaluate()`.

```python
from deepeval.evaluate import CacheConfig
from deepeval import evaluate

evaluate(cache_config=CacheConfig(), ...)
```

There are **TWO** optional parameters when creating an `CacheConfig`:

- [Optional] `use_cache`: a boolean which when set to `True`, uses cached test run results instead. Defaulted to `False`.
- [Optional] `write_cache`: a boolean which when set to `True`, uses writes test run results to **DISK**. Defaulted to `True`.

The `write_cache` parameter writes to disk and so you should disable it if that is causing any errors in your environment.

## Flags for `deepeval test run`:

### Parallelization

Evaluate each test case in parallel by providing a number to the `-n` flag to specify how many processes to use.

```
deepeval test run test_example.py -n 4
```

### Cache

Provide the `-c` flag (with no arguments) to read from the local `deepeval` cache instead of re-evaluating test cases on the same metrics.

```
deepeval test run test_example.py -c
```

:::info
This is extremely useful if you're running large amounts of test cases. For example, lets say you're running 1000 test cases using `deepeval test run`, but you encounter an error on the 999th test case. The cache functionality would allow you to skip all the previously evaluated 999 test cases, and just evaluate the remaining one.
:::

### Ignore Errors

The `-i` flag (with no arguments) allows you to ignore errors for metrics executions during a test run. An example of where this is helpful is if you're using a custom LLM and often find it generating invalid JSONs that will stop the execution of the entire test run.

```
deepeval test run test_example.py -i
```

:::tip
You can combine different flags, such as the `-i`, `-c`, and `-n` flag to execute any uncached test cases in parallel while ignoring any errors along the way:

```python
deepeval test run test_example.py -i -c -n 2
```

:::

### Verbose Mode

The `-v` flag (with no arguments) allows you to turn on [`verbose_mode` for all metrics](/docs/metrics-introduction#debugging-a-metric) ran using `deepeval test run`. Not supplying the `-v` flag will default each metric's `verbose_mode` to its value at instantiation.

```python
deepeval test run test_example.py -v
```

:::note
When a metric's `verbose_mode` is `True`, it prints the intermediate steps used to calculate said metric to the console during evaluation.
:::

### Skip Test Cases

The `-s` flag (with no arguments) allows you to skip metric executions where the test case has missing//insufficient parameters (such as `retrieval_context`) that is required for evaluation. An example of where this is helpful is if you're using a metric such as the `ContextualPrecisionMetric` but don't want to apply it when the `retrieval_context` is `None`.

```
deepeval test run test_example.py -s
```

### Identifier

The `-id` flag followed by a string allows you to name test runs and better identify them on [Confident AI](https://confident-ai.com). An example of where this is helpful is if you're running automated deployment pipelines, have deployment IDs, or just want a way to identify which test run is which for comparison purposes.

```
deepeval test run test_example.py -id "My Latest Test Run"
```

### Display Mode

The `-d` flag followed by a string of "all", "passing", or "failing" allows you to display only certain test cases in the terminal. For example, you can display "failing" only if you only care about the failing test cases.

```
deepeval test run test_example.py -d "failing"
```

### Repeats

Repeat each test case by providing a number to the `-r` flag to specify how many times to rerun each test case.

```
deepeval test run test_example.py -r 2
```

### Hooks

`deepeval`'s Pytest integration allows you to run custom code at the end of each evaluation via the `@deepeval.on_test_run_end` decorator:

```python title="test_example.py"
...

@deepeval.on_test_run_end
def function_to_be_called_after_test_run():
    print("Test finished!")
```

## Model Settings 

Most model integrations in DeepEval can be configured either by:

- Passing explicit arguments to the model constructor (e.g. `OpenAIModel(api_key=...)`), or
- Setting environment variables (loaded through `Settings` / `.env`).

**Precedence:** explicit constructor arguments always win over environment variables.

### Common model configuration {#model-settings-common}

| Variable      | Values         | Effect |
| ------------- | -------------- | ------ |
| `TEMPERATURE` | float / unset  | Optional default temperature used when a model instance is created without an explicit `temperature=...` argument. |

### Provider selection flags {#model-settings-provider-selection}

Some parts of DeepEval can select a "default" LLM provider based on these flags:

| Variable                    | Values      | Effect |
| --------------------------- | ----------- | ------ |
| `USE_OPENAI_MODEL`          | `1` / unset | Prefer OpenAI as the default LLM provider (where applicable). |
| `USE_AZURE_OPENAI`          | `1` / unset | Prefer Azure OpenAI as the default LLM provider (where applicable). |
| `USE_AWS_BEDROCK_MODEL`     | `1` / unset | Prefer Amazon Bedrock as the default LLM provider (where applicable). |
| `USE_DEEPSEEK_MODEL`        | `1` / unset | Prefer DeepSeek as the default LLM provider (where applicable). |
| `USE_GEMINI_MODEL`          | `1` / unset | Prefer Gemini as the default LLM provider (where applicable). |
| `USE_GROK_MODEL`            | `1` / unset | Prefer Grok as the default LLM provider (where applicable). |
| `USE_LITELLM`               | `1` / unset | Prefer LiteLLM as the default LLM provider (where applicable). |
| `USE_LOCAL_MODEL`           | `1` / unset | Prefer the local model adapter as the default LLM provider (where applicable). |
| `USE_MOONSHOT_MODEL`        | `1` / unset | Prefer Moonshot as the default LLM provider (where applicable). |
| `USE_PORTKEY_MODEL`         | `1` / unset | Prefer Portkey as the default LLM provider (where applicable). |
| `USE_AZURE_OPENAI_EMBEDDING` | `1` / unset | Prefer Azure OpenAI embeddings as the default embeddings provider (where applicable). |
| `USE_LOCAL_EMBEDDINGS`      | `1` / unset | Prefer local embeddings as the default embeddings provider (where applicable). |

### AWS / Amazon Bedrock {#model-settings-aws-amazon-bedrock}

If `AWS_ACCESS_KEY_ID` and `AWS_SECRET_ACCESS_KEY` are not set, the AWS SDK default credentials chain is used.

| Variable                            | Values         | Effect |
| ----------------------------------- | -------------- | ------ |
| `AWS_ACCESS_KEY_ID`                 | string / unset | Optional AWS access key ID for authentication. |
| `AWS_SECRET_ACCESS_KEY`             | string / unset | Optional AWS secret access key for authentication. |
| `USE_AWS_BEDROCK_MODEL`             | `1` / unset    | Prefer Bedrock as the default LLM provider (where applicable). |
| `AWS_BEDROCK_MODEL_NAME`            | string / unset | Bedrock model ID (e.g. `anthropic.claude-3-opus-20240229-v1:0`). |
| `AWS_BEDROCK_REGION`                | string / unset | AWS region (e.g. `us-east-1`). |
| `AWS_BEDROCK_COST_PER_INPUT_TOKEN`  | float / unset  | Optional input-token cost used for cost reporting. |
| `AWS_BEDROCK_COST_PER_OUTPUT_TOKEN` | float / unset  | Optional output-token cost used for cost reporting. |

### Anthropic {#model-settings-anthropic}

| Variable                          | Values         | Effect |
| --------------------------------- | -------------- | ------ |
| `ANTHROPIC_API_KEY`               | string / unset | Anthropic API key. |
| `ANTHROPIC_MODEL_NAME`            | string / unset | Optional default Anthropic model name. |
| `ANTHROPIC_COST_PER_INPUT_TOKEN`  | float / unset  | Optional input-token cost used for cost reporting. |
| `ANTHROPIC_COST_PER_OUTPUT_TOKEN` | float / unset  | Optional output-token cost used for cost reporting. |

### Azure OpenAI {#model-settings-azure-openai}

| Variable                | Values         | Effect |
| ----------------------- | -------------- | ------ |
| `USE_AZURE_OPENAI`      | `1` / unset    | Prefer Azure OpenAI as the default LLM provider (where applicable). |
| `AZURE_OPENAI_API_KEY`  | string / unset | Azure OpenAI API key. |
| `AZURE_OPENAI_ENDPOINT` | string / unset | Azure OpenAI endpoint URL. |
| `OPENAI_API_VERSION`    | string / unset | Azure OpenAI API version. |
| `AZURE_DEPLOYMENT_NAME` | string / unset | Azure deployment name. |
| `AZURE_MODEL_NAME`      | string / unset | Optional Azure model name (for metadata / reporting). |
| `AZURE_MODEL_VERSION`   | string / unset | Optional Azure model version (for metadata / reporting). |

### OpenAI {#model-settings-openai}

| Variable                       | Values         | Effect |
| ------------------------------ | -------------- | ------ |
| `USE_OPENAI_MODEL`             | `1` / unset    | Prefer OpenAI as the default LLM provider (where applicable). |
| `OPENAI_API_KEY`               | string / unset | OpenAI API key. |
| `OPENAI_MODEL_NAME`            | string / unset | Optional default OpenAI model name. |
| `OPENAI_COST_PER_INPUT_TOKEN`  | float / unset  | Optional input-token cost used for cost reporting. |
| `OPENAI_COST_PER_OUTPUT_TOKEN` | float / unset  | Optional output-token cost used for cost reporting. |

### DeepSeek {#model-settings-deepseek}

| Variable                        | Values         | Effect |
| ------------------------------- | -------------- | ------ |
| `USE_DEEPSEEK_MODEL`            | `1` / unset    | Prefer DeepSeek as the default LLM provider (where applicable). |
| `DEEPSEEK_API_KEY`              | string / unset | DeepSeek API key. |
| `DEEPSEEK_MODEL_NAME`           | string / unset | Optional default DeepSeek model name. |
| `DEEPSEEK_COST_PER_INPUT_TOKEN` | float / unset  | Optional input-token cost used for cost reporting. |
| `DEEPSEEK_COST_PER_OUTPUT_TOKEN`| float / unset  | Optional output-token cost used for cost reporting. |

### Gemini {#model-settings-gemini}

| Variable                     | Values            | Effect |
| ---------------------------- | ----------------- | ------ |
| `USE_GEMINI_MODEL`           | `1` / unset       | Prefer Gemini as the default LLM provider (where applicable). |
| `GOOGLE_API_KEY`             | string / unset    | Google API key. |
| `GEMINI_MODEL_NAME`          | string / unset    | Optional default Gemini model name. |
| `GOOGLE_GENAI_USE_VERTEXAI`  | `1` / `0` / unset | If set, use Vertex AI via google-genai (where supported). |
| `GOOGLE_CLOUD_PROJECT`       | string / unset    | Optional GCP project (Vertex AI). |
| `GOOGLE_CLOUD_LOCATION`      | string / unset    | Optional GCP location/region (Vertex AI). |
| `GOOGLE_SERVICE_ACCOUNT_KEY` | string / unset    | Optional service account key (Vertex AI). |
| `VERTEX_AI_MODEL_NAME`       | string / unset    | Optional Vertex AI model name. |

### Grok {#model-settings-grok}

| Variable                    | Values         | Effect |
| --------------------------- | -------------- | ------ |
| `USE_GROK_MODEL`            | `1` / unset    | Prefer Grok as the default LLM provider (where applicable). |
| `GROK_API_KEY`              | string / unset | Grok API key. |
| `GROK_MODEL_NAME`           | string / unset | Optional default Grok model name. |
| `GROK_COST_PER_INPUT_TOKEN` | float / unset  | Optional input-token cost used for cost reporting. |
| `GROK_COST_PER_OUTPUT_TOKEN`| float / unset  | Optional output-token cost used for cost reporting. |

### LiteLLM {#model-settings-litellm}

| Variable                 | Values         | Effect |
| ------------------------ | -------------- | ------ |
| `USE_LITELLM`            | `1` / unset    | Prefer LiteLLM as the default LLM provider (where applicable). |
| `LITELLM_API_KEY`        | string / unset | Optional API key passed to LiteLLM. |
| `LITELLM_MODEL_NAME`     | string / unset | Default LiteLLM model name. |
| `LITELLM_API_BASE`       | string / unset | Optional base URL for the LiteLLM endpoint. |
| `LITELLM_PROXY_API_BASE` | string / unset | Optional proxy base URL (if using a proxy). |
| `LITELLM_PROXY_API_KEY`  | string / unset | Optional proxy API key (if using a proxy). |

### Local Model {#model-settings-local-model}

| Variable               | Values         | Effect |
| ---------------------- | -------------- | ------ |
| `USE_LOCAL_MODEL`      | `1` / unset    | Prefer the local model adapter as the default LLM provider (where applicable). |
| `LOCAL_MODEL_API_KEY`  | string / unset | Optional API key for the local model endpoint (if required). |
| `LOCAL_MODEL_NAME`     | string / unset | Optional default local model name. |
| `LOCAL_MODEL_BASE_URL` | string / unset | Base URL for the local model endpoint. |
| `LOCAL_MODEL_FORMAT`   | string / unset | Optional format hint for the local model integration. |

### Kimi (Moonshot) {#model-settings-kimi}

| Variable                        | Values         | Effect |
| ------------------------------- | -------------- | ------ |
| `USE_MOONSHOT_MODEL`            | `1` / unset    | Prefer Moonshot as the default LLM provider (where applicable). |
| `MOONSHOT_API_KEY`              | string / unset | Moonshot API key. |
| `MOONSHOT_MODEL_NAME`           | string / unset | Optional default Moonshot model name. |
| `MOONSHOT_COST_PER_INPUT_TOKEN` | float / unset  | Optional input-token cost used for cost reporting. |
| `MOONSHOT_COST_PER_OUTPUT_TOKEN`| float / unset  | Optional output-token cost used for cost reporting. |

### Ollama {#model-settings-ollama}

| Variable            | Values         | Effect |
| ------------------- | -------------- | ------ |
| `OLLAMA_MODEL_NAME` | string / unset | Optional default Ollama model name. |

### Portkey {#model-settings-portkey}

| Variable               | Values         | Effect |
| ---------------------- | -------------- | ------ |
| `USE_PORTKEY_MODEL`    | `1` / unset    | Prefer Portkey as the default LLM provider (where applicable). |
| `PORTKEY_API_KEY`      | string / unset | Portkey API key. |
| `PORTKEY_MODEL_NAME`   | string / unset | Optional default model name passed to Portkey. |
| `PORTKEY_BASE_URL`     | string / unset | Optional Portkey base URL. |
| `PORTKEY_PROVIDER_NAME`| string / unset | Optional provider name (Portkey routing). |

### Embeddings {#model-settings-embeddings}

| Variable                         | Values         | Effect |
| -------------------------------- | -------------- | ------ |
| `USE_AZURE_OPENAI_EMBEDDING`     | `1` / unset    | Prefer Azure OpenAI embeddings as the default embeddings provider (where applicable). |
| `AZURE_EMBEDDING_DEPLOYMENT_NAME`| string / unset | Azure embedding deployment name. |
| `USE_LOCAL_EMBEDDINGS`           | `1` / unset    | Prefer local embeddings as the default embeddings provider (where applicable). |
| `LOCAL_EMBEDDING_API_KEY`        | string / unset | Optional API key for the local embeddings endpoint (if required). |
| `LOCAL_EMBEDDING_MODEL_NAME`     | string / unset | Optional default local embedding model name. |
| `LOCAL_EMBEDDING_BASE_URL`       | string / unset | Base URL for the local embeddings endpoint. |
