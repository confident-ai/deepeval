---
id: tutorial-dataset-confident
title: Pushing Datasets 
sidebar_label: Pushing Datasets to Confident AI
---

In the previous section, we generated multiple synthetic datasets. The next step is to finalize these datasets for evaluation by **pushing them to Confident AI** and reviewing each test case. 

:::note
In this tutorial, weâ€™ll walk through the process of pushing and pulling datasets from Confident AI and reviewing them directly on the platform.
:::

## Pushing Your Dataset

To push your dataset to Confident AI, simply provide an alias (dataset name) and call the `push` method on the dataset. Optionally, you can use the `overwrite` parameter to replace an existing dataset with the same alias if it has already been pushed.

```python
dataset.push(alias="Synthetic Test", overwrite=False)
```

## Reviewing your Dataset

You can easy review synthetically generated datasets on Confident AI. To get started, simply head to the datasets page on the platform and select the dataset you're interested in reviewing.

:::tip
Having a centralized dataset management system is particularly important for **larger teams with non-technical members** (such as domain experts or human reviewers). 
:::

<div
  style={{
    display: "flex",
    alignItems: "center",
    justifyContent: "center",
  }}
>
  <img
    src="https://confident-bucket.s3.amazonaws.com/tutorial_datasets_01.png"
    alt="Datasets 1"
    style={{
      marginTop: "20px",
      marginBottom: "20px",
      height: "auto",
      maxHeight: "800px",
    }}
  />
</div>

Confident AI allows project members to edit each golden directly on the platform. This includes all the golden fields, such as **input**, **actual output**, **expected output**, **context**, and **retrieval context**.

:::info
If you have domain experts, they will primarily focus on discussing and refining the **expected output** for specific use queries.
:::

You can also toggle whether each golden is finalized or not to notify other team members that a golden still needs reviewing. As a best practice, a dataset should only be ready for evaluation once all test cases are reviewed and marked as finalized.

<div
  style={{
    display: "flex",
    alignItems: "center",
    justifyContent: "center",
  }}
>
  <img
    src="https://confident-bucket.s3.amazonaws.com/tutorial_datasets_02.png"
    alt="Datasets 2"
    style={{
      marginBottom: "20px",
      height: "auto",
      maxHeight: "800px",
    }}
  />
</div>

To view each test case's parameters in the side panel, you can click on the inspect icon (pen icon) for any test case. Additionally, you can leave comments for other team members directly within the side panel.

<div
  style={{
    display: "flex",
    alignItems: "center",
    justifyContent: "center",
  }}
>
  <img
    src="https://confident-bucket.s3.amazonaws.com/tutorial_datasets_03.png"
    alt="Datasets 3"
    style={{
      height: "auto",
      maxHeight: "800px",
    }}
  />
</div>

## Pulling Your Dataset

Once the dataset review is complete and all test cases are finalized, engineers can easily pull the entire dataset with a single line of code and begin evaluating at scale.

```python
dataset.pull(alias="Synthetic Test")
