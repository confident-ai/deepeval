---
id: metrics-recall
title: Recall
sidebar_label: Recall
---

<head>
  <link
    rel="canonical"
    href="https://deepeval.com/docs/metrics-recall"
  />
</head>

import Equation from "@site/src/components/Equation";
import MetricTagsDisplayer from "@site/src/components/MetricTagsDisplayer";

<MetricTagsDisplayer singleTurn={true} usesLLMs={false} referenceless={false} />

The Recall metric measures how many tokens in the `expected_output` were correctly captured in your application's `actual_output`. It captures the **completeness** of the response.

:::note
The `RecallMetric` does **not** rely on an LLM for evaluation. It purely compares the outputs on a lexical level (string or token-level comparison).
:::

## Required Arguments

To use the `RecallMetric`, you'll have to provide the following arguments when creating an [`LLMTestCase`](/docs/evaluation-test-cases#llm-test-case):

- `input`
- `actual_output`
- `expected_output`

Read the [How Is It Calculated](#how-is-it-calculated) section below to learn how test case parameters are used for metric calculation.

## Usage

```python
from deepeval import evaluate
from deepeval.metrics import RecallMetric
from deepeval.test_case import LLMTestCase

metric = RecallMetric(
    threshold=0.5,
    verbose_mode=True,
)

test_case = LLMTestCase(
    input="Translate 'Hello' to French",
    actual_output="Bonjour le monde",
    expected_output="Bonjour"
)

# To run metric as a standalone
# metric.measure(test_case)
# print(metric.score, metric.reason)

evaluate(test_cases=[test_case], metrics=[metric])
```

There are **TWO** optional parameters when creating an `RecallMetric`:

- [Optional] `threshold`: a float representing the minimum passing threshold, defaulted to 0.5.
- [Optional] `verbose_mode`: a boolean which when set to `True`, prints the intermediate steps used to calculate said metric to the console, as outlined in the [How Is It Calculated](#how-is-it-calculated) section. Defaulted to `False`.

### As a Standalone

You can also run the `RecallMetric` on a single test case as a standalone, one-off execution.

```python
...

metric.measure(test_case)
print(metric.score, metric.reason)
```

## How Is It Calculated?

The `RecallMetric` score is calculated according to the following equation:

<Equation formula="\text{Recall} = \frac{\text{Num of overlapping tokens}}{\text{Num of tokens in expected output}}" />

A higher recall means the model captured more of the relevant expected content.
