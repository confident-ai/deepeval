---
id: metrics-precision
title: Precision
sidebar_label: Precision
---

<head>
  <link
    rel="canonical"
    href="https://deepeval.com/docs/metrics-precision"
  />
</head>

import Equation from "@site/src/components/Equation";
import MetricTagsDisplayer from "@site/src/components/MetricTagsDisplayer";

<MetricTagsDisplayer singleTurn={true} usesLLMs={false} referenceless={false} />

The Precision metric measures how many tokens in your application's `actual_output` are **correct** with respect to the `expected_output`. It captures the **exactness** of the generated response.

:::note
The `PrecisionMetric` does **not** rely on an LLM for evaluation. It purely compares the outputs on a lexical level (string or token-level comparison).
:::

## Required Arguments

To use the `PrecisionMetric`, you'll have to provide the following arguments when creating an [`LLMTestCase`](/docs/evaluation-test-cases#llm-test-case):

- `input`
- `actual_output`
- `expected_output`

Read the [How Is It Calculated](#how-is-it-calculated) section below to learn how test case parameters are used for metric calculation.

## Usage

```python
from deepeval import evaluate
from deepeval.metrics import PrecisionMetric
from deepeval.test_case import LLMTestCase

metric = PrecisionMetric(
    threshold=0.5,
    verbose_mode=True,
)

test_case = LLMTestCase(
    input="Translate 'Hello' to French",
    actual_output="Bonjour le monde",
    expected_output="Bonjour"
)

# To run metric as a standalone
# metric.measure(test_case)
# print(metric.score, metric.reason)

evaluate(test_cases=[test_case], metrics=[metric])
```

There are **TWO** optional parameters when creating an `PrecisionMetric`:

- [Optional] `threshold`: a float representing the minimum passing threshold, defaulted to 0.5.
- [Optional] `verbose_mode`: a boolean which when set to `True`, prints the intermediate steps used to calculate said metric to the console, as outlined in the [How Is It Calculated](#how-is-it-calculated) section. Defaulted to `False`.

### As a Standalone

You can also run the `PrecisionMetric` on a single test case as a standalone, one-off execution.

```python
...

metric.measure(test_case)
print(metric.score, metric.reason)
```

## How Is It Calculated?

The `PrecisionMetric` score is calculated according to the following equation:

<Equation formula="\text{Precision} = \frac{\text{Num of overlapping tokens}}{\text{Num of tokens in actual output}}" />

The score ranges from 0 to 1, where 1 indicates all tokens in `expected_output` are present in `actual_output`.
