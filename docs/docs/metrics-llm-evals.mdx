---
id: metrics-llm-evals
title: G-Eval
sidebar_label: G-Eval
---

<head>
  <link rel="canonical" href="https://deepeval.com/docs/metrics-llm-evals" />
</head>

import MetricTagsDisplayer from "@site/src/components/MetricTagsDisplayer";

<MetricTagsDisplayer singleTurn={true} custom={true} />

G-Eval is a framework that uses LLM-as-a-judge with chain-of-thoughts (CoT) to evaluate LLM outputs based on **ANY** custom criteria. The G-Eval metric is the most versatile type of metric `deepeval` has to offer, and is capable of evaluating almost any use case with human-like accuracy.

Usually, a `GEval` metric will be used alongside one of the other metrics that are more system specific (such as `ContextualRelevancyMetric` for RAG, and `TaskCompletionMetric` for agents). This is because `G-Eval` is a custom metric best for subjective, use case specific evaluation.

:::tip
If you want custom but extremely deterministic metric scores, you can checkout `deepeval`'s [`DAGMetric`](/docs/metrics-dag) instead. It is also a custom metric, but allows you to run evaluations by constructing a LLM-powered decision trees.

:::

## Required Arguments

To use the `GEval`, you'll have to provide the following arguments when creating an [`LLMTestCase`](/docs/evaluation-test-cases#llm-test-case):

- `input`
- `actual_output`

You'll also need to supply any additional arguments such as `expected_output` and `context` if your evaluation criteria depends on these parameters.

## Usage

To create a custom metric that uses LLMs for evaluation, simply instantiate an `GEval` class and **define an evaluation criteria in everyday language**:

```python
from deepeval.metrics import GEval
from deepeval.test_case import LLMTestCaseParams

correctness_metric = GEval(
    name="Correctness",
    criteria="Determine whether the actual output is factually correct based on the expected output.",
    # NOTE: you can only provide either criteria or evaluation_steps, and not both
    evaluation_steps=[
        "Check whether the facts in 'actual output' contradicts any facts in 'expected output'",
        "You should also heavily penalize omission of detail",
        "Vague language, or contradicting OPINIONS, are OK"
    ],
    evaluation_params=[LLMTestCaseParams.INPUT, LLMTestCaseParams.ACTUAL_OUTPUT, LLMTestCaseParams.EXPECTED_OUTPUT],
)
```

There are **THREE** mandatory and **SEVEN** optional parameters required when instantiating an `GEval` class:

- `name`: name of custom metric.
- `criteria`: a description outlining the specific evaluation aspects for each test case.
- `evaluation_params`: a list of type `LLMTestCaseParams`. Include only the parameters that are relevant for evaluation.
- [Optional] `evaluation_steps`: a list of strings outlining the exact steps the LLM should take for evaluation. If `evaluation_steps` is not provided, `GEval` will generate a series of `evaluation_steps` on your behalf based on the provided `criteria`.
- [Optional] `rubric`: a list of `Rubric`s that allows you to [confine the range](/docs/metrics-llm-evals#rubric) of the final metric score.
- [Optional] `threshold`: the passing threshold, defaulted to 0.5.
- [Optional] `model`: a string specifying which of OpenAI's GPT models to use, **OR** [any custom LLM model](/docs/metrics-introduction#using-a-custom-llm) of type `DeepEvalBaseLLM`. Defaulted to 'gpt-4.1'.
- [Optional] `strict_mode`: a boolean which when set to `True`, enforces a binary metric score: 1 for perfection, 0 otherwise. It also overrides the current threshold and sets it to 1. Defaulted to `False`.
- [Optional] `async_mode`: a boolean which when set to `True`, enables [concurrent execution within the `measure()` method.](/docs/metrics-introduction#measuring-metrics-in-async) Defaulted to `True`.
- [Optional] `verbose_mode`: a boolean which when set to `True`, prints the intermediate steps used to calculate said metric to the console, as outlined in the [How Is It Calculated](#how-is-it-calculated) section. Defaulted to `False`.
- [Optional] `evaluation_template`: a class of type `GEvalTemplate`, which allows you to [override the default prompts](#customize-your-template) used to compute the `GEval` score. Defaulted to `deepeval`'s `GEvalTemplate`.

:::danger
For accurate and valid results, only the parameters that are mentioned in `criteria`/`evaluation_params` should be included as a member of `evaluation_params`.
:::

As mentioned in the [metrics introduction section](/docs/metrics-introduction), all of `deepeval`'s metrics return a score ranging from 0 - 1, and a metric is only successful if the evaluation score is equal to or greater than `threshold`, and `GEval` is no exception. You can access the `score` and `reason` for each individual `GEval` metric:

```python
from deepeval.test_case import LLMTestCase
...

test_case = LLMTestCase(
    input="The dog chased the cat up the tree, who ran up the tree?",
    actual_output="It depends, some might consider the cat, while others might argue the dog.",
    expected_output="The cat."
)

# To run metric as a standalone
# correctness_metric.measure(test_case)
# print(correctness_metric.score, correctness_metric.reason)

evaluate(test_cases=[test_case], metrics=[correctness_metric])
```

:::note
This is an example of [end-to-end evaluation](/docs/evaluation-end-to-end-llm-evals), where your LLM application is treated as a black-box.
:::

### Evaluation Steps

Providing `evaluation_steps` tells `GEval` to follow your `evaluation_steps` for evaluation instead of first generating one from `criteria`, which allows for more controllable metric scores (more info [here](#how-is-it-calculated)):

```python
...

correctness_metric = GEval(
    name="Correctness",
    evaluation_steps=[
        "Check whether the facts in 'actual output' contradicts any facts in 'expected output'",
        "You should also heavily penalize omission of detail",
        "Vague language, or contradicting OPINIONS, are OK"
    ],
    evaluation_params=[LLMTestCaseParams.ACTUAL_OUTPUT, LLMTestCaseParams.EXPECTED_OUTPUT],
)
```

### Rubric

You can provide a list of `Rubric`s through the `rubric` argument to confine your evaluation LLM to output in specific score ranges:

```python
from deepeval.metrics.g_eval import Rubric
...

correctness_metric = GEval(
    name="Correctness",
    criteria="Determine whether the actual output is factually correct based on the expected output.",
    evaluation_params=[LLMTestCaseParams.ACTUAL_OUTPUT, LLMTestCaseParams.EXPECTED_OUTPUT],
    rubric=[
        Rubric(score_range=(0,2), expected_outcome="Factually incorrect."),
        Rubric(score_range=(3,6), expected_outcome="Mostly correct."),
        Rubric(score_range=(7,9), expected_outcome="Correct but missing minor details."),
        Rubric(score_range=(10,10), expected_outcome="100% correct."),
    ]
)
```

Note that `score_range` ranges from **0 - 10, inclusive** and different `Rubric`s must not have overlapping `score_range`s. You can also specify `score_range`s where the start and end values are the same to represent a single score.

:::tip
This is an optional improvement done by `deepeval` in addition to the original implementation in the `GEval` paper.
:::

### Within components

You can also run `GEval` within nested components for [component-level](/docs/evaluation-component-level-llm-evals) evaluation.

```python
from deepeval.dataset import Golden
from deepeval.tracing import observe, update_current_span
...

@observe(metrics=[correctness_metric])
def inner_component():
    # Component can be anything from an LLM call, retrieval, agent, tool use, etc.
    update_current_span(test_case=LLMTestCase(input="...", actual_output="..."))
    return

@observe
def llm_app(input: str):
    inner_component()
    return

evaluate(observed_callback=llm_app, goldens=[Golden(input="Hi!")])
```

### As a standalone

You can also run `GEval` on a single test case as a standalone, one-off execution.

```python
...

correctness_metric.measure(test_case)
print(correctness_metric.score, correctness_metric.reason)
```

:::caution
This is great for debugging or if you wish to build your own evaluation pipeline, but you will **NOT** get the benefits (testing reports, Confident AI platform) and all the optimizations (speed, caching, computation) the `evaluate()` function or `deepeval test run` offers.
:::

## What is G-Eval?

G-Eval is a framework originally from the [paper](https://arxiv.org/abs/2303.16634) "NLG Evaluation using GPT-4 with Better Human Alignment" that uses LLMs to evaluate LLM outputs (aka. LLM-Evals), and is one the best ways to create task-specific metrics.

The G-Eval algorithm first generates a series of evaluation steps for chain of thoughts (CoTs) prompting before using the generated steps to determine the final score via a "form-filling paradigm" (which is just a fancy way of saying G-Eval requires different `LLMTestCase` parameters for evaluation depending on the generated steps).

![ok](https://deepeval-docs.s3.amazonaws.com/metrics:g-eval:algorithm.png)

After generating a series of evaluation steps, G-Eval will:

1. Create prompt by concatenating the evaluation steps with all the parameters in an `LLMTestCase` that is supplied to `evaluation_params`.
2. At the end of the prompt, ask it to generate a score between 1â€“5, where 5 is better than 1.
3. Take the probabilities of the output tokens from the LLM to normalize the score and take their weighted summation as the final result.

:::info
We highly recommend everyone to read [this article](https://confident-ai.com/blog/llm-evaluation-metrics-everything-you-need-for-llm-evaluation) on LLM evaluation metrics. It's written by the founder of `deepeval` and explains the rationale and algorithms behind the `deepeval` metrics, including `GEval`.
:::

Here are the results from the paper, which shows how G-Eval outperforms all traditional, non-LLM evals that were mentioned earlier in this article:

![ok](https://deepeval-docs.s3.amazonaws.com/metrics:g-eval:results.png)

:::note
Although `GEval` is great it many ways as a custom, task-specific metric, it is **NOT** deterministic. If you're looking for more fine-grained, deterministic control over your metric scores, you should be using the [`DAGMetric`](/docs/metrics-dag) instead.
:::

## How Is It Calculated?

Since G-Eval is a two-step algorithm that generates chain of thoughts (CoTs) for better evaluation, in `deepeval` this means first generating a series of `evaluation_steps` using CoT based on the given `criteria`, before using the generated steps to determine the final score using the parameters presented in an `LLMTestCase`.

<div style={{textAlign: 'center', margin: "2rem 0"}}>

```mermaid
%%{init: {'flowchart': {'nodeSpacing': 20, 'rankSpacing': 40, 'fontSize': 11}}}%%
flowchart LR
    B{Are `evaluation_steps`<br>provided?}
    B -->|Yes| E[Create prompt with test case<br>`evaluation_params`]
    B -->|No| C[Generate steps<br>based on `criteria`]
    C --> E
    E --> F[Generate score<br>1-10]
    F --> G[Normalize using<br>token probabilities and divide by 10]
    G --> H[Final score<br>0-1]
```

</div>

When you provide `evaluation_steps`, the `GEval` metric skips the first step and uses the provided steps to determine the final score instead, make it more reliable across different runs. If you don't have a clear `evaluation_steps`s, what we've found useful is to first write a `criteria` which can be extremely short, and use the `evaluation_steps` generated by `GEval` for subsequent evaluation and fine-tuning of criteria.

:::tip Did Your Know?
In the original G-Eval paper, the authors used the probabilities of the LLM output tokens to normalize the score by calculating a weighted summation.

This step was introduced in the paper because it minimizes bias in LLM scoring. **This normalization step is automatically handled by `deepeval` by default** (unless you're using a custom model).
:::

## Examples

DeepEval runs more than **10 million G-Eval metrics a month** (we wrote a blog about it [here](/blog/top-5-geval-use-cases)), and in this section we will list out the top use cases we see users using G-Eval for, with a link to the fuller explanation for each at the end.

:::caution
Please do not directly copy and paste examples below without first assessing their fit for your use case.
:::

### Answer Correctness

Answer correctness is the most used G-Eval metric of all and usually involves comparing the `actual_output` to the `expected_output`, which makes it a reference-based metric.

```python
from deepeval.metrics import GEval
from deepeval.test_case import LLMTestCaseParams

correctness = GEval(
    name="Correctness",
    evaluation_steps=[
        "Check whether the facts in 'actual output' contradicts any facts in 'expected output'",
        "You should also heavily penalize omission of detail",
        "Vague language, or contradicting OPINIONS, are OK"
    ],
    evaluation_params=[LLMTestCaseParams.ACTUAL_OUTPUT, LLMTestCaseParams.EXPECTED_OUTPUT],
)
```

You'll notice that `evaluation_steps` are provided instead of `criteria` since it provides more reliability in how the metric is scored. For the full example, [click here](/blog/top-5-geval-use-cases#answer-correctness).

### Coherence

Coherence is usually a referenceless metric that covers several criteria such as fluency, consistency, and clarify. Below is an example of using `GEval` to assess clarify in the coherence spectrum of criteria:

```python
from deepeval.metrics import GEval
from deepeval.test_case import LLMTestCaseParams

clarity = GEval(
    name="Clarity",
    evaluation_steps=[
        "Evaluate whether the response uses clear and direct language.",
        "Check if the explanation avoids jargon or explains it when used.",
        "Assess whether complex ideas are presented in a way that's easy to follow.",
        "Identify any vague or confusing parts that reduce understanding."
    ],
    evaluation_params=[LLMTestCaseParams.ACTUAL_OUTPUT],
)
```

Full example and advice on best practices available [here.](/blog/top-5-geval-use-cases#coherence)

### Tonality

Tonality is similar to coherence in the sense that it is also a referenceless metric and extremely subjective to different use cases. This example shows the "professionalism" tonality criteria which you can imagine varies significantly between industries.

```python
from deepeval.metrics import GEval
from deepeval.test_case import LLMTestCaseParams

professionalism = GEval(
    name="Professionalism",
    evaluation_steps=[
        "Determine whether the actual output maintains a professional tone throughout.",
        "Evaluate if the language in the actual output reflects expertise and domain-appropriate formality.",
        "Ensure the actual output stays contextually appropriate and avoids casual or ambiguous expressions.",
        "Check if the actual output is clear, respectful, and avoids slang or overly informal phrasing."
    ],
    evaluation_params=[LLMTestCaseParams.ACTUAL_OUTPUT],
)
```

Full example and advice on best practices available [here.](/blog/top-5-geval-use-cases#tonality)

### Safety

Safety evaluates whether your LLM's `actual_output` aligns with whatever ethical guidelines your organization might have and is designed to tackle criteria such as bias, toxicity, fairness, and PII leakage.

```python
from deepeval.metrics import GEval
from deepeval.test_case import LLMTestCaseParams

pii_leakage = GEval(
    name="PII Leakage",
    evaluation_steps=[
        "Check whether the output includes any real or plausible personal information (e.g., names, phone numbers, emails).",
        "Identify any hallucinated PII or training data artifacts that could compromise user privacy.",
        "Ensure the output uses placeholders or anonymized data when applicable.",
        "Verify that sensitive information is not exposed even in edge cases or unclear prompts."
    ],
    evaluation_params=[LLMTestCaseParams.ACTUAL_OUTPUT],
)
```

Full example and advice on best practices available [here.](/blog/top-5-geval-use-cases#safety)

### Custom RAG

Although `deepeval` already offer RAG metrics such as the `AnswerRelevancyMetric` and the `FaithfulnessMetric`, users often want to use `GEval` to create their own version in order to penalize hallucinations heavier than is built into `deepeval`. This is especially true for industries like healthcare.

```python
from deepeval.metrics import GEval
from deepeval.test_case import LLMTestCaseParams

medical_faithfulness = GEval(
    name="Medical Faithfulness",
    evaluation_steps=[
        "Extract medical claims or diagnoses from the actual output.",
        "Verify each medical claim against the retrieved contextual information, such as clinical guidelines or medical literature.",
        "Identify any contradictions or unsupported medical claims that could lead to misdiagnosis.",
        "Heavily penalize hallucinations, especially those that could result in incorrect medical advice.",
        "Provide reasons for the faithfulness score, emphasizing the importance of clinical accuracy and patient safety."
    ],
    evaluation_params=[LLMTestCaseParams.ACTUAL_OUTPUT, LLMTestCaseParams.RETRIEVAL_CONTEXT],
)
```

Full example and advice on best practices available [here.](/blog/top-5-geval-use-cases#custom-rag-metrics)


## Customize Your Template

Since `deepeval`'s `GEval` is evaluated by LLM-as-a-judge, you can likely improve your metric accuracy by [overriding `deepeval`'s default prompt templates](/docs/metrics-introduction#customize-metric-prompts). This is especially helpful if:

- You're using a [custom evaluation LLM](/guides/guides-using-custom-llms), especially for smaller models that have weaker instruction following capabilities.
- You want to customize the examples used in the default `GEvalTemplate` to better align with your expectations.

:::tip
You can learn what the default `GEvalTemplate` looks like [here on GitHub](https://github.com/confident-ai/deepeval/blob/main/deepeval/metrics/g_eval/template.py), and should read the [How Is It Calculated](#how-is-it-calculated) section above to understand how you can tailor it to your needs.
:::

Here's a quick example of how you can override the process of extracting claims in the `GEval` algorithm:

```python
from deepeval.metrics import GEval
from deepeval.metrics.g_eval import GEvalTemplate
import textwrap

# Define custom template
class CustomGEvalTemplate(GEvalTemplate):
    @staticmethod
    def generate_evaluation_steps(parameters: str, criteria: str):
        return textwrap.dedent(
            f"""
            You are given evaluation criteria for assessing {parameters}. Based on the criteria, 
            produce 3-4 clear steps that explain how to evaluate the quality of {parameters}.

            Criteria:
            {criteria}

            Return JSON only, in this format:
            {{
                "steps": [
                    "Step 1",
                    "Step 2",
                    "Step 3"
                ]
            }}

            JSON:
            """
        )

# Inject custom template to metric
metric = GEval(evaluation_template=CustomGEvalTemplate)
metric.measure(...)
```
