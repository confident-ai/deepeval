---
# id: ollama
title: Ollama
sidebar_label: Ollama
---

DeepEval allows you to use any model from Ollama to run evals, either through the CLI or directly in python.

:::note
Before getting started, make sure your Ollama model is installed and running. See the full list of available models [here](<](https://ollama.com/search)>).

```bash
ollama run deepseek-r1:1.5b
```

:::

### Command Line

To configure your Ollama model through the CLI, run the following command. Replace `deepseek-r1:1.5b` with any Ollama-supported model of your choice.

```bash
deepeval set-ollama deepseek-r1:1.5b
```

You may also specify the **base URL** of your local Ollama model instance if you've defined a custom port. By default, the base URL is set to `http://localhost:11434`.

```bash
deepeval set-ollama deepseek-r1:1.5b \
    --base-url="http://localhost:11434"
```

:::info
The CLI command above sets Ollama as the default provider for all metrics, unless overridden in Python code. To use a different default model provider, you must first unset Ollama:

```bash
deepeval unset-ollama
```

:::

### Python

Alternatively, you can specify your model directly in code using `OllamaModel` from DeepEval's model collection.

```python
from deepeval.models import OllamaModel
from deepeval.metrics import AnswerRelevancyMetric

model = OllamaModel(
    model="deepseek-r1:1.5b",
    base_url="http://localhost:11434",
    temperature=0
)

answer_relevancy = AnswerRelevancyMetric(model=model)
```

There are **FIVE** mandatory and **ONE** optional parameters when creating an `AzureOpenAIModel`:

- `model`: A string specifying the name of the Ollama model to use.
- [Optional] `base_url`: A string specifying the base URL of the Ollama server. Defaulted to `'http://localhost:11434'`.
- [Optional] `temperature`: A float specifying the model temperature. Defaulted to 0.
- [Optional] `generation_kwargs`: A dictionary of additional generation parameters supported by OpenAI models, such as:
    - `top_p`: Controls nucleus sampling (probability mass for token selection).
    - `max_tokens`: Maximum number of tokens in the generated response.
    - `stop`: A string or list of strings where the generation will stop.
    - `presence_penalty`: Penalizes new tokens based on whether they appear in the text so far (reduces repetition).
    - `frequency_penalty`: Penalizes new tokens based on their existing frequency in the text (reduces frequent token usage).
    - `logit_bias`: A dictionary mapping token IDs to bias values to increase or decrease likelihood.
    - `n`: Number of completions to generate per prompt.
    - `seed`: Seed for reproducibility (when supported).
    - `tools` / `tool_choice`: Parameters related to tool usage for tool-using models.
    - `logprobs`, `top_logprobs`: Return token-level log probabilities (if supported).

### Available Ollama Models

:::note
This list only displays some of the available models. For a comprehensive list, refer to the Ollama's official documentation.
:::

Below is a list of commonly used Ollama models:

- `deepseek-r1`
- `llama3.1`
- `gemma`
- `qwen`
- `mistral`
- `codellama`
- `phi3`
- `tinyllama`
- `starcoder2`
