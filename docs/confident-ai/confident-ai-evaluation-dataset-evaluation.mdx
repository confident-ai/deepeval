---
id: confident-ai-evaluation-dataset-evaluation
title: Using Datasets
sidebar_label: Using Datasets
---

:::note
A more comprehensive walkthrough on how to use datasets for LLM evaluation can be found in the [evaluation section](/confident-ai/confident-ai-testing-n-evaluation-test-run), but this is a great introduction nevertheless.
:::

You can pull evaluation datasets created on Confident AI like how you would pull code from a GitHub repo by specifying the dataset `alias`, which the unique name of your dataset within a project.

```python
from deepeval.dataset import EvaluationDataset

dataset = EvaluationDataset()
dataset.pull(alias="QA Dataset", auto_convert_goldens_to_test_cases=True)
print(dataset)
```

Once the dataset is pulled **locally**, you can start running evaluations using [`deepeval`'s metrics](/docs/metrics-introduction), which is **also ran locally:**

```python
from deepeval.metrics import AnswerRelevancyMetric
from deepeval import evaluate
...

evaluate(dataset, metrics=[AnswerRelevancyMetric()])
```

We're just showing the simplest example here, and this code assumes your dataset already have pre-computed `actual_output`s. In reality, you will likely have to generate `actual_output`s before running evaluations as this is the whole point of testing and evaluation.

:::tip
If you want to run metrics on the cloud instead (although not recommended), [click here.](/confident-ai/confident-ai-testing-n-evaluation-metric-collections) We don't recommend running metrics on the cloud unless you have to because running evals locally gives you more control in your code to determine what metrics to customize, and how your LLM application wish to generate these `actual_output`s at evaluation time.
:::

When you pull a dataset from Confident AI, you are pulling **raw goldens**, not test cases. For those of you where your dataset does not have pre-computed `actual_output`, `retrieval_context`, and/or `tools_called`, you'll need an additional step to generate these parameters at evaluation time based on the `input` of each golden.

In this case, `auto_convert_goldens_to_test_cases` should be set to `False` (which is the default value).

## Pull Your Dataset

Pull datasets from Confident by specifying its `alias`:

```python
from deepeval.dataset import EvaluationDataset

# Initialize empty dataset object
dataset = EvaluationDataset()

# Pull from Confident
dataset.pull(alias="QA Dataset")
```

An `EvaluationDataset` accepts **ONE** mandatory and **ONE** optional argument:

- `alias`: the alias of your dataset on Confident. A dataset alias is unique for each project.
- [Optional] `auto_convert_goldens_to_test_cases`: When set to `True`, `dataset.pull()` will automatically convert all goldens that were fetched from Confident into test cases and override all test cases you currently have in your `EvaluationDataset` instance. Defaulted to `False`.

:::info

Essentially, `auto_convert_goldens_to_test_cases` is convenient if you have a complete, pre-computed dataset on Confident ready for evaluation. However, this might not always be the case. To disable the automatic conversion of goldens to test cases within your dataset, set `auto_convert_goldens_to_test_cases` to `False`. You might find this useful if you:

- have goldens in your `EvaluationDataset` that are missing essential components, such as the `actual_output`, to be converted to test cases for evaluation. This may be the case if you're looking to generate `actual_output`s at evaluation time. Remember, a golden does not require an `actual_output`, but a test case does.
- have extra data preprocessing to do before converting goldens to test cases. Remember, goldens have an extra `additional_metadata` field, which is a dictionary that contains additional metadata for you to generate custom outputs.

:::

## Convert Goldens Into Test Cases

Here is an example of how you can use goldens as an intermediary variable to **generate** an `actual_output` before converting them to test cases for evaluation:

```python
# Pull from Confident
dataset.pull(alias="QA Dataset")
```

Then, process the goldens and convert them into test cases:

```python
# A hypothetical LLM application example
from chatbot import query
from typing import List
from deepeval.test_case import LLMTestCase
from deepeval.dataset import Golden
...

def convert_goldens_to_test_cases(goldens: List[Golden]) -> List[LLMTestCase]:
    test_cases = []
    for golden in goldens:
        test_case = LLMTestCase(
            input=golden.input,
            # Generate actual output using the 'input' and 'additional_metadata'
            actual_output=query(golden.input, golden.additional_metadata),
            expected_output=golden.expected_output,
            context=golden.context,
        )
        test_cases.append(test_case)
    return test_cases

# Data preprocessing before setting the dataset test cases
dataset.test_cases = convert_goldens_to_test_cases(dataset.goldens)
```

:::caution
**PLEASE DO NOT** create a list of test cases and **not** add it to a dataset (ie. always do `dataset.test_cases = ...`). Adding test cases to a dataset allows `deepeval` to preserve the ordering of `Goldens` stored in your dataset, which allows for regression testing, and better management of testing data.
:::

## Running Evaluations With Datasets

To use the dataset in the previous example, choose your metrics metric(s) as laid out in previous sections in this documentation and simply call the `evaluate` function:

```python
from deepeval.metrics import HallucinationMetric
...

evaluate(dataset, metrics=[HallucinationMetric()])
```

You'll definitely want to reuse the same dataset to test a certain functionality of your LLM use case, as it is the only valid way of conducting experiments to determine which implementation of your LLM application performs best. For example, if you're building a Text-SQL, you'll probably need a different dataset for each different database you're building text-sql for.

:::tip

Don't forget to login, as you'll only be able to see evaluation results/test runs on Confident AI if you've supplied your Confident AI API Key.

```console
deepeval login
```

:::

The idea is for the same set of `input`s, what are the `actual_output`s that are being generated looks like, and based on these metric results, we'll be able to compare benchmarks to see which LLM implementation performs best.

### In CI/CD Pipelines

You can also incorporate testing in CI/CD pipelines using `deepeval`'s extensive Pytest integrations. You can also execute run a test run in CI/CD pipelines by executing `deepeval test run` to regression test your LLM application using the dataset you've defined on Confident AI.

:::note
In [later sections](/confident-ai/confident-ai-testing-n-evaluation-unit-testing), you'll also get the full version of how to unit-test in CI/CD pipelines, so feel free to skip this and save it for later.
:::

Here, we'll be using GitHub Actions as an example and first let's create a test file, `test_llm.py`:

```python title="test_llm.py"
from deepeval.test_case import LLMTestCase
from deepeval.dataset import EvaluationDataset
from deepeval.metrics import AnswerRelevancyMetric
from deepeval import assert_test

...
dataset = EvaluationDataset()
dataset.pull(alias="Your Dataset Alias")

# use the convert_goldens_to_test_cases function in the previous example
dataset.test_cases = convert_goldens_to_test_cases(dataset.goldens)

@pytest.mark.parametrize("test_case", dataset)
def test_llm_app(test_case: LLMTestCase):
    assert_test(test_case, [AnswerRelevancyMetric()])
```

Finally, create a `YAML` file that would be used in your CI/CD pipelines to run this file:

```yaml title=".github/workflows/regression.yml"
name: LLM Regression Test

on:
  push:
  pull_request:

jobs:
  test:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout Code
        uses: actions/checkout@v2

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: "3.10"

      - name: Install Poetry
        run: |
          curl -sSL https://install.python-poetry.org | python3 -
          echo "$HOME/.local/bin" >> $GITHUB_PATH

      - name: Install Dependencies
        run: poetry install --no-root

      - name: Login to Confident AI
        env:
          CONFIDENT_API_KEY: ${{ secrets.CONFIDENT_API_KEY }}
        run: poetry run deepeval login --confident-api-key "$CONFIDENT_API_KEY"

      - name: Run DeepEval Test Run
        run: poetry run deepeval test run test_llm_app.py
```

:::tip
Don't forget to also login to Confident AI, otherwise you wouldn't be able to pull your dataset, nor have testing reports on Confident AI.
:::

## Using Datasets on Confident AI

With a dataset, you can enable non-technical team members to [trigger evaluations directly on Confident AI](/confident-ai/confident-ai-testing-n-evaluation-metric-collections#triggering-evaluations-on-confident-ai) in a click of a button, without needing to code.

It involves a 4-step process:

1. [Create a metric collection](/confident-ai/confident-ai-testing-n-evaluation-metric-collections#create-a-metric-collection) to specify which metrics to evaluation your LLM application on.
2. Select which dataset you'll be using to test your LLM application.
3. [Setup and connect your LLM endpoint](/confident-ai/confident-ai-advanced-llm-connection) for Confident AI to prompt at evaluation time. Test cases will be used for evaluation based on the `actual_output` (and/or `retrieval_context` and `tools_called`) your LLM connection endpoint has returned.
4. Press the evaluate button on the **Evaluations** page to start an evaluation.

:::info
We don't recommend you setting up this workflow unless you need to be able to run evaluations without touching code. There is nothing you can do by running evaluations on Confident AI that you cannot do with evaluating locally with `deepeval`.

Running evaluations with `deepeval` gives you more flexibility for metrics and customizations, but running evaluations on Confident AI unlocks evaluations for non-technical team members.
:::

Since the quality of your evaluation results is a function of the quality of your dataset, we'll see how you can improve your datasets over time to make your benchmarking more and more robust throughout the lifetime of your LLM application.
