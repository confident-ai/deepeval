---
id: confident-ai-introduction
title: Confident AI QuickStart
sidebar_label: Confident AI QuickStart
---

import Equation from "@site/src/components/equation";

:::caution
Are you following best LLM evaluation practices? Without a serious evaluation workflow, your testing results aren't really valid, and you might be wasting a lot of time iterating on the wrong things.
:::

**Confident AI is the LLM evaluation platform for DeepEval**. It is native to DeepEval, and was designed for teams building LLM applications to maximize its performance, and to safeguard against unsatisfactory LLM outputs. Whilst DeepEval's open-source metrics are great for running evaluations, there is so much more to building a robust LLM evaluation workflow than collecting metric scores.

If you're _serious_ about LLM evaluation, Confident AI is for you.

<div
  style={{
    width: "100%",
    display: "flex",
    alignItems: "center",
    justifyContent: "center",
    position: "relative",
    marginBottom: "2rem",
  }}
>
  <img
    id="light-invertable-img"
    src="https://confident-docs.s3.us-east-1.amazonaws.com/confident-flywheel.svg"
    alt="Confident AI"
    style={{
      marginTop: "2rem",
      marginBottom: "2rem",
      height: "auto",
    }}
  />
  <img
    src="/icons/logo.svg"
    alt="Confident AI"
    className="glowing"
    style={{
      width: "80px",
      height: "80px",
      transform: "translateY(-50%) translateX(-50%)",
      position: "absolute",
      left: "50%",
      top: "50%",
      filter: "drop-shadow(0px 0px 16px #7c3aed)",
    }}
  />
</div>

Apart from running the actual evaluations, you'll need a way to:

- Curate a robust testing dataset
- Perform LLM benchmark analysis
- Tailor evaluation metrics to your opinions
- Improve your testing dataset over time

Confident AI enforces this by offering an opinionated, centralized platform to manage all the things mentioned above, which for you means "more accurate, informative, and faster insights", and allows you to "identify any performance gaps and identify how to improve my LLM system".

:::tip DID YOU KNOW?

<Equation formula="\textbf{Great LLM Evaluation} == \textbf{Quality of Dataset} \times \textbf{Quality of Metrics}" />

:::

## Why Confident AI?

If your team has ever tried building its own LLM evaluation pipeline, here are the list of problems your team has likely encountered (and it's a long list):

- **Dataset Curation Is Fragmented And Annoying**

  - Your team often juggle between tools like Google Sheets or Notion to curate and update datasets, leading to constant back-and-forth between engineers and domain expert annotators.
  - There is no "source of truth" since datasets aren't in-sync with your codebase for evaluations.

- **Evaluation Results Are (Still) More Vibe Checks Rather Than Experimentation**

  - You basically just look at failing test cases, but they donâ€™t provide actionable insights, and sharing it among your team is hard.
  - Itâ€™s impossible to compare benchmarks side-by-side to understand how changes impact performance for each unit test, making it more guesswork than experimentation.

- **Testing Data Are Static With No Easy Way To Keep Them Updated**

  - Your LLM application needs and priorities evolves in production, but your datasets donâ€™t.
  - Figuring out how to query and incorporate real-world interactions into evaluation datasets is tedious and error-prone.

- **Building A/B Testing Infrastructure Is Hard And Current Tools Don't Cut It**

  - Setting up A/B testing for prompts/models to route traffic between versions is easy, but figuring out which version performed better and on what areas is hard.
  - Tools like PostHog or Mixpanel give user-level analytics, while other LLM observability tools focus too much on cost and latency, none of which tell you anything about the end output quality.

- **Human Feedback Doesn't Lead to Improvements**

  - Teams spend time collecting feedback from end-users or internal reviewers, but thereâ€™s no clear path to integrate it back into datasets.
  - A lot of manual effort is needed to make good use of feedback, and unfortunately it is a waste of everyone's time.

- **There's No End To Manual Human Intervention**

  - Teams rely on human reviewers to gatekeep LLM outputs before it reaches users in production, but the process is random, unstructured, and never ending.
  - No automation to focus reviewers on high-risk areas or repetitive tasks.

Confident AI solves all of your LLM evaluation problems so you can stop going around in circles. Here's a diagram outlining how Confident AI works:

<div
  style={{
    marginTop: "80px",
    marginBottom: "70px",
    display: "flex",
    justifyContent: "center",
  }}
>
  <img
    id="confident-workflow"
    src="https://d2lsxfc3p6r9rv.cloudfront.net/confident-workflow-4.png"
  />
</div>

## Installation

Go to the root directory of your project and create a virtual environment (if you don't already have one). In the CLI, run:

```python
python3 -m venv venv
source venv/bin/activate
```

In your newly created virtual environment, run:

```python
pip install -U deepeval
```

:::note
We always recommend keeping `deepeval` updated to its latest version to use Confident AI.
:::

## Login to Confident AI

Everything in `deepeval` is already automatically integrated with Confident AI, including any [custom metrics](/docs/metrics-custom) you've built on `deepeval`. To start using Confident AI with `deepeval`, simply login in the CLI:

```
deepeval login
```

Follow the instructions displayed on the CLI (to create an account, get your Confident API key, paste it in the CLI), and you're good to go.

:::tip DID YOU KNOW?
You can also login directly in Python if you already have a Confident API Key:

```python
deepeval.login_with_confident_api_key("your-confident-api-key")
```

Or, via the CLI:

```python
deepeval login --confident-api-key "your-confident-api-key"
```

:::

## Setup Your Evaluation Model

:::info
You can also use [**ANY** custom LLM of your choice](/guides/guides-using-custom-llms), although we **DON'T** recommend it for this quickstart guide due to some custom models being especially error-prone to outputting valid Jsons.
:::

You'll need to set your `OPENAI_API_KEY` as an enviornment variable before running an evaluation, since the metrics we'll be using is an LLM-evaluated metric.

```console
export OPENAI_API_KEY=<your-openai-api-key>
```

Alternatively, if you're working in a notebook enviornment (Jupyter or Colab), set your `OPENAI_API_KEY` in a cell:

```console
 %env OPENAI_API_KEY=<your-openai-api-key>
```

Please **do not include** quotation marks when setting your `OPENAI_API_KEY` if you're working in a notebook enviornment as it is invalid syntax.

:::note
You can also run evaluations on Confident AI using our models, but that's a more advanced topic for later on in this documentation.
:::

## Run Your First Evaluation

Now that you're logged in, create a python file, for example say `experiment_llm.py`. We're going to be evaluating a medical chatbot for this quickstart guide, but it can be any other LLM systems that you are building.

<details><summary>Click to see fake data we'll be using</summary>
<p>

```python
fake_data = [
    {
        "input": "I have a persistent cough and fever. Should I be worried?",
        "actual_output": (
            "Based on your symptoms, it could be a sign of a viral or bacterial infection. "
            "However, if the fever persists for more than three days or you experience difficulty breathing, "
            "please consult a doctor immediately."
        ),
        "retrieval_context": [
            "Coughing that lasts more than three weeks is typically classified as a chronic cough and could indicate conditions such as asthma, chronic bronchitis, or gastroesophageal reflux disease (GERD).",
            "A fever is the body's natural response to infections, often caused by viruses or bacteria. Persistent fevers lasting more than three days should be evaluated by a healthcare professional as they may indicate conditions like pneumonia, tuberculosis, or sepsis.",
            "Shortness of breath associated with fever and cough can be a sign of serious respiratory issues such as pneumonia, bronchitis, or COVID-19.",
            "Self-care tips for mild symptoms include staying hydrated, taking over-the-counter fever reducers (e.g., acetaminophen or ibuprofen), and resting. Avoid suppressing a productive cough without consulting a healthcare provider."
        ]
    },
    {
        "input": "What should I do if I accidentally cut my finger deeply?",
        "actual_output": (
            "If you cut your finger deeply, just rinse it with water and avoid applying any pressure. "
            "Tetanus shots aren't necessary unless you see redness immediately."
        ),
        "retrieval_context": [
            "Deep cuts that are more than 0.25 inches deep or expose fat, muscle, or bone require immediate medical attention. Such wounds may need stitches to heal properly.",
            "To minimize the risk of infection, wash the wound thoroughly with soap and water. Avoid using alcohol or hydrogen peroxide, as these can irritate the tissue and delay healing.",
            "If the bleeding persists for more than 10 minutes or soaks through multiple layers of cloth or bandages, seek emergency care. Continuous bleeding might indicate damage to an artery or vein.",
            "Watch for signs of infection, including redness, swelling, warmth, pain, or pus. Infections can develop even in small cuts if not properly cleaned or if the individual is at risk (e.g., diabetic or immunocompromised).",
            "Tetanus, a bacterial infection caused by Clostridium tetani, can enter the body through open wounds. Ensure that your tetanus vaccination is up to date, especially if the wound was caused by a rusty or dirty object."
        ]
    }
]

```

</p>
</details>

```python title="experiment_llm.py"
from deepeval import evaluate
from deepeval.test_case import LLMTestCase
from deepeval.metrics import AnswerRelevancyMetric, FaithfulnessMetric

# See above for contents of fake data
fake_data = [...]

# Create a list of LLMTestCase
test_cases = []
for fake_datum in fake_data:
  test_case = LLMTestCase(
    input=fake_datum["input"],
    actual_output=fake_datum["actual_output"],
    retrieval_context=fake_datum["retrieval_context"]
  )
  test_cases.append(test_case)

# Define metrics
answer_relevancy = AnswerRelevancyMetric(threshold=0.5)
faithfulness = FaithfulnessMetric(threshold=0.5)

# Run evaluation
evaluate(test_cases=test_cases, metrics=[answer_relevancy, faithfulness])
```

:::note
In reality, you'll want to **generate `actual_output`s and `retrieval_context`s at evaluation time**. This is because by having a static dataset of `input`s as a list of [`LLMTestCase`s](/docs/evaluation-test-cases#llm-test-case), you'll be able to experiment which two versions of your LLM application performs better.
:::

Finally, run `experiment_llm.py` to have Confident AI benchmark your LLM application for you:

```console
python experiment_llm.py
```

**Congratulations ðŸŽ‰!** You just ran your first evaluation which created a **test run** on Confident AI. Before diving into the platform, let's breakdown what happened.

- We looped through the `fake_data` dataset, and created a list of `LLMTestCase`s.
- The `LLMTestCase` variable `input` mimics a user input, and `actual_output` is a placeholder for what your application's supposed to output based on this input.
- The `LLMTestCase` variable `retrieval_context` contains the retrieved context from your knowledge base, and `AnswerRelevancyMetric(threshold=0.5)` and `FaithfulnessMetric(threshold=0.5)` is an default metric provided by `deepeval` for you to evaluate your LLM output's relevancy based on the provided retrieval context.
- All metric scores range from 0 - 1, which the `threshold=0.5` threshold ultimately determines if your metric have passed or not. A test case only passes if all of its metric passes.

**ðŸš¨ But unfortunately, not all test cases have passed.** Can you click on **_View Test Case Details_** on the failing test case to figure out why?

<img
  src="https://confident-docs.s3.us-east-1.amazonaws.com/confident-quickstart-test-cases.png"
  alt="Confident AI"
/>

:::info
Failing test cases represents areas of improvement, and by improving your LLM application through various means, such as writing better prompts, using better tool calls, or even fine-tuning a custom model, you'll be able to make failing test cases passing.
:::

In the next section, we'll learn how to create and use a dataset on Confident AI so we can move away from `fake_data`.
