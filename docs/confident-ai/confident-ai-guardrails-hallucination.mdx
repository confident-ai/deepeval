---
id: confident-ai-guardrails-hallucination
title: Hallucination Guard
sidebar_label: Hallucination Guard
---

The **Hallucination Guard** is an output guard that analyzes the responses generated by your language model to detect any fabricated or inaccurate information, ensuring all outputs are factually correct and reliable.

:::info
`HallucinationGuard` is only available as an **output guard**.
:::

Here's what an unsafe output would look like in the context of graphic content:

> _"You should visit the Sydney sometime. Australia's capitol is a great place to be."_
>
> _â€” probably your LLM_

## Example

Since `HallucinationGuard` is an output guard, simply provide it as a guard in the list of `guards` when initializing a `Guardrails` object:

```python
from deepeval.guardrails import Guardrails, HallucinationGuard

guardrails = Guardrails(guards=[HallucinationGuard()])
```

Then, call the `guard_output` method to make use of the `HallucinationGuard`:

```python
...

output = generate_output(input)
guard_result = guardrails.guard_output(input=input, output=output)
print(guard_result)
```

:::note
There are no required arguments when initializing a `HallucinationGuard`.
:::

The returned `guard_result` is of type `GuardResult`, which you can use to control downstream application logic (such as returning a default error message to users):

```python
...

print(guard_result.breached, guard_result.guard_data)
```
