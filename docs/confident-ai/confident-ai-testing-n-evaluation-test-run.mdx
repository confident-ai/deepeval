---
id: confident-ai-testing-n-evaluation-test-run
title: Running Evaluations
sidebar_label: Running Evaluations
---

As you saw in the [quickstart section earlier](/confident-ai/confident-ai-introduction#run-your-first-evaluation), a test run on Confident AI is created when you run an evaluation with `deepeval`. A test run is made up of a list of test cases, containing the `input`, `actual_output`, etc. of your LLM application, and each with its own set of metric scores to represent the performance of your LLM application as a whole.

Running evaluations allow you to benchmark different iterations of your LLM application, and is something you would do **in development** in order to catch problems before it is deployed to production.

:::caution
**Everything in this section and the next is about running evaluations locally** using `deepeval`. If you wish to learn how to run evaluations on the cloud instead, [visit this section.](/confident-ai/confident-ai-testing-n-evaluation-metric-collections)

Typically, there are only a three reasons why you'll need to run evaluations on the cloud instead:

- You're building in Typescript, and so cannot leverage `deepeval`'s metrics in Python.
- You want to trigger evaluations directly on the platform in a click of a button.
- You want to save cost on tokens used for evaluation.

Saving cost is the weakest reason to run evaluations on the cloud. Code-driven metrics that you run locally is the best way to customize and improve metrics as your testing criteria grows and matures.

:::

## Running An Evaluation

To run an evaluation, the first step is to choose your LLM evaluation metrics, and you can see what's available here (custom metrics are available if non of `deepeval`'s pre-defined metrics resonate with your use case).

:::note
For an agentic evaluation use case, you can also include `LLMTestCase` parameters such as [`tools_called`](/docs/evaluation-test-cases#tools-called) to encapsulate what tools were used during the generation of each `actual_output`. For this example though, since we're just evaluating a RAG-based medical chatbot with no function/tool calling, we're just going to populate the `input`, `actual_output`, and `retrieval_context` for our `LLMTestCase`.
:::

For the sake of simplicity, we'll reuse the example in quickstart for demontration purposes, which uses the [`AnswerRelevancyMetric`](/docs/metrics-answer-relevancy) and [`FaithfulnessMetric`](/docs/metrics-faithfulness), which requires the `input`, `actual_output`, and `retrieval_context` for each `LLMTestCase`.

Also, don't forget to login to Confident AI:

```console
deepeval login
```

```python title="experiment_llm.py"
import hypothetical_chatbot
from deepeval.dataset import EvaluationDataset
from deepeval.test_case import LLMTestCase
from deepeval.metrics import AnswerRelevancyMetric, FaithfulnessMetric
from deepeval import evaluate

# Use dataset from last section
dataset = EvaluationDataset()
dataset.pull(alias="QA Dataset")

# Create a list of LLMTestCase
test_cases = []
for golden in dataset.goldens:
  input = golden.input
  # Replace with your own LLM application
  actual_output, retrieval_context = hypothetical_chatbot(input)
  test_case = LLMTestCase(
    input=input,
    actual_output=actual_output,
    retrieval_context=retrieval_context
  )
  test_cases.append(test_case)

# Define metrics
answer_relevancy = AnswerRelevancyMetric(threshold=0.5)
faithfulness = FaithfulnessMetric(threshold=0.5)

# Run evaluation
evaluate(test_cases=test_cases, metrics=[answer_relevancy, faithfulness], identifier="First Run")
```

You'll notice that unlike the quickstart guide, we actually added an `identifier` argument for this `evaluate()` call. The `identifier` is a string which you can supply to identify test runs more easily, because otherwise all you'll be seeing on Confident AI is gibberish test run IDs we've generated for you.

:::info
To learn about all the parameters the `evaluate()` accepts, [click here.](/docs/evaluation-introduction#evaluating-without-pytest)
:::

As you'll see on Confident AI, your second test case is actually failing for the `FaithfulnessMetric` metric - that's fine, we're going to fix it.

<img
  src="https://confident-docs.s3.us-east-1.amazonaws.com/confident-quickstart-test-cases.png"
  alt="Confident AI"
/>

:::tip
If you click on the _**View test case details**_ button, you'll be able to see the reasons why the `FaithfulnessMetric` metric failed, as well as the `verbose_logs` which details how the metric was calculated.
:::

## Running Another Evaluation

For now, lets pretend that we've improved our medical chatbot and here is the newly generated `actual_output` for the second `LLMTestCase` after one iteration:

```python
"""If you cut your finger deeply, rinse it with soap and water, apply pressure to stop bleeding,
and cover it with a clean bandage. Seek medical care if it keeps bleeding, is very deep, or shows
signs of infection like redness or swelling. Ensure your tetanus shot is up to date."""
```

Go ahead and replace the second `actual_output` with the latest one above, and run evaluations again:

```python title="experiment_llm.py"
import hypothetical_chatbot
from deepeval import evaluate
from deepeval.dataset import EvaluationDataset
from deepeval.test_case import LLMTestCase
from deepeval.metrics import AnswerRelevancyMetric, FaithfulnessMetric

# Use dataset from last section
dataset = EvaluationDataset()
dataset.pull(alias="QA Dataset")

# Create a list of LLMTestCase
test_cases = []
for golden in dataset.goldens:
  input = golden.input
  # Don't forget to hardcode the second actual_output!
  actual_output, retrieval_context = hypothetical_chatbot(input)
  test_case = LLMTestCase(
    input=input,
    actual_output=actual_output,
    retrieval_context=retrieval_context
  )
  test_cases.append(test_case)

# Define metrics
answer_relevancy = AnswerRelevancyMetric(threshold=0.5)
faithfulness = FaithfulnessMetric(threshold=0.5)

# Run evaluation
evaluate(test_cases=test_cases, metrics=[answer_relevancy, faithfulness], identifier="Second Run")
```

Don't forget to update your `identifier` and congratulations ðŸŽ‰! All your test cases are now passing with flying colors.

## Catching Regressions

Now that you've created two test runs, (`identifier`="First Run" and `identifier`="Second Run") - which are benchmarks of your LLM application, you can compare test results side by side to identify regressions and improvements.

:::info
In this example, we only have two test cases, but this can be scaled to thousands of test cases that allows you to quickly identify how does your most recent changes to your LLM application affect its performance.
:::

In your individual **Test Run** page, navigate to the **Compare Test Results** page through the side navigation drawer, and select a test run to compare against in the dropdown menu. The rows you when you open the dropdown are the test run IDs displayed for historical test runs, and if you've supplied an `identifier` during `evaluate()` you'll be able to easily identify which test run's which.

<video width="100%" autoPlay loop muted playsInlines>
  <source
    src="https://confident-docs.s3.us-east-1.amazonaws.com/confident-compare-test-run.mp4"
    type="video/mp4"
  />
</video>

:::tip
Confident AI groups your test cases by its `name`, and so if you want to compare test runs legitimately you should supply a hard-coded name for the same `LLMTestCase`s you want to compare side by side.

```python
from deepeval.test_case import LLMTestCase

test_case = LLMTestCase(name="239874", ...)
```

You can learn more about **[labelling test cases here.](/docs/evaluation-test-cases#labeling-test-cases-for-confident-ai)**

:::

By selecting one of the historical test runs, you'll be able to see exactly how the metric scores differed across them, and hence working out if your latest change is an improvement or regression, and whether it caused any unacceptable breaking changes. Luckily for us, it didn't.

## Testing Reports On Confident AI

All evaluations ran, either locally via `deepeval`(ie. `evaluate()`, `deepeval test run`) or [on Confident AI](/confident-ai/confident-ai-testing-n-evaluation-metric-collections), are automatically sent to Confident AI. Confident AI will generate testing reports out of your testing data for anyone in your organization to analyze, share, download, and collaborate on.

Here are the list of benefits you get with Confident AI's testing reports:

- Analyze metric score distributions, averages, and median scores.
- Inspect test cases, including parameters such as `actual_output`, `retrieval_context`, and `tools_called`, and their associated metric results including score, reasoning, errors (if any), and verbose logs (for debugging your evaluation model's chain of thought).
- Filter for and select specific test cases for sharing.
- Download test cases as CSV or JSON (if you're running a conversational test run).
- [Create a dataset](/confident-ai/confident-ai-evaluation-dataset-management#from-an-existing-test-run) from the test cases in a test run.
- Compare different test runs side by side.
- Create a public link to share with external stakeholders that might be interested in your LLM evaluation process.

<video width="100%" controls muted playsInline>
  <source
    src="https://confident-docs.s3.us-east-1.amazonaws.com/confident-full-test-run.mp4"
    type="video/mp4"
  />
</video>

:::info
Confident AI also keeps track of additional metadata such as time taken for a test run to finish executing, and the evaluation cost associated with this test run.
:::

## Logging Models, Prompts, And More

In the previous sections, we saw how to create a test run by dynamically generating `actual_output` for pulled datasets at evaluation time. But in addition to that, in order to experiemnt with different hyperparameter values to iterate towards the optimal LLM to use for your LLM application for example, you have to **associate hyperparameter values to test runs to compare and pick the best hyperparameters on Confident AI.**

Continuing from the previous example, if for example `llm_app` uses `gpt-4o` as its LLM, you can associate the `gpt-4o` value as such:

```python
...

evaluate(
    test_cases=test_cases,
    metrics=[metric],
    hyperparameters={"model": "gpt4o", "prompt template": "your-prompt-template-goes-here"}
)
```

:::tip
You can run a heavily nested for loop to generate `actual_output`s to get test run evaluation results for all hyperparameter combinations.

```python
for model in models:
    for prompt in prompts:
        evaluate(..., hyperparameters={...})
```

:::

This will help Confident AI associate which hyperparameter combination were used for each test run results, which you can ultimately easily filter for and visualize which performs best on Confident AI.

![ok](https://confident-bucket.s3.amazonaws.com/experiments.png)

In the next section, we'll show how you can setup this workflow in CI/CD pipelines to unit-test your LLM application.
