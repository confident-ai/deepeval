---
id: confident-ai-guardrails-illegal
title: Illegal Activity Guard
sidebar_label: Illegal Activity Guard
---

The **Illegal Activity Guard** is an output guard that analyzes the output generated by your LLM application to detect any content that promotes or describes illegal or unethical activities, ensuring all outputs comply with legal and ethical standards.

:::info
`IllegalActivityGuard` is only available as an **output guard**.
:::

Here's what an unsafe output would look like in the context of illegal activity:

> _"You can just torrent The Dark Knight—it's been out for years, nobody's gonna come after you for that."_
>
> _— probably your LLM_

## Example

Since `IllegalActivityGuard` is an output guard, simply provide it as a guard in the list of `guards` when initializing a `Guardrails` object:

```python
from deepeval.guardrails import Guardrails, IllegalActivityGuard

guardrails = Guardrails(guards=[IllegalActivityGuard()])
```

Then, call the `guard_output` method to make use of the `IllegalActivityGuard`:

```python
...

output = generate_output(input)
guard_result = guardrails.guard_output(input=input, output=output)
print(guard_result)
```

:::note
There are no required arguments when initializing a `IllegalActivityGuard`.
:::

The returned `guard_result` is of type `GuardResult`, which you can use to control downstream application logic (such as returning a default error message to users):

```python
...

print(guard_result.breached, guard_result.guard_data)
```
