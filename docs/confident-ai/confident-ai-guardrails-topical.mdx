---
id: confident-ai-guardrails-topical
title: Topical Guard
sidebar_label: Topical Guard
---

import Equation from "@site/src/components/equation";

The **Topical Guard** is an input guard that ensures user inputs adhere to a predefined set of allowed topics. It analyzes the input, compares it against the specified topics, and determines whether the input is relevant or off-topic.

:::info
`TopicalGuard` is only available as an **input guard**.
:::

Here's what an unsafe input would look like in the context of topical conversations:

> _"I want to get your opinion on Donald Trump"_
>
> _â€” probably a random user_

## Example

Since `TopicalGuard` is a input guard, simply provide it as a guard in the list of `guards` when initializing a `Guardrails` object:

```python
from deepeval.guardrails import TopicalGuard

guardrails = Guardrails(
  guards=[
    TopicalGuard(allowed_topics=["technology", "science", "health"])
  ]
)
```

There is 1 required parameter when creating a `TopicalGuard`:

- `allowed_topics`: A list of topics (strings) that are permitted for discussion.

Then, call the `guard_input` method to make use of the `TopicalGuard`:

```python
...

guard_result = guardrails.guard_input(input=input)
print(guard_result)
```

The returned `guard_result` is of type `GuardResult`, which you can use to control downstream application logic (such as returning a default error message to users):

```python
...

print(guard_result.breached, guard_result.guard_data)
```

## Example

```python
from deepeval.guardrails import Guardrails, TopicalGuard

allowed_topics = ["technology", "science", "health"]
user_input = "Can you tell me about the latest advancements in quantum computing?"

topical_guard = TopicalGuard(allowed_topics=allowed_topics)
guard_result = topical_guard.guard(input=user_input)
```

There is 1 required parameter when creating a `TopicalGuard`:

- `allowed_topics`: A list of topics (strings) that are permitted for discussion.

The `guard` function accepts a single parameter `input`, representing the user input to your LLM application.

## How Is it Calculated?

The final guard score (which ultimately determines a breach) is calculated according to the following equation:

<Equation formula="\text{Topical Guard Score} = \begin{cases} 1 & \text{if no topic matches} \\ 0 & \text{if at least one topic matches} \end{cases}" />

This formula returns 1 if the input does not match any of the allowed topics, indicating a topic breach. It returns 0 if the input matches at least one allowed topic, indicating no breach.
