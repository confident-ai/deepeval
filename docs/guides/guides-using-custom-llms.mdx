---
# id: using-custom-llms
title: Using Custom LLMs for Evaluation
sidebar_label: Using Custom LLMs for Evaluation
---

All of `deepeval`'s metrics uses LLMs for evaluation, and is currently defaulted to OpenAI's GPT models. However, for users that don't wish to use OpenAI's GPT models and would instead prefer other providers such as Claude (Anthropic), Gemini (Google), Llama-3 (Meta), or general-purpose evaluation models like Selene Mini (Atla), `deepeval` provides an easy way for anyone to use literaly **ANY** custom LLM for evaluation.

This guide will show you how to create custom LLMs for evaluation in `deepeval`, and demonstrate various methods to enforce valid JSON LLM outputs that are required for evaluation with the following examples:

- Atla Selene Mini from Hugging Face `transformers`
- Mistral-7B v0.3 from Hugging Face `transformers`
- Gemini 1.5 Flash from Vertex AI
- Claude-3 Opus from Anthropic

## Creating A Custom LLM

Here's a quick example on a custom Selene Mini (8B) model being used for evaluation in `deepeval`. Selene is based on the same architecture as Llama-3.1, but is optimized for evaluation tasks.

**Install the necessary libraries**
```bash
!pip install -U bitsandbytes lm-format-enforcer
```

**Load the model and tokenizer**
```python
import transformers
from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig
import torch

quantization_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_compute_dtype=torch.float16,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_use_double_quant=True,
)

selene_model = AutoModelForCausalLM.from_pretrained(
    "AtlaAI/Selene-1-Mini-Llama-3.1-8B",
    device_map="auto",
    quantization_config=quantization_config # remove to load FP16 model
)
selene_tokenizer = AutoTokenizer.from_pretrained(
    "AtlaAI/Selene-1-Mini-Llama-3.1-8B"
)
```

**Create a custom model class**
```python
import transformers
import torch
from transformers import BitsAndBytesConfig
from transformers import AutoModelForCausalLM, AutoTokenizer

from deepeval.models import DeepEvalBaseLLM

from deepeval.models.base_model import DeepEvalBaseLLM
from lmformatenforcer import JsonSchemaParser
from lmformatenforcer.integrations.transformers import (
    build_transformers_prefix_allowed_tokens_fn,
)
from transformers import pipeline
from pydantic import BaseModel
import json

class CustomSeleneMini(DeepEvalBaseLLM):
    def __init__(
        self,
        model,
        tokenizer
    ):
        self.model = model
        self.tokenizer = tokenizer

    def load_model(self):
        return self.model

    def generate(self, prompt: str, schema: BaseModel = None) -> BaseModel | str:
        model = self.load_model()

        # HF pipeline for inference of text generation
        pipeline = transformers.pipeline(
            "text-generation",
            model=model,
            tokenizer=self.tokenizer,
            use_cache=True,
            device_map="auto",
            num_return_sequences=1,
            max_new_tokens=512,
            do_sample=True,
            top_k=5,
            eos_token_id=self.tokenizer.eos_token_id,
            pad_token_id=self.tokenizer.eos_token_id,
        )

        if schema is not None:
            # Create parser required for JSON confinement using lmformatenforcer
            parser = JsonSchemaParser(schema.model_json_schema())
            prefix_function = build_transformers_prefix_allowed_tokens_fn(
                pipeline.tokenizer, parser
            )
            # Output and load valid JSON
            output_dict = pipeline(chat_template, prefix_allowed_tokens_fn=prefix_function)
            output = output_dict[0]["generated_text"][len(chat_template):]
            json_result = json.loads(output)
            # Return valid JSON object according to the schema DeepEval supplied
            return schema(**json_result)
        return pipeline(prompt)

    async def a_generate(self, prompt: str, schema: BaseModel = None) -> BaseModel | str:
        return self.generate(prompt, schema)

    def get_model_name(self):
        return "Atla Selene Mini"
```

There are **SIX** rules to follow when creating a custom LLM evaluation model:

1. Inherit `DeepEvalBaseLLM`.
2. Implement the `get_model_name()` method, which simply returns a string representing your custom model name.
3. Implement the `load_model()` method, which will be responsible for returning a model object.
4. Implement the `generate()` method with 1) a parameter of type string that acts as the prompt to your custom LLM and 2) a parameter of type `BaseModel` which is used to confine the LLM output to a valid JSON. See the **JSON Confinement for Custom LLMs** section below for implementation details.
5. The `generate()` method should return a `BaseModel` object containing the generated output from your custom LLM. Note that we called `pipeline(prompt)` to access the model generations in this particular example, but this could be different depending on the implementation of your custom model object.
6. Implement the `a_generate()` method, with the same function signature as `generate()`. **Note that this is an async method**. In this example, we called `self.generate(prompt)`, which simply reuses the synchronous `generate()` method. However, although optional, you should implement an asynchronous version (if possible) to speed up evaluation.

Then, instantiate the `CustomSeleneMini` class and test the `generate()` (or `a_generate()`) method out:

```python
...

custom_llm = CustomSeleneMini(model=selene_model, tokenizer=selene_tokenizer)
print(custom_llm.generate("Write me a joke"))
```

Finally, supply it to a metric to run evaluations using your custom LLM:

```python
from deepeval.metrics import AnswerRelevancyMetric
...

metric = AnswerRelevancyMetric(model=custom_llm)
metric.measure(...)
```

**Congratulations ðŸŽ‰!** You can now evaluate using any custom LLM of your choice on all LLM evaluation metrics offered by `deepeval`.

## JSON Confinement for Custom LLMs

:::tip
When using open-source LLMs with `deepeval`, it's important to implement JSON output confinement to ensure they generate valid responses for supported public benchmarks. This allows you to properly [benchmark your own LLM](/docs/benchmarks-introduction) against other models.
:::

When creating the CustomModel() class, we need to ensure that the `generate()` method returns a valid JSON to avoid the below error:

```bash
ValueError: Evaluation LLM outputted an invalid JSON. Please use a better evaluation model.
```

This arises when the custom LLM used for evaluation is unable to generate valid JSONs during metric calculation, which stops the evaluation process altogether. This happens because for smaller and less powerful LLMs, prompt engineering alone is not sufficient to enforce JSON outputs, which so happens to be the method used in `deepeval`'s metrics.

:::info
All of `deepeval`'s metrics require the evaluation model to generate valid JSONs to extract properties such as: reasons, verdicts, statements, and other types of LLM-generated responses that are later used for calculating metric scores, and so when the generated JSONs required to extract these properties are invalid (eg. missing brackets, incomplete string quotations, extra trailing commas, or mismatched keys), `deepeval` won't be able to use the necessary information required for metric calculation. Here's an example of an invalid JSON an open-source model like `mistralai/Mistral-7B-Instruct-v0.3` might output:

```console
{
    "reaso: "The actual output does directly not address the input",
}
```

:::

### `generate()` and `a_generate()` Method Signatures

In order to enforce JSON outputs generated by your custom LLM, we write the `generate()` and `a_generate()` method to **accept an additional argument of type `BaseModel`, and output a `BaseModel` instead of a `str`.**

:::note
The `BaseModel` type is a type provided by the `pydantic` library, which is an extremely common typing library in Python.

```python
from pydantic import BaseModel
```

:::

You might be wondering, **how does this help with enforcing JSON outputs?**

It helps because in `deepeval`'s metrics, when there is a `schema: BaseModel` argument defined for the `generate()` and/or `a_generate()` method, `deepeval` will inject your generate methods with the Pydantic schemas which you can leverage to enforce JSON outputs. Let's see how we do that.

### Generating valid JSONs

With the method signatures, `deepeval` automatically injects your custom LLM with the required Pydantic schemas, which we can leverage to enforce JSON outputs for each LLM generation.

There are many ways to leverage Pydantic schemas to confine LLMs to generate valid JSONs. In our `CustomSeleneMini` example, we used the `lm-format-enforcer` library to confine JSON outputs using the provided Pydantic schema.

```console
pip install lm-format-enforcer
```

:::tip
We called `self.generate(prompt, schema)` in the `a_generate()` method to keep things simple, but you should aim to implement an asynchronous version of your custom LLM implementation and enforce JSON outputs the same way you would in the `generate()` method to keep evaluations fast.
:::

In the next section, we'll go through two JSON confinement libraries that covers a wide range of LLM interfaces.

## JSON Confinement libraries

There are two JSON confinement libraries that you should know about depending on the custom LLM you're using:

1. `lm-format-enforcer`: The **LM-Format-Enforcer** is a versatile library designed to standardize the output formats of language models. It supports Python-based language models across various platforms, including popular frameworks such as `transformers`, `langchain`, `llamaindex`, llama.cpp, vLLM, Haystack, NVIDIA, TensorRT-LLM, and ExLlamaV2. For comprehensive details about the package and advanced usage instructions, [please visit the LM-format-enforcer github page](https://github.com/noamgat/lm-format-enforcer). The LM-Format-Enforcer combines a **character-level parser** with a **tokenizer prefix tree**. Unlike other libraries that strictly enforce output formats, this method enables LLMs to sequentially generate tokens that meet output format constraints, thereby enhancing the quality of the output.

2. `instructor`: **Instructor** is a user-friendly python library built on top of Pydantic. It enables straightforward confinement of your LLM's output by encapsulating your LLM client within an Instructor method. It simplifies the process of extracting structured data, such as JSON, from LLMs including GPT-3.5, GPT-4, GPT-4-Vision, and open-source models like Mistral/Mixtral, Anyscale, Ollama, and llama-cpp-python. For more information on advanced usage or integration with other models not covered here, [please consult the documentation](https://github.com/jxnl/instructor).

:::note
You may wish to wish any JSON confinement libraries out there, and we're just suggesting two that we have found useful when crafting this guide.
:::

In the final section, we'll show several popular end-to-end examples of custom LLMs using either `lm-format-enforcer` or `instructor` for JSON confinement.

## More Examples

### `Mistral-7B-Instruct-v0.3` through `transformers`

Begin by installing the `lm-format-enforcer` package:

```console
pip install lm-format-enforcer
```

Here's a full example of a JSON confined custom Mistral 7B model implemented through `transformers`:

```python
import json
from pydantic import BaseModel
import torch
from lmformatenforcer import JsonSchemaParser
from lmformatenforcer.integrations.transformers import (
    build_transformers_prefix_allowed_tokens_fn,
)
from transformers import BitsAndBytesConfig
from transformers import AutoModelForCausalLM, AutoTokenizer

from deepeval.models import DeepEvalBaseLLM


class CustomMistral7B(DeepEvalBaseLLM):
    def __init__(self):
        quantization_config = BitsAndBytesConfig(
            load_in_4bit=True,
            bnb_4bit_compute_dtype=torch.float16,
            bnb_4bit_quant_type="nf4",
            bnb_4bit_use_double_quant=True,
        )

        model_4bit = AutoModelForCausalLM.from_pretrained(
            "mistralai/Mistral-7B-Instruct-v0.3",
            device_map="auto",
            quantization_config=quantization_config,
        )
        tokenizer = AutoTokenizer.from_pretrained(
            "mistralai/Mistral-7B-Instruct-v0.3"
        )

        self.model = model_4bit
        self.tokenizer = tokenizer

    def load_model(self):
        return self.model

    def generate(self, prompt: str, schema: BaseModel) -> BaseModel:
        model = self.load_model()
        pipeline = pipeline(
            "text-generation",
            model=model,
            tokenizer=self.tokenizer,
            use_cache=True,
            device_map="auto",
            max_length=2500,
            do_sample=True,
            top_k=5,
            num_return_sequences=1,
            eos_token_id=self.tokenizer.eos_token_id,
            pad_token_id=self.tokenizer.eos_token_id,
        )

        # Create parser required for JSON confinement using lmformatenforcer
        parser = JsonSchemaParser(schema.model_json_schema())
        prefix_function = build_transformers_prefix_allowed_tokens_fn(
            pipeline.tokenizer, parser
        )

        # Output and load valid JSON
        output_dict = pipeline(prompt, prefix_allowed_tokens_fn=prefix_function)
        output = output_dict[0]["generated_text"][len(prompt) :]
        json_result = json.loads(output)

        # Return valid JSON object according to the schema supplied
        return schema(**json_result)

    async def a_generate(self, prompt: str, schema: BaseModel) -> BaseModel:
        return self.generate(prompt, schema)

    def get_model_name(self):
        return "Mistral-7B v0.3"
```

:::note
Similar to the `CustomSeleneMini` example, you can similarly:

- pass in a `quantization_config` parameter if your compute resources are limited
- use the `lm-format-enforcer` library for JSON confinement

This is because the `CustomMistral7B` model is implemented through HF `transformers` as well.

:::

### `gemini-1.5-flash` through Vertex AI

Begin by installing the `instructor` package via pip:

```bash
pip install instructor
```

```python
from pydantic import BaseModel
import google.generativeai as genai
import instructor

from deepeval.models import DeepEvalBaseLLM


class CustomGeminiFlash(DeepEvalBaseLLM):
    def __init__(self):
        self.model = genai.GenerativeModel(model_name="models/gemini-1.5-flash")

    def load_model(self):
        return self.model

    def generate(self, prompt: str, schema: BaseModel) -> BaseModel:
        client = self.load_model()
        instructor_client = instructor.from_gemini(
            client=client,
            mode=instructor.Mode.GEMINI_JSON,
        )
        resp = instructor_client.messages.create(
            messages=[
                {
                    "role": "user",
                    "content": prompt,
                }
            ],
            response_model=schema,
        )
        return resp

    async def a_generate(self, prompt: str, schema: BaseModel) -> BaseModel:
        return self.generate(prompt, schema)

    def get_model_name(self):
        return "Gemini 1.5 Flash"
```

:::info
The `instructor` client automatically allows you to create a structured response by defining a `response_model` parameter which accepts a Pydantic `BaseModel` schema.
:::

### `claude-3-opus` through Anthropic

Begin by installing the `instructor` package via pip:

```bash
pip install instructor
```

```python
from pydantic import BaseModel
from anthropic import Anthropic

from deepeval.models import DeepEvalBaseLLM


class CustomClaudeOpus(DeepEvalBaseLLM):
    def __init__(self):
        self.model = Anthropic()

    def load_model(self):
        return self.model

    def generate(self, prompt: str, schema: BaseModel) -> BaseModel:
        client = self.load_model()
        instructor_client = instructor.from_anthropic(client)
        resp = instructor_client.messages.create(
            model="claude-3-opus-20240229",
            max_tokens=1024,
            messages=[
                {
                    "role": "user",
                    "content": prompt,
                }
            ],
            response_model=schema,
        )
        return resp

    async def a_generate(self, prompt: str, schema: BaseModel) -> BaseModel:
        return self.generate(prompt, schema)

    def get_model_name(self):
        return "Claude-3 Opus"
```

### Others

For any additional implementations, please come and ask away in the [DeepEval discord server](https://discord.com/invite/a3K9c8GRGt), we'll be happy to have you.
