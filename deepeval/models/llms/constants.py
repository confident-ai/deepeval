from typing import Any, Callable, Union

from deepeval.models.base_model import DeepEvalModelData


DEFAULT_GPT_MODEL = "gpt-4.1"
# OpenRouter uses provider/model format (e.g., "openai/gpt-4", "anthropic/claude-3-opus")
# DeepEval does not validate OpenRouter model strings.
DEFAULT_OPENROUTER_MODEL = f"openai/{DEFAULT_GPT_MODEL}"

ModelDataFactory = Callable[[], DeepEvalModelData]
ModelDataValue = Union[DeepEvalModelData, ModelDataFactory]


def default_model_data() -> DeepEvalModelData:
    return DeepEvalModelData()


class ModelDataRegistry(dict[str, ModelDataValue]):
    def get(  # type: ignore[override]
        self,
        key: str,
        default: ModelDataValue = default_model_data,
    ) -> DeepEvalModelData:
        model_data_value = super().get(key, default)
        return (
            model_data_value()
            if callable(model_data_value)
            else model_data_value
        )

    def __getitem__(self, key: str) -> DeepEvalModelData:
        model_data_value = super().__getitem__(key)
        return (
            model_data_value()
            if callable(model_data_value)
            else model_data_value
        )


def make_model_data(**kwargs: Any) -> ModelDataFactory:
    return lambda: DeepEvalModelData(**kwargs)


OPENAI_MODELS_DATA = ModelDataRegistry(
    {
        "gpt-3.5-turbo": make_model_data(
            supports_log_probs=True,
            supports_multimodal=False,
            supports_structured_outputs=False,
            supports_json=True,
            input_price=0.50 / 1e6,
            output_price=1.50 / 1e6,
        ),
        "gpt-3.5-turbo-0125": make_model_data(
            supports_log_probs=True,
            supports_multimodal=False,
            supports_structured_outputs=False,
            supports_json=True,
            input_price=0.50 / 1e6,
            output_price=1.50 / 1e6,
        ),
        "gpt-3.5-turbo-1106": make_model_data(
            supports_log_probs=True,
            supports_multimodal=False,
            supports_structured_outputs=False,
            supports_json=True,
            input_price=1.00 / 1e6,
            output_price=2.00 / 1e6,
        ),
        "gpt-4-0125-preview": make_model_data(
            supports_log_probs=True,
            supports_multimodal=True,
            supports_structured_outputs=False,
            supports_json=True,
            input_price=10.00 / 1e6,
            output_price=30.00 / 1e6,
        ),
        "gpt-4-1106-preview": make_model_data(
            supports_log_probs=True,
            supports_multimodal=True,
            supports_structured_outputs=False,
            supports_json=True,
            input_price=10.00 / 1e6,
            output_price=30.00 / 1e6,
        ),
        "gpt-4-turbo": make_model_data(
            supports_log_probs=True,
            supports_multimodal=True,
            supports_structured_outputs=False,
            supports_json=True,
            input_price=10.00 / 1e6,
            output_price=30.00 / 1e6,
        ),
        "gpt-4-turbo-2024-04-09": make_model_data(
            supports_log_probs=True,
            supports_multimodal=True,
            supports_structured_outputs=False,
            supports_json=True,
            input_price=10.00 / 1e6,
            output_price=30.00 / 1e6,
        ),
        "gpt-4-turbo-preview": make_model_data(
            supports_log_probs=True,
            supports_multimodal=True,
            supports_structured_outputs=False,
            supports_json=True,
            input_price=10.00 / 1e6,
            output_price=30.00 / 1e6,
        ),
        "gpt-4o": make_model_data(
            supports_log_probs=True,
            supports_multimodal=True,
            supports_structured_outputs=True,
            supports_json=False,
            input_price=2.50 / 1e6,
            output_price=10.00 / 1e6,
        ),
        "gpt-4": make_model_data(
            supports_log_probs=True,
            supports_multimodal=False,
            supports_structured_outputs=True,
            supports_json=True,
            input_price=30.00 / 1e6,
            output_price=60.00 / 1e6,
        ),
        "gpt-4o-2024-05-13": make_model_data(
            supports_log_probs=True,
            supports_multimodal=True,
            supports_structured_outputs=True,
            supports_json=False,
            input_price=2.50 / 1e6,
            output_price=10.00 / 1e6,
        ),
        "gpt-4o-2024-08-06": make_model_data(
            supports_log_probs=True,
            supports_multimodal=True,
            supports_structured_outputs=True,
            supports_json=False,
            input_price=2.50 / 1e6,
            output_price=10.00 / 1e6,
        ),
        "gpt-4o-2024-11-20": make_model_data(
            supports_log_probs=True,
            supports_multimodal=True,
            supports_structured_outputs=True,
            supports_json=False,
            input_price=2.50 / 1e6,
            output_price=10.00 / 1e6,
        ),
        "gpt-4o-mini": make_model_data(
            supports_log_probs=True,
            supports_multimodal=True,
            supports_structured_outputs=True,
            supports_json=False,
            input_price=0.150 / 1e6,
            output_price=0.600 / 1e6,
        ),
        "gpt-4o-mini-2024-07-18": make_model_data(
            supports_log_probs=True,
            supports_multimodal=True,
            supports_structured_outputs=True,
            supports_json=False,
            input_price=0.150 / 1e6,
            output_price=0.600 / 1e6,
        ),
        "gpt-4-32k": make_model_data(
            supports_log_probs=True,
            supports_multimodal=True,
            supports_structured_outputs=False,
            supports_json=True,
            input_price=60.00 / 1e6,
            output_price=120.00 / 1e6,
        ),
        "gpt-4-32k-0613": make_model_data(
            supports_log_probs=True,
            supports_multimodal=True,
            supports_structured_outputs=False,
            supports_json=False,
            input_price=60.00 / 1e6,
            output_price=120.00 / 1e6,
        ),
        "gpt-4.1": make_model_data(
            supports_log_probs=True,
            supports_multimodal=True,
            supports_structured_outputs=True,
            supports_json=False,
            input_price=2.00 / 1e6,
            output_price=8.00 / 1e6,
        ),
        "gpt-4.1-mini": make_model_data(
            supports_log_probs=True,
            supports_multimodal=True,
            supports_structured_outputs=True,
            supports_json=False,
            input_price=0.4 / 1e6,
            output_price=1.60 / 1e6,
        ),
        "gpt-4.1-nano": make_model_data(
            supports_log_probs=True,
            supports_multimodal=False,
            supports_structured_outputs=True,
            supports_json=False,
            input_price=0.1 / 1e6,
            output_price=0.4 / 1e6,
        ),
        "gpt-4.5-preview": make_model_data(
            supports_log_probs=False,
            supports_multimodal=False,
            supports_structured_outputs=False,
            supports_json=False,
            input_price=75.00 / 1e6,
            output_price=150.00 / 1e6,
        ),
        "gpt-4.5-preview-2025-02-27": make_model_data(
            supports_log_probs=False,
            supports_multimodal=False,
            supports_structured_outputs=True,
            supports_json=False,
            input_price=75.00 / 1e6,
            output_price=150.00 / 1e6,
        ),
        # Reasoning models - require temperature=1 (no custom temperature)
        "o1": make_model_data(
            supports_log_probs=False,
            supports_multimodal=False,
            supports_structured_outputs=True,
            supports_json=False,
            supports_temperature=False,
            input_price=15.00 / 1e6,
            output_price=60.00 / 1e6,
        ),
        "o1-preview": make_model_data(
            supports_log_probs=False,
            supports_multimodal=False,
            supports_structured_outputs=True,
            supports_json=False,
            input_price=15.00 / 1e6,
            output_price=60.00 / 1e6,
        ),
        "o1-2024-12-17": make_model_data(
            supports_log_probs=False,
            supports_multimodal=False,
            supports_structured_outputs=True,
            supports_json=False,
            supports_temperature=False,
            input_price=15.00 / 1e6,
            output_price=60.00 / 1e6,
        ),
        "o1-preview-2024-09-12": make_model_data(
            supports_log_probs=False,
            supports_multimodal=False,
            supports_structured_outputs=False,
            supports_json=False,
            input_price=15.00 / 1e6,
            output_price=60.00 / 1e6,
        ),
        "o1-mini": make_model_data(
            supports_log_probs=False,
            supports_multimodal=False,
            supports_structured_outputs=False,
            supports_json=False,
            supports_temperature=False,
            input_price=None,
            output_price=None,
        ),
        "o1-mini-2024-09-12": make_model_data(
            supports_log_probs=False,
            supports_multimodal=False,
            supports_structured_outputs=False,
            supports_json=False,
            supports_temperature=False,
            input_price=None,
            output_price=None,
        ),
        "o3-mini": make_model_data(
            supports_log_probs=False,
            supports_multimodal=False,
            supports_structured_outputs=True,
            supports_json=False,
            supports_temperature=False,
            input_price=1.10 / 1e6,
            output_price=4.40 / 1e6,
        ),
        "o3-mini-2025-01-31": make_model_data(
            supports_log_probs=False,
            supports_multimodal=False,
            supports_structured_outputs=True,
            supports_json=False,
            supports_temperature=False,
            input_price=1.10 / 1e6,
            output_price=4.40 / 1e6,
        ),
        "o4-mini": make_model_data(
            supports_log_probs=False,
            supports_multimodal=False,
            supports_structured_outputs=True,
            supports_json=False,
            supports_temperature=False,
            input_price=1.10 / 1e6,
            output_price=4.40 / 1e6,
        ),
        "o4-mini-2025-04-16": make_model_data(
            supports_log_probs=False,
            supports_multimodal=False,
            supports_structured_outputs=True,
            supports_json=False,
            supports_temperature=False,
            input_price=1.10 / 1e6,
            output_price=4.40 / 1e6,
        ),
        "gpt-5": make_model_data(
            supports_log_probs=False,
            supports_multimodal=True,
            supports_structured_outputs=True,
            supports_json=False,
            supports_temperature=False,
            input_price=1.25 / 1e6,
            output_price=10.00 / 1e6,
        ),
        "gpt-5-2025-08-07": make_model_data(
            supports_log_probs=False,
            supports_multimodal=False,
            supports_structured_outputs=True,
            supports_json=False,
            supports_temperature=False,
            input_price=1.25 / 1e6,
            output_price=10.00 / 1e6,
        ),
        "gpt-5-mini": make_model_data(
            supports_log_probs=False,
            supports_multimodal=False,
            supports_structured_outputs=True,
            supports_json=False,
            supports_temperature=False,
            input_price=0.25 / 1e6,
            output_price=2.00 / 1e6,
        ),
        "gpt-5-mini-2025-08-07": make_model_data(
            supports_log_probs=False,
            supports_multimodal=False,
            supports_structured_outputs=True,
            supports_json=False,
            supports_temperature=False,
            input_price=0.25 / 1e6,
            output_price=2.00 / 1e6,
        ),
        "gpt-5-nano": make_model_data(
            supports_log_probs=False,
            supports_multimodal=False,
            supports_structured_outputs=True,
            supports_json=False,
            supports_temperature=False,
            input_price=0.05 / 1e6,
            output_price=0.40 / 1e6,
        ),
        "gpt-5-nano-2025-08-07": make_model_data(
            supports_log_probs=False,
            supports_multimodal=False,
            supports_structured_outputs=True,
            supports_json=False,
            supports_temperature=False,
            input_price=0.05 / 1e6,
            output_price=0.40 / 1e6,
        ),
        "gpt-5-chat-latest": make_model_data(
            supports_log_probs=False,
            supports_multimodal=False,
            supports_structured_outputs=False,
            supports_json=False,
            input_price=1.25 / 1e6,
            output_price=10.00 / 1e6,
        ),
        "gpt-5.1": make_model_data(
            supports_log_probs=False,
            supports_multimodal=True,
            supports_structured_outputs=True,
            supports_json=False,
            supports_temperature=False,
            input_price=1.25 / 1e6,
            output_price=10.00 / 1e6,
        ),
        "gpt-5.2": make_model_data(
            supports_log_probs=False,
            supports_multimodal=True,
            supports_structured_outputs=True,
            supports_json=False,
            supports_temperature=False,
            input_price=1.75 / 1e6,
            output_price=14.00 / 1e6,
        ),
    }
)


ANTHROPIC_MODELS_DATA = ModelDataRegistry(
    {
        "claude-3-opus-20240229": make_model_data(
            supports_log_probs=False,
            supports_multimodal=True,
            supports_structured_outputs=False,
            supports_json=True,
            input_price=15.00 / 1e6,
            output_price=75.00 / 1e6,
        ),
        "claude-3-sonnet-20240229": make_model_data(
            supports_log_probs=False,
            supports_multimodal=True,
            supports_structured_outputs=False,
            supports_json=True,
            input_price=3.00 / 1e6,
            output_price=15.00 / 1e6,
        ),
        "claude-3-haiku-20240307": make_model_data(
            supports_log_probs=False,
            supports_multimodal=True,
            supports_structured_outputs=False,
            supports_json=True,
            input_price=0.25 / 1e6,
            output_price=1.25 / 1e6,
        ),
        "claude-3-5-sonnet-20240620": make_model_data(
            supports_log_probs=False,
            supports_multimodal=True,
            supports_structured_outputs=True,
            supports_json=True,
            input_price=3.00 / 1e6,
            output_price=15.00 / 1e6,
        ),
        "claude-3-5-sonnet-20241022": make_model_data(
            supports_log_probs=False,
            supports_multimodal=True,
            supports_structured_outputs=True,
            supports_json=True,
            input_price=3.00 / 1e6,
            output_price=15.00 / 1e6,
        ),
        "claude-3-5-haiku-20241022": make_model_data(
            supports_log_probs=False,
            supports_multimodal=True,
            supports_structured_outputs=True,
            supports_json=True,
            input_price=1.00 / 1e6,
            output_price=5.00 / 1e6,
        ),
        "claude-3-7-sonnet-20250219": make_model_data(
            supports_log_probs=False,
            supports_multimodal=True,
            supports_structured_outputs=True,
            supports_json=True,
            input_price=3.00 / 1e6,
            output_price=15.00 / 1e6,
        ),
        "claude-opus-4-20250514": make_model_data(
            supports_log_probs=False,
            supports_multimodal=True,
            supports_structured_outputs=True,
            supports_json=True,
            input_price=15.00 / 1e6,
            output_price=75.00 / 1e6,
        ),
        "claude-opus-4-1-20250805": make_model_data(
            supports_log_probs=False,
            supports_multimodal=True,
            supports_structured_outputs=True,
            supports_json=True,
            input_price=15.00 / 1e6,
            output_price=75.00 / 1e6,
        ),
        "claude-sonnet-4-20250514": make_model_data(
            supports_log_probs=False,
            supports_multimodal=True,
            supports_structured_outputs=True,
            supports_json=True,
            input_price=3.00 / 1e6,
            output_price=15.00 / 1e6,
        ),
        "claude-sonnet-4-5-20250929": make_model_data(
            supports_log_probs=False,
            supports_multimodal=True,
            supports_structured_outputs=True,
            supports_json=True,
            input_price=3.00 / 1e6,
            output_price=15.00 / 1e6,
        ),
        "claude-haiku-4-5-20251001": make_model_data(
            supports_log_probs=False,
            supports_multimodal=True,
            supports_structured_outputs=True,
            supports_json=True,
            input_price=1.00 / 1e6,
            output_price=5.00 / 1e6,
        ),
        "claude-opus-4-5-20251124": make_model_data(
            supports_log_probs=False,
            supports_multimodal=True,
            supports_structured_outputs=True,
            supports_json=True,
            input_price=15.00 / 1e6,
            output_price=75.00 / 1e6,
        ),
        "claude-3-opus": make_model_data(
            supports_log_probs=False,
            supports_multimodal=True,
            supports_structured_outputs=False,
            supports_json=True,
            input_price=15.00 / 1e6,
            output_price=75.00 / 1e6,
        ),
        "claude-3-sonnet": make_model_data(
            supports_log_probs=False,
            supports_multimodal=True,
            supports_structured_outputs=False,
            supports_json=True,
            input_price=3.00 / 1e6,
            output_price=15.00 / 1e6,
        ),
        "claude-3-haiku": make_model_data(
            supports_log_probs=False,
            supports_multimodal=True,
            supports_structured_outputs=False,
            supports_json=True,
            input_price=0.25 / 1e6,
            output_price=1.25 / 1e6,
        ),
        "claude-3-5-sonnet": make_model_data(
            supports_log_probs=False,
            supports_multimodal=True,
            supports_structured_outputs=True,
            supports_json=True,
            input_price=3.00 / 1e6,
            output_price=15.00 / 1e6,
        ),
        "claude-3-5-haiku": make_model_data(
            supports_log_probs=False,
            supports_multimodal=True,
            supports_structured_outputs=True,
            supports_json=True,
            input_price=1.00 / 1e6,
            output_price=5.00 / 1e6,
        ),
        "claude-opus-4": make_model_data(
            supports_log_probs=False,
            supports_multimodal=True,
            supports_structured_outputs=True,
            supports_json=True,
            input_price=15.00 / 1e6,
            output_price=75.00 / 1e6,
        ),
        "claude-sonnet-4": make_model_data(
            supports_log_probs=False,
            supports_multimodal=True,
            supports_structured_outputs=True,
            supports_json=True,
            input_price=3.00 / 1e6,
            output_price=15.00 / 1e6,
        ),
        "claude-sonnet-4-5": make_model_data(
            supports_log_probs=False,
            supports_multimodal=True,
            supports_structured_outputs=True,
            supports_json=True,
            input_price=3.00 / 1e6,
            output_price=15.00 / 1e6,
        ),
        "claude-haiku-4-5": make_model_data(
            supports_log_probs=False,
            supports_multimodal=True,
            supports_structured_outputs=True,
            supports_json=True,
            input_price=1.00 / 1e6,
            output_price=5.00 / 1e6,
        ),
        "claude-opus-4-5": make_model_data(
            supports_log_probs=False,
            supports_multimodal=True,
            supports_structured_outputs=True,
            supports_json=True,
            input_price=15.00 / 1e6,
            output_price=75.00 / 1e6,
        ),
    }
)


GEMINI_MODELS_DATA = ModelDataRegistry(
    {
        "gemini-1.5-pro": make_model_data(
            supports_log_probs=False,
            supports_multimodal=True,
            supports_structured_outputs=True,
            supports_json=True,
            input_price=1.25 / 1e6,
            output_price=5.00 / 1e6,
        ),
        "gemini-1.5-pro-002": make_model_data(
            supports_log_probs=False,
            supports_multimodal=True,
            supports_structured_outputs=True,
            supports_json=True,
            input_price=1.25 / 1e6,
            output_price=5.00 / 1e6,
        ),
        "gemini-1.5-flash": make_model_data(
            supports_log_probs=False,
            supports_multimodal=True,
            supports_structured_outputs=True,
            supports_json=True,
            input_price=0.075 / 1e6,
            output_price=0.30 / 1e6,
        ),
        "gemini-1.5-flash-002": make_model_data(
            supports_log_probs=False,
            supports_multimodal=True,
            supports_structured_outputs=True,
            supports_json=True,
            input_price=0.075 / 1e6,
            output_price=0.30 / 1e6,
        ),
        "gemini-1.5-flash-8b": make_model_data(
            supports_log_probs=False,
            supports_multimodal=True,
            supports_structured_outputs=True,
            supports_json=True,
            input_price=0.0375 / 1e6,
            output_price=0.15 / 1e6,
        ),
        "gemini-2.0-flash": make_model_data(
            supports_log_probs=False,
            supports_multimodal=True,
            supports_structured_outputs=True,
            supports_json=True,
            input_price=0.15 / 1e6,
            output_price=0.60 / 1e6,
        ),
        "gemini-2.0-flash-lite": make_model_data(
            supports_log_probs=False,
            supports_multimodal=True,
            supports_structured_outputs=True,
            supports_json=True,
            input_price=0.075 / 1e6,
            output_price=0.30 / 1e6,
        ),
        "gemini-2.5-pro": make_model_data(
            supports_log_probs=False,
            supports_multimodal=True,
            supports_structured_outputs=True,
            supports_json=True,
            input_price=1.25 / 1e6,
            output_price=10.00 / 1e6,
        ),
        "gemini-2.5-flash": make_model_data(
            supports_log_probs=False,
            supports_multimodal=True,
            supports_structured_outputs=True,
            supports_json=True,
            input_price=0.15 / 1e6,
            output_price=0.60 / 1e6,
        ),
        "gemini-2.5-flash-lite": make_model_data(
            supports_log_probs=False,
            supports_multimodal=True,
            supports_structured_outputs=True,
            supports_json=True,
            input_price=0.075 / 1e6,
            output_price=0.30 / 1e6,
        ),
        "gemini-3-pro": make_model_data(
            supports_log_probs=False,
            supports_multimodal=True,
            supports_structured_outputs=True,
            supports_json=True,
            input_price=1.25 / 1e6,
            output_price=10.00 / 1e6,
        ),
        "gemini-3-pro-preview": make_model_data(
            supports_log_probs=False,
            supports_multimodal=True,
            supports_structured_outputs=True,
            supports_json=True,
            input_price=1.25 / 1e6,
            output_price=10.00 / 1e6,
        ),
        "gemini-pro": make_model_data(
            supports_log_probs=False,
            supports_multimodal=False,
            supports_structured_outputs=True,
            supports_json=True,
            input_price=0.50 / 1e6,
            output_price=1.50 / 1e6,
        ),
        "gemini-pro-vision": make_model_data(
            supports_log_probs=False,
            supports_multimodal=True,
            supports_structured_outputs=True,
            supports_json=True,
            input_price=0.50 / 1e6,
            output_price=1.50 / 1e6,
        ),
    }
)


GROK_MODELS_DATA = ModelDataRegistry(
    {
        "grok-3": make_model_data(
            supports_log_probs=False,
            supports_multimodal=False,
            supports_structured_outputs=True,
            supports_json=True,
            input_price=None,
            output_price=None,
        ),
        "grok-4": make_model_data(
            supports_log_probs=False,
            supports_multimodal=True,
            supports_structured_outputs=True,
            supports_json=True,
            input_price=3.00 / 1e6,
            output_price=15.00 / 1e6,
        ),
        "grok-4-fast": make_model_data(
            supports_log_probs=False,
            supports_multimodal=False,
            supports_structured_outputs=True,
            supports_json=True,
            input_price=0.20 / 1e6,
            output_price=0.50 / 1e6,
        ),
        "grok-4-heavy": make_model_data(
            supports_log_probs=False,
            supports_multimodal=True,
            supports_structured_outputs=True,
            supports_json=True,
            input_price=3.00 / 1e6,
            output_price=15.00 / 1e6,
        ),
        "grok-4.1": make_model_data(
            supports_log_probs=False,
            supports_multimodal=True,
            supports_structured_outputs=True,
            supports_json=True,
            input_price=3.00 / 1e6,
            output_price=15.00 / 1e6,
        ),
        "grok-beta": make_model_data(
            supports_log_probs=False,
            supports_multimodal=True,
            supports_structured_outputs=True,
            supports_json=True,
            input_price=5.00 / 1e6,
            output_price=15.00 / 1e6,
        ),
        "grok-2": make_model_data(
            supports_log_probs=False,
            supports_multimodal=True,
            supports_structured_outputs=True,
            supports_json=True,
            input_price=2.00 / 1e6,
            output_price=10.00 / 1e6,
        ),
        "grok-2-mini": make_model_data(
            supports_log_probs=False,
            supports_multimodal=True,
            supports_structured_outputs=True,
            supports_json=True,
            input_price=0.50 / 1e6,
            output_price=2.00 / 1e6,
        ),
        "grok-code-fast-1": make_model_data(
            supports_log_probs=False,
            supports_multimodal=False,
            supports_structured_outputs=True,
            supports_json=True,
            input_price=0.20 / 1e6,
            output_price=1.50 / 1e6,
        ),
    }
)


KIMI_MODELS_DATA = ModelDataRegistry(
    {
        "kimi-k2": make_model_data(
            supports_log_probs=False,
            supports_multimodal=True,
            supports_structured_outputs=True,
            supports_json=True,
            input_price=0.58 / 1e6,
            output_price=2.29 / 1e6,
        ),
        "kimi-k2-instruct": make_model_data(
            supports_log_probs=False,
            supports_multimodal=True,
            supports_structured_outputs=True,
            supports_json=True,
            input_price=0.58 / 1e6,
            output_price=2.29 / 1e6,
        ),
        "kimi-k2-base": make_model_data(
            supports_log_probs=False,
            supports_multimodal=True,
            supports_structured_outputs=True,
            supports_json=True,
            input_price=None,
            output_price=None,
        ),
        "moonshot-v1-8k": make_model_data(
            supports_log_probs=False,
            supports_multimodal=False,
            supports_structured_outputs=True,
            supports_json=True,
            input_price=0.15 / 1e6,
            output_price=2.50 / 1e6,
        ),
        "moonshot-v1-32k": make_model_data(
            supports_log_probs=False,
            supports_multimodal=False,
            supports_structured_outputs=True,
            supports_json=True,
            input_price=0.15 / 1e6,
            output_price=2.50 / 1e6,
        ),
        "moonshot-v1-128k": make_model_data(
            supports_log_probs=False,
            supports_multimodal=False,
            supports_structured_outputs=True,
            supports_json=True,
            input_price=0.15 / 1e6,
            output_price=2.50 / 1e6,
        ),
    }
)


DEEPSEEK_MODELS_DATA = ModelDataRegistry(
    {
        "deepseek-chat": make_model_data(
            supports_log_probs=False,
            supports_multimodal=False,
            supports_structured_outputs=True,
            supports_json=True,
            input_price=0.028 / 1e6,
            output_price=0.42 / 1e6,
        ),
        "deepseek-v3.2": make_model_data(
            supports_log_probs=False,
            supports_multimodal=False,
            supports_structured_outputs=True,
            supports_json=True,
            input_price=0.028 / 1e6,
            output_price=0.42 / 1e6,
        ),
        "deepseek-v3.2-exp": make_model_data(
            supports_log_probs=False,
            supports_multimodal=False,
            supports_structured_outputs=True,
            supports_json=True,
            input_price=0.028 / 1e6,
            output_price=0.42 / 1e6,
        ),
        "deepseek-v3.1": make_model_data(
            supports_log_probs=False,
            supports_multimodal=False,
            supports_structured_outputs=True,
            supports_json=True,
            input_price=0.14 / 1e6,
            output_price=0.28 / 1e6,
        ),
        "deepseek-v3": make_model_data(
            supports_log_probs=False,
            supports_multimodal=False,
            supports_structured_outputs=True,
            supports_json=True,
            input_price=0.14 / 1e6,
            output_price=0.28 / 1e6,
        ),
        "deepseek-reasoner": make_model_data(
            supports_log_probs=False,
            supports_multimodal=False,
            supports_structured_outputs=True,
            supports_json=True,
            input_price=0.14 / 1e6,
            output_price=2.19 / 1e6,
        ),
        "deepseek-r1": make_model_data(
            supports_log_probs=False,
            supports_multimodal=False,
            supports_structured_outputs=True,
            supports_json=True,
            input_price=0.14 / 1e6,
            output_price=2.19 / 1e6,
        ),
        "deepseek-r1-lite": make_model_data(
            supports_log_probs=False,
            supports_multimodal=False,
            supports_structured_outputs=True,
            supports_json=True,
            input_price=0.055 / 1e6,
            output_price=0.28 / 1e6,
        ),
        "deepseek-v2.5": make_model_data(
            supports_log_probs=False,
            supports_multimodal=False,
            supports_structured_outputs=True,
            supports_json=True,
            input_price=0.14 / 1e6,
            output_price=0.28 / 1e6,
        ),
        "deepseek-coder": make_model_data(
            supports_log_probs=False,
            supports_multimodal=False,
            supports_structured_outputs=True,
            supports_json=True,
            input_price=0.14 / 1e6,
            output_price=0.28 / 1e6,
        ),
        "deepseek-coder-6.7b": make_model_data(
            supports_log_probs=False,
            supports_multimodal=False,
            supports_structured_outputs=True,
            supports_json=True,
            input_price=0.20 / 1e6,
            output_price=0.40 / 1e6,
        ),
        "deepseek-coder-33b": make_model_data(
            supports_log_probs=False,
            supports_multimodal=False,
            supports_structured_outputs=True,
            supports_json=True,
            input_price=1.00 / 1e6,
            output_price=2.00 / 1e6,
        ),
    }
)


OLLAMA_MODELS_DATA = ModelDataRegistry(
    {
        "qwen3": make_model_data(
            supports_log_probs=True,
            supports_multimodal=False,
            supports_structured_outputs=True,
            supports_json=True,
            input_price=None,
            output_price=None,
        ),
        "qwen3:8b": make_model_data(
            supports_log_probs=True,
            supports_multimodal=False,
            supports_structured_outputs=True,
            supports_json=True,
            input_price=None,
            output_price=None,
        ),
        "qwen3:14b": make_model_data(
            supports_log_probs=True,
            supports_multimodal=False,
            supports_structured_outputs=True,
            supports_json=True,
            input_price=None,
            output_price=None,
        ),
        "qwen3:30b": make_model_data(
            supports_log_probs=True,
            supports_multimodal=False,
            supports_structured_outputs=True,
            supports_json=True,
            input_price=None,
            output_price=None,
        ),
        "qwen3-vl": make_model_data(
            supports_log_probs=True,
            supports_multimodal=True,
            supports_structured_outputs=True,
            supports_json=True,
            input_price=None,
            output_price=None,
        ),
        "qwen3-coder": make_model_data(
            supports_log_probs=True,
            supports_multimodal=False,
            supports_structured_outputs=True,
            supports_json=True,
            input_price=None,
            output_price=None,
        ),
        "qwen2.5": make_model_data(
            supports_log_probs=True,
            supports_multimodal=False,
            supports_structured_outputs=True,
            supports_json=True,
            input_price=None,
            output_price=None,
        ),
        "qwen2.5:7b": make_model_data(
            supports_log_probs=True,
            supports_multimodal=False,
            supports_structured_outputs=True,
            supports_json=True,
            input_price=None,
            output_price=None,
        ),
        "qwen2.5:14b": make_model_data(
            supports_log_probs=True,
            supports_multimodal=False,
            supports_structured_outputs=True,
            supports_json=True,
            input_price=None,
            output_price=None,
        ),
        "qwen2.5:32b": make_model_data(
            supports_log_probs=True,
            supports_multimodal=False,
            supports_structured_outputs=True,
            supports_json=True,
            input_price=None,
            output_price=None,
        ),
        "qwen2.5:72b": make_model_data(
            supports_log_probs=True,
            supports_multimodal=False,
            supports_structured_outputs=True,
            supports_json=True,
            input_price=None,
            output_price=None,
        ),
        "qwen2.5-coder": make_model_data(
            supports_log_probs=True,
            supports_multimodal=False,
            supports_structured_outputs=True,
            supports_json=True,
            input_price=None,
            output_price=None,
        ),
        "deepseek-r1": make_model_data(
            supports_log_probs=True,
            supports_multimodal=False,
            supports_structured_outputs=True,
            supports_json=True,
            input_price=None,
            output_price=None,
        ),
        "deepseek-r1:7b": make_model_data(
            supports_log_probs=True,
            supports_multimodal=False,
            supports_structured_outputs=True,
            supports_json=True,
            input_price=None,
            output_price=None,
        ),
        "deepseek-r1:14b": make_model_data(
            supports_log_probs=True,
            supports_multimodal=False,
            supports_structured_outputs=True,
            supports_json=True,
            input_price=None,
            output_price=None,
        ),
        "deepseek-r1:32b": make_model_data(
            supports_log_probs=True,
            supports_multimodal=False,
            supports_structured_outputs=True,
            supports_json=True,
            input_price=None,
            output_price=None,
        ),
        "deepseek-r1:70b": make_model_data(
            supports_log_probs=True,
            supports_multimodal=False,
            supports_structured_outputs=True,
            supports_json=True,
            input_price=None,
            output_price=None,
        ),
        "deepseek-r1:671b": make_model_data(
            supports_log_probs=True,
            supports_multimodal=False,
            supports_structured_outputs=True,
            supports_json=True,
            input_price=None,
            output_price=None,
        ),
        "deepseek-v3.1": make_model_data(
            supports_log_probs=True,
            supports_multimodal=False,
            supports_structured_outputs=True,
            supports_json=True,
            input_price=None,
            output_price=None,
        ),
        "deepseek-v3": make_model_data(
            supports_log_probs=True,
            supports_multimodal=False,
            supports_structured_outputs=True,
            supports_json=True,
            input_price=None,
            output_price=None,
        ),
        "deepseek-coder": make_model_data(
            supports_log_probs=True,
            supports_multimodal=False,
            supports_structured_outputs=True,
            supports_json=True,
            input_price=None,
            output_price=None,
        ),
        "deepseek-coder:6.7b": make_model_data(
            supports_log_probs=True,
            supports_multimodal=False,
            supports_structured_outputs=True,
            supports_json=True,
            input_price=None,
            output_price=None,
        ),
        "deepseek-coder:33b": make_model_data(
            supports_log_probs=True,
            supports_multimodal=False,
            supports_structured_outputs=True,
            supports_json=True,
            input_price=None,
            output_price=None,
        ),
        "deepseek-coder-v2": make_model_data(
            supports_log_probs=True,
            supports_multimodal=False,
            supports_structured_outputs=True,
            supports_json=True,
            input_price=None,
            output_price=None,
        ),
        "gemma3": make_model_data(
            supports_log_probs=True,
            supports_multimodal=True,
            supports_structured_outputs=True,
            supports_json=True,
            input_price=None,
            output_price=None,
        ),
        "gemma3:1b": make_model_data(
            supports_log_probs=True,
            supports_multimodal=False,
            supports_structured_outputs=True,
            supports_json=True,
            input_price=None,
            output_price=None,
        ),
        "gemma3:4b": make_model_data(
            supports_log_probs=True,
            supports_multimodal=False,
            supports_structured_outputs=True,
            supports_json=True,
            input_price=None,
            output_price=None,
        ),
        "gemma3:12b": make_model_data(
            supports_log_probs=True,
            supports_multimodal=False,
            supports_structured_outputs=True,
            supports_json=True,
            input_price=None,
            output_price=None,
        ),
        "gemma3:27b": make_model_data(
            supports_log_probs=True,
            supports_multimodal=False,
            supports_structured_outputs=True,
            supports_json=True,
            input_price=None,
            output_price=None,
        ),
        "gemma2": make_model_data(
            supports_log_probs=True,
            supports_multimodal=False,
            supports_structured_outputs=True,
            supports_json=True,
            input_price=None,
            output_price=None,
        ),
        "gemma2:2b": make_model_data(
            supports_log_probs=True,
            supports_multimodal=False,
            supports_structured_outputs=True,
            supports_json=True,
            input_price=None,
            output_price=None,
        ),
        "gemma2:9b": make_model_data(
            supports_log_probs=True,
            supports_multimodal=False,
            supports_structured_outputs=True,
            supports_json=True,
            input_price=None,
            output_price=None,
        ),
        "gemma2:27b": make_model_data(
            supports_log_probs=True,
            supports_multimodal=False,
            supports_structured_outputs=True,
            supports_json=True,
            input_price=None,
            output_price=None,
        ),
        "llama3.3": make_model_data(
            supports_log_probs=True,
            supports_multimodal=False,
            supports_structured_outputs=True,
            supports_json=True,
            input_price=None,
            output_price=None,
        ),
        "llama3.3:70b": make_model_data(
            supports_log_probs=True,
            supports_multimodal=False,
            supports_structured_outputs=True,
            supports_json=True,
            input_price=None,
            output_price=None,
        ),
        "llama3.2": make_model_data(
            supports_log_probs=True,
            supports_multimodal=False,
            supports_structured_outputs=True,
            supports_json=True,
            input_price=None,
            output_price=None,
        ),
        "llama3.2:1b": make_model_data(
            supports_log_probs=True,
            supports_multimodal=False,
            supports_structured_outputs=True,
            supports_json=True,
            input_price=None,
            output_price=None,
        ),
        "llama3.2:3b": make_model_data(
            supports_log_probs=True,
            supports_multimodal=False,
            supports_structured_outputs=True,
            supports_json=True,
            input_price=None,
            output_price=None,
        ),
        "llama3.2-vision": make_model_data(
            supports_log_probs=True,
            supports_multimodal=True,
            supports_structured_outputs=True,
            supports_json=True,
            input_price=None,
            output_price=None,
        ),
        "llama3.2-vision:11b": make_model_data(
            supports_log_probs=True,
            supports_multimodal=True,
            supports_structured_outputs=True,
            supports_json=True,
            input_price=None,
            output_price=None,
        ),
        "llama3.2-vision:90b": make_model_data(
            supports_log_probs=True,
            supports_multimodal=True,
            supports_structured_outputs=True,
            supports_json=True,
            input_price=None,
            output_price=None,
        ),
        "llama3.1": make_model_data(
            supports_log_probs=True,
            supports_multimodal=False,
            supports_structured_outputs=True,
            supports_json=True,
            input_price=None,
            output_price=None,
        ),
        "llama3.1:8b": make_model_data(
            supports_log_probs=True,
            supports_multimodal=False,
            supports_structured_outputs=True,
            supports_json=True,
            input_price=None,
            output_price=None,
        ),
        "llama3.1:70b": make_model_data(
            supports_log_probs=True,
            supports_multimodal=False,
            supports_structured_outputs=True,
            supports_json=True,
            input_price=None,
            output_price=None,
        ),
        "llama3.1:405b": make_model_data(
            supports_log_probs=True,
            supports_multimodal=False,
            supports_structured_outputs=True,
            supports_json=True,
            input_price=None,
            output_price=None,
        ),
        "llama3": make_model_data(
            supports_log_probs=True,
            supports_multimodal=False,
            supports_structured_outputs=True,
            supports_json=True,
            input_price=None,
            output_price=None,
        ),
        "llama3:8b": make_model_data(
            supports_log_probs=True,
            supports_multimodal=False,
            supports_structured_outputs=True,
            supports_json=True,
            input_price=None,
            output_price=None,
        ),
        "llama3:70b": make_model_data(
            supports_log_probs=True,
            supports_multimodal=False,
            supports_structured_outputs=True,
            supports_json=True,
            input_price=None,
            output_price=None,
        ),
        "llama2": make_model_data(
            supports_log_probs=True,
            supports_multimodal=False,
            supports_structured_outputs=True,
            supports_json=True,
            input_price=None,
            output_price=None,
        ),
        "llama2:7b": make_model_data(
            supports_log_probs=True,
            supports_multimodal=False,
            supports_structured_outputs=True,
            supports_json=True,
            input_price=None,
            output_price=None,
        ),
        "llama2:13b": make_model_data(
            supports_log_probs=True,
            supports_multimodal=False,
            supports_structured_outputs=True,
            supports_json=True,
            input_price=None,
            output_price=None,
        ),
        "llama2:70b": make_model_data(
            supports_log_probs=True,
            supports_multimodal=False,
            supports_structured_outputs=True,
            supports_json=True,
            input_price=None,
            output_price=None,
        ),
        "llama4": make_model_data(
            supports_log_probs=True,
            supports_multimodal=True,
            supports_structured_outputs=True,
            supports_json=True,
            input_price=None,
            output_price=None,
        ),
        "mistral": make_model_data(
            supports_log_probs=True,
            supports_multimodal=False,
            supports_structured_outputs=True,
            supports_json=True,
            input_price=None,
            output_price=None,
        ),
        "mistral:7b": make_model_data(
            supports_log_probs=True,
            supports_multimodal=False,
            supports_structured_outputs=True,
            supports_json=True,
            input_price=None,
            output_price=None,
        ),
        "mistral-nemo": make_model_data(
            supports_log_probs=True,
            supports_multimodal=False,
            supports_structured_outputs=True,
            supports_json=True,
            input_price=None,
            output_price=None,
        ),
        "mistral-small": make_model_data(
            supports_log_probs=True,
            supports_multimodal=False,
            supports_structured_outputs=True,
            supports_json=True,
            input_price=None,
            output_price=None,
        ),
        "mistral-large": make_model_data(
            supports_log_probs=True,
            supports_multimodal=False,
            supports_structured_outputs=True,
            supports_json=True,
            input_price=None,
            output_price=None,
        ),
        "mixtral": make_model_data(
            supports_log_probs=True,
            supports_multimodal=False,
            supports_structured_outputs=True,
            supports_json=True,
            input_price=None,
            output_price=None,
        ),
        "mixtral:8x7b": make_model_data(
            supports_log_probs=True,
            supports_multimodal=False,
            supports_structured_outputs=True,
            supports_json=True,
            input_price=None,
            output_price=None,
        ),
        "mixtral:8x22b": make_model_data(
            supports_log_probs=True,
            supports_multimodal=False,
            supports_structured_outputs=True,
            supports_json=True,
            input_price=None,
            output_price=None,
        ),
        "ministral-3": make_model_data(
            supports_log_probs=True,
            supports_multimodal=True,
            supports_structured_outputs=True,
            supports_json=True,
            input_price=None,
            output_price=None,
        ),
        "codestral": make_model_data(
            supports_log_probs=True,
            supports_multimodal=False,
            supports_structured_outputs=True,
            supports_json=True,
            input_price=None,
            output_price=None,
        ),
        "phi4": make_model_data(
            supports_log_probs=True,
            supports_multimodal=False,
            supports_structured_outputs=True,
            supports_json=True,
            input_price=None,
            output_price=None,
        ),
        "phi4:14b": make_model_data(
            supports_log_probs=True,
            supports_multimodal=False,
            supports_structured_outputs=True,
            supports_json=True,
            input_price=None,
            output_price=None,
        ),
        "phi3": make_model_data(
            supports_log_probs=True,
            supports_multimodal=False,
            supports_structured_outputs=True,
            supports_json=True,
            input_price=None,
            output_price=None,
        ),
        "phi3:3.8b": make_model_data(
            supports_log_probs=True,
            supports_multimodal=False,
            supports_structured_outputs=True,
            supports_json=True,
            input_price=None,
            output_price=None,
        ),
        "phi3:14b": make_model_data(
            supports_log_probs=True,
            supports_multimodal=False,
            supports_structured_outputs=True,
            supports_json=True,
            input_price=None,
            output_price=None,
        ),
        "llava": make_model_data(
            supports_log_probs=True,
            supports_multimodal=True,
            supports_structured_outputs=True,
            supports_json=True,
            input_price=None,
            output_price=None,
        ),
        "llava:7b": make_model_data(
            supports_log_probs=True,
            supports_multimodal=True,
            supports_structured_outputs=True,
            supports_json=True,
            input_price=None,
            output_price=None,
        ),
        "llava:13b": make_model_data(
            supports_log_probs=True,
            supports_multimodal=True,
            supports_structured_outputs=True,
            supports_json=True,
            input_price=None,
            output_price=None,
        ),
        "llava:34b": make_model_data(
            supports_log_probs=True,
            supports_multimodal=True,
            supports_structured_outputs=True,
            supports_json=True,
            input_price=None,
            output_price=None,
        ),
        "minicpm-v": make_model_data(
            supports_log_probs=True,
            supports_multimodal=True,
            supports_structured_outputs=True,
            supports_json=True,
            input_price=None,
            output_price=None,
        ),
        "moondream": make_model_data(
            supports_log_probs=True,
            supports_multimodal=True,
            supports_structured_outputs=True,
            supports_json=True,
            input_price=None,
            output_price=None,
        ),
        "codellama": make_model_data(
            supports_log_probs=True,
            supports_multimodal=False,
            supports_structured_outputs=True,
            supports_json=True,
            input_price=None,
            output_price=None,
        ),
        "codellama:7b": make_model_data(
            supports_log_probs=True,
            supports_multimodal=False,
            supports_structured_outputs=True,
            supports_json=True,
            input_price=None,
            output_price=None,
        ),
        "codellama:13b": make_model_data(
            supports_log_probs=True,
            supports_multimodal=False,
            supports_structured_outputs=True,
            supports_json=True,
            input_price=None,
            output_price=None,
        ),
        "codellama:34b": make_model_data(
            supports_log_probs=True,
            supports_multimodal=False,
            supports_structured_outputs=True,
            supports_json=True,
            input_price=None,
            output_price=None,
        ),
        "codellama:70b": make_model_data(
            supports_log_probs=True,
            supports_multimodal=False,
            supports_structured_outputs=True,
            supports_json=True,
            input_price=None,
            output_price=None,
        ),
        "starcoder2": make_model_data(
            supports_log_probs=True,
            supports_multimodal=False,
            supports_structured_outputs=True,
            supports_json=True,
            input_price=None,
            output_price=None,
        ),
        "starcoder2:3b": make_model_data(
            supports_log_probs=True,
            supports_multimodal=False,
            supports_structured_outputs=True,
            supports_json=True,
            input_price=None,
            output_price=None,
        ),
        "starcoder2:7b": make_model_data(
            supports_log_probs=True,
            supports_multimodal=False,
            supports_structured_outputs=True,
            supports_json=True,
            input_price=None,
            output_price=None,
        ),
        "starcoder2:15b": make_model_data(
            supports_log_probs=True,
            supports_multimodal=False,
            supports_structured_outputs=True,
            supports_json=True,
            input_price=None,
            output_price=None,
        ),
        "codegemma": make_model_data(
            supports_log_probs=True,
            supports_multimodal=False,
            supports_structured_outputs=True,
            supports_json=True,
            input_price=None,
            output_price=None,
        ),
        "codegemma:2b": make_model_data(
            supports_log_probs=True,
            supports_multimodal=False,
            supports_structured_outputs=True,
            supports_json=True,
            input_price=None,
            output_price=None,
        ),
        "codegemma:7b": make_model_data(
            supports_log_probs=True,
            supports_multimodal=False,
            supports_structured_outputs=True,
            supports_json=True,
            input_price=None,
            output_price=None,
        ),
        "tinyllama": make_model_data(
            supports_log_probs=True,
            supports_multimodal=False,
            supports_structured_outputs=True,
            supports_json=True,
            input_price=None,
            output_price=None,
        ),
        "tinyllama:1.1b": make_model_data(
            supports_log_probs=True,
            supports_multimodal=False,
            supports_structured_outputs=True,
            supports_json=True,
            input_price=None,
            output_price=None,
        ),
        "smollm2": make_model_data(
            supports_log_probs=True,
            supports_multimodal=False,
            supports_structured_outputs=True,
            supports_json=True,
            input_price=None,
            output_price=None,
        ),
        "smollm2:135m": make_model_data(
            supports_log_probs=True,
            supports_multimodal=False,
            supports_structured_outputs=True,
            supports_json=True,
            input_price=None,
            output_price=None,
        ),
        "smollm2:360m": make_model_data(
            supports_log_probs=True,
            supports_multimodal=False,
            supports_structured_outputs=True,
            supports_json=True,
            input_price=None,
            output_price=None,
        ),
        "smollm2:1.7b": make_model_data(
            supports_log_probs=True,
            supports_multimodal=False,
            supports_structured_outputs=True,
            supports_json=True,
            input_price=None,
            output_price=None,
        ),
        # IBM Granite Models
        "granite4": make_model_data(
            supports_log_probs=True,
            supports_multimodal=False,
            supports_structured_outputs=True,
            supports_json=True,
            input_price=None,
            output_price=None,
        ),
        "granite3.3": make_model_data(
            supports_log_probs=True,
            supports_multimodal=False,
            supports_structured_outputs=True,
            supports_json=True,
            input_price=None,
            output_price=None,
        ),
        "granite3.1-moe": make_model_data(
            supports_log_probs=True,
            supports_multimodal=False,
            supports_structured_outputs=True,
            supports_json=True,
            input_price=None,
            output_price=None,
        ),
        "granite-code": make_model_data(
            supports_log_probs=True,
            supports_multimodal=False,
            supports_structured_outputs=True,
            supports_json=True,
            input_price=None,
            output_price=None,
        ),
        # Embedding Models
        "nomic-embed-text": make_model_data(
            supports_log_probs=False,
            supports_multimodal=False,
            supports_structured_outputs=False,
            supports_json=False,
            input_price=None,
            output_price=None,
        ),
        "mxbai-embed-large": make_model_data(
            supports_log_probs=False,
            supports_multimodal=False,
            supports_structured_outputs=False,
            supports_json=False,
            input_price=None,
            output_price=None,
        ),
        "bge-m3": make_model_data(
            supports_log_probs=False,
            supports_multimodal=False,
            supports_structured_outputs=False,
            supports_json=False,
            input_price=None,
            output_price=None,
        ),
        "bge-large": make_model_data(
            supports_log_probs=False,
            supports_multimodal=False,
            supports_structured_outputs=False,
            supports_json=False,
            input_price=None,
            output_price=None,
        ),
        "all-minilm": make_model_data(
            supports_log_probs=False,
            supports_multimodal=False,
            supports_structured_outputs=False,
            supports_json=False,
            input_price=None,
            output_price=None,
        ),
        "snowflake-arctic-embed": make_model_data(
            supports_log_probs=False,
            supports_multimodal=False,
            supports_structured_outputs=False,
            supports_json=False,
            input_price=None,
            output_price=None,
        ),
        "dolphin3": make_model_data(
            supports_log_probs=True,
            supports_multimodal=False,
            supports_structured_outputs=True,
            supports_json=True,
            input_price=None,
            output_price=None,
        ),
        "dolphin-llama3": make_model_data(
            supports_log_probs=True,
            supports_multimodal=False,
            supports_structured_outputs=True,
            supports_json=True,
            input_price=None,
            output_price=None,
        ),
        "dolphin-mixtral": make_model_data(
            supports_log_probs=True,
            supports_multimodal=False,
            supports_structured_outputs=True,
            supports_json=True,
            input_price=None,
            output_price=None,
        ),
        "orca-mini": make_model_data(
            supports_log_probs=True,
            supports_multimodal=False,
            supports_structured_outputs=True,
            supports_json=True,
            input_price=None,
            output_price=None,
        ),
        "orca2": make_model_data(
            supports_log_probs=True,
            supports_multimodal=False,
            supports_structured_outputs=True,
            supports_json=True,
            input_price=None,
            output_price=None,
        ),
        "vicuna": make_model_data(
            supports_log_probs=True,
            supports_multimodal=False,
            supports_structured_outputs=True,
            supports_json=True,
            input_price=None,
            output_price=None,
        ),
        "nous-hermes2": make_model_data(
            supports_log_probs=True,
            supports_multimodal=False,
            supports_structured_outputs=True,
            supports_json=True,
            input_price=None,
            output_price=None,
        ),
        "command-r": make_model_data(
            supports_log_probs=True,
            supports_multimodal=False,
            supports_structured_outputs=True,
            supports_json=True,
            input_price=None,
            output_price=None,
        ),
        "command-r-plus": make_model_data(
            supports_log_probs=True,
            supports_multimodal=False,
            supports_structured_outputs=True,
            supports_json=True,
            input_price=None,
            output_price=None,
        ),
    }
)


BEDROCK_MODELS_DATA = ModelDataRegistry(
    {
        ########################
        # anthropic (claude 3) #
        ########################
        "anthropic.claude-3-opus-20240229-v1:0": make_model_data(
            supports_log_probs=False,  # Converse responses don't include logprobs.
            supports_multimodal=False,  # SDK/model supports image input; DeepEval AmazonBedrockModel is text-only today.  # noqa: E501
            supports_structured_outputs=True,  # We support `schema` by parsing JSON from text (not toolConfig).
            supports_json=False,  # No cross-model JSON-mode supported by AmazonBedrockModel yet
            input_price=None,
            output_price=None,
        ),
        "anthropic.claude-3-sonnet-20240229-v1:0": make_model_data(
            supports_log_probs=False,
            supports_multimodal=False,  # SDK/model supports image input; DeepEval AmazonBedrockModel is text-only today.  # noqa: E501
            supports_structured_outputs=True,
            supports_json=False,
            input_price=None,
            output_price=None,
        ),
        ################################
        # anthropic (claude 4 / 4.5)   #
        ################################
        "anthropic.claude-opus-4-20250514-v1:0": make_model_data(
            supports_log_probs=False,
            supports_multimodal=False,  # SDK/model supports image input; DeepEval AmazonBedrockModel is text-only today.  # noqa: E501
            supports_structured_outputs=True,  # SDK supports tool use for some Converse models.  # noqa: E501
            supports_json=False,
            input_price=None,
            output_price=None,
        ),
        "anthropic.claude-opus-4-1-20250805-v1:0": make_model_data(
            supports_log_probs=False,
            supports_multimodal=False,  # SDK/model supports image input; DeepEval AmazonBedrockModel is text-only today.  # noqa: E501
            supports_structured_outputs=True,  # SDK supports tool use for some Converse models.  # noqa: E501
            supports_json=False,
            input_price=None,
            output_price=None,
        ),
        "anthropic.claude-sonnet-4-20250514-v1:0": make_model_data(
            supports_log_probs=False,
            supports_multimodal=False,  # SDK/model supports image input; DeepEval AmazonBedrockModel is text-only today.  # noqa: E501
            supports_structured_outputs=True,  # SDK supports tool use for some Converse models.  # noqa: E501
            supports_json=False,
            input_price=None,
            output_price=None,
        ),
        "anthropic.claude-sonnet-4-5-20250929-v1:0": make_model_data(
            supports_log_probs=False,
            supports_multimodal=False,  # SDK/model supports image input; DeepEval AmazonBedrockModel is text-only today.  # noqa: E501
            supports_structured_outputs=True,  # SDK supports tool use for some Converse models.  # noqa: E501
            supports_json=False,
            input_price=None,
            output_price=None,
        ),
        "anthropic.claude-haiku-4-5-20251001-v1:0": make_model_data(
            supports_log_probs=False,
            supports_multimodal=False,  # SDK/model supports image input; DeepEval AmazonBedrockModel is text-only today.  # noqa: E501
            supports_structured_outputs=True,  # SDK supports tool use for some Converse models.  # noqa: E501
            supports_json=False,
            input_price=None,
            output_price=None,
        ),
        ################
        # amazon titan #
        ################
        # NOTE: AWS examples for Titan Text are shown via InvokeModel (provider-specific),
        # not Converse, so these may not work with AmazonBedrockModel, which is converse only.
        "amazon.titan-text-express-v1": make_model_data(
            supports_log_probs=False,
            supports_multimodal=False,
            supports_structured_outputs=True,
            supports_json=False,
            input_price=None,
            output_price=None,
        ),
        "amazon.titan-text-premier-v1:0": make_model_data(
            supports_log_probs=False,
            supports_multimodal=False,
            supports_structured_outputs=True,
            supports_json=False,
            input_price=None,
            output_price=None,
        ),
        ###############
        # amazon nova #
        ###############
        "amazon.nova-micro-v1:0": make_model_data(
            supports_log_probs=False,
            supports_multimodal=False,
            supports_structured_outputs=True,
            supports_json=False,
            input_price=None,
            output_price=None,
        ),
        "amazon.nova-lite-v1:0": make_model_data(
            supports_log_probs=False,
            supports_multimodal=False,  # Some Nova models support multimodal via Converse; DeepEval AmazonBedrockModel is text-only today.  # noqa: E501
            supports_structured_outputs=True,  # Some Nova models support tool use.  # noqa: E501
            supports_json=False,
            input_price=None,
            output_price=None,
        ),
        "amazon.nova-pro-v1:0": make_model_data(
            supports_log_probs=False,
            supports_multimodal=False,  # Some Nova models support multimodal via Converse; DeepEval AmazonBedrockModel is text-only today.  # noqa: E501
            supports_structured_outputs=True,  # Some Nova models support tool use.  # noqa: E501
            supports_json=False,
            input_price=None,
            output_price=None,
        ),
        "amazon.nova-premier-v1:0": make_model_data(
            supports_log_probs=False,
            supports_multimodal=False,  # Some Nova models support multimodal via Converse; DeepEval AmazonBedrockModel is text-only today.  # noqa: E501
            supports_structured_outputs=True,  # Some Nova models support tool use.  # noqa: E501
            supports_json=False,
            input_price=None,
            output_price=None,
        ),
        ##################
        # meta (llama 4) #
        ##################
        "meta.llama4-maverick-17b-instruct-v1:0": make_model_data(
            supports_log_probs=False,
            supports_multimodal=False,  # SDK/model supports image input; DeepEval AmazonBedrockModel is text-only today.  # noqa: E501
            supports_structured_outputs=True,  # SDK tool use varies by model.  # noqa: E501
            supports_json=False,
            input_price=None,
            output_price=None,
        ),
        "meta.llama4-maverick-17b-instruct-128k-v1:0": make_model_data(
            supports_log_probs=False,
            supports_multimodal=False,  # SDK/model supports image input; DeepEval AmazonBedrockModel is text-only today.  # noqa: E501
            supports_structured_outputs=True,
            supports_json=False,
            input_price=None,
            output_price=None,
        ),
        "meta.llama4-scout-17b-instruct-v1:0": make_model_data(
            supports_log_probs=False,
            supports_multimodal=False,  # SDK/model supports image input; DeepEval AmazonBedrockModel is text-only today.  # noqa: E501
            supports_structured_outputs=True,
            supports_json=False,
            input_price=None,
            output_price=None,
        ),
        "meta.llama4-scout-17b-instruct-128k-v1:0": make_model_data(
            supports_log_probs=False,
            supports_multimodal=False,  # SDK/model supports image input; DeepEval AmazonBedrockModel is text-only today.  # noqa: E501
            supports_structured_outputs=True,
            supports_json=False,
            input_price=None,
            output_price=None,
        ),
        ##################
        # mistral (text) #
        ##################
        "mistral.mistral-large-2407-v1:0": make_model_data(
            supports_log_probs=False,
            supports_multimodal=False,
            supports_structured_outputs=True,
            supports_json=False,
            input_price=None,
            output_price=None,
        ),
        "mistral.mistral-large-2411-v1:0": make_model_data(
            supports_log_probs=False,
            supports_multimodal=False,
            supports_structured_outputs=True,
            supports_json=False,
            input_price=None,
            output_price=None,
        ),
        ############################
        # mistral (pixtral/vision) #
        ############################
        "mistral.pixtral-large-2411-v1:0": make_model_data(
            supports_log_probs=False,
            supports_multimodal=False,  # SDK/model supports image input; DeepEval AmazonBedrockModel is text-only today.  # noqa: E501
            supports_structured_outputs=True,
            supports_json=False,
            input_price=None,
            output_price=None,
        ),
        "mistral.pixtral-large-2502-v1:0": make_model_data(
            supports_log_probs=False,
            supports_multimodal=False,  # SDK/model supports image input; DeepEval AmazonBedrockModel is text-only today.  # noqa: E501
            supports_structured_outputs=True,
            supports_json=False,
            input_price=None,
            output_price=None,
        ),
        "mistral.pixtral-large-2511-v1:0": make_model_data(
            supports_log_probs=False,
            supports_multimodal=False,  # SDK/model supports image input; DeepEval AmazonBedrockModel is text-only today.  # noqa: E501
            supports_structured_outputs=True,
            supports_json=False,
            input_price=None,
            output_price=None,
        ),
        ####################
        # openai (gpt-oss) #
        ####################
        "openai.gpt-oss-20b-1:0": make_model_data(
            supports_log_probs=False,
            supports_multimodal=False,
            supports_structured_outputs=True,
            supports_json=False,
            input_price=None,
            output_price=None,
        ),
        "openai.gpt-oss-120b-1:0": make_model_data(
            supports_log_probs=False,
            supports_multimodal=False,
            supports_structured_outputs=True,
            supports_json=False,
            input_price=None,
            output_price=None,
        ),
    }
)
